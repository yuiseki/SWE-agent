{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SWE-agent: Turn LMs into SWE agents!","text":"<p>SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can fix bugs and issues in real GitHub repositories.</p> <ul> <li> <p> Background &amp; goals</p> <p>Learn more about the project goals and academic research.</p> <p> Learn more</p> </li> <li> <p> Installation</p> <p>We provide three different ways to get started, including running installation-free in your browser.</p> <p> Get started</p> </li> <li> <p> Usage</p> <p>Learn how to make the most out of SWE-agent.</p> <p> Tutorials, tips and tricks</p> </li> <li> <p> Changelog</p> <p>See what's new in SWE-agent</p> <p> Read the changelog</p> </li> <li> <p> Configuration</p> <p>SWE-agent can be tweaked extensively without modifying the code.</p> <p> Modify SWE-agent behavior</p> </li> <li> <p> Development</p> <p>Dig into SWE-agent's code and build your own agent!</p> <p> Development information</p> </li> </ul>"},{"location":"_footer/","title":"footer","text":"<ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"background/","title":"Learn more about the SWE-agent project.","text":"<p>This section of the documentation talks about the architecture, research goals and general direction of the project.</p> <p>Want to just run it? Skip ahead to our installation notes.</p>"},{"location":"background/aci/","title":"Agent Computer Interface (ACI)","text":"<p>We accomplish our results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an Agent-Computer Interface (ACI) and build the SWE-agent repository to make it easy to iterate on ACI design for repository-level coding agents.</p> <p>Just like how typical language models requires good prompt engineering, good ACI design leads to much better results when using agents. As we show in our paper, a baseline agent without a well-tuned ACI does much worse than SWE-agent.</p> <p>SWE-agent contains features that we discovered to be immensely helpful during the agent-computer interface design process:</p> <ol> <li>We add a linter that runs when an edit command is issued, and do not let the edit command go through if the code isn't syntactically correct.</li> <li>We supply the agent with a special-built file viewer, instead of having it just <code>cat</code> files. We found that this file viewer works best when displaying just 100 lines in each turn. The file editor that we built has commands for scrolling up and down and for performing a search within the file.</li> <li>We supply the agent with a special-built full-directory string searching command. We found that it was important for this tool to succinctly list the matches- we simply list each file that had at least one match. Showing the model more context about each match proved to be too confusing for the model.</li> <li>When commands have an empty output we return a message saying \"Your command ran successfully and did not produce any output.\"</li> </ol> <p>Read our paper for more details here.</p>"},{"location":"background/overview/","title":"Overview","text":"<p>SWE-agent turns LMs (e.g. GPT-4) into software engineering agents that can fix bugs and issues in real GitHub repositories.</p> <p>On SWE-bench, SWE-agent resolves 12.29% of issues, achieving the state-of-the-art performance on the full test set.</p> <p>We accomplish our results by designing simple LM-centric commands and feedback formats to make it easier for the LM to browse the repository, view, edit and execute code files. We call this an \ud83e\udd16 Agent-Computer Interface (ACI). Read more about the ACI here.</p> <p>SWE-agent is built and maintained by researchers from Princeton University.</p> <p> </p> <p>If you found this work helpful, please consider using the following citation:</p> <pre><code>@misc{yang2024sweagent,\n      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},\n      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},\n      year={2024},\n}\n</code></pre>"},{"location":"config/commands/","title":"Command Configuration","text":"<p>In this document, we describe how to implement your own commands for the SWE-agent ACI. To see examples of command implementations, open the <code>.sh</code> and <code>.py</code> files in the <code>config/commands</code> folder.</p>"},{"location":"config/commands/#scaffolding","title":"Scaffolding","text":"<p>Every command subscribes to the following skeleton code.</p> <pre><code># @yaml\n# signature: [command] [argument(s)]\n# docstring: [Brief description of what your command does.]\n# arguments:\n#   [argument 1 name]:\n#       type: [type (i.e. integer, string)]\n#       description: [Brief description of this argument]\n#       required: [true|false]\n#   [argument 2 name]:\n#       ...\n[command]() {\n    # Implementation here\n}\n</code></pre> <ul> <li>If a command takes in arguments, reference them via positional parameters notation (i.e. <code>$1</code>).</li> <li>If there are no arguments, omit the <code>arguments</code> section.</li> <li>The implementation for your command is unconstrained. There are no limitations on the form of the underlying command code.</li> <li>The minimal documentation requirements are <code>signature</code> and <code>docstring</code>.</li> <li>If you'd like multiple commands to make modifications to a similar body of functions, we recommend using global variables.<ul> <li>For instance, in <code>config/commands/default.sh</code>, you'll see we define the <code>CURRENT_LINE</code> variable for the file viewer. This variable is modified across multiple commands, including <code>open</code>, <code>goto</code>, <code>scroll_up</code>, <code>scroll_down</code>, and <code>edit</code>.</li> <li>You can also leverage third party libraries (check out how we do linting enabled <code>edit</code> in <code>config/commands/edit_linting.sh</code>).</li> </ul> </li> <li>To show effects of the command, print to standard output (i.e. <code>echo</code>). SWE-agent is implemented such that it does not look for a return value from these commands.</li> </ul>"},{"location":"config/commands/#displaying-the-command-to-swe-agent","title":"Displaying the Command to SWE-agent","text":"<p>After you define a command, there are a small set of additional steps to making it available for the agent to use.</p> <p>First, within your config file...</p> <ul> <li>Add <code>config/commands/&lt;file name&gt;.sh</code> file to the <code>command_files</code> field.</li> <li>Set the <code>parse_command</code> field to <code>ParseCommandBash</code> or <code>ParseCommandDetailed</code>. This key points to the functionality that generates how command documentation is shown to the agent.</li> <li>Decide which template(s) you want to show the <code>{command_docs}</code> in.<ul> <li>We strongly recommend including <code>{command_docs}</code> in the <code>system_template</code>, which is the first message shown to the agent for every task instance episode.</li> <li>You might also consider adding <code>{command_docs}</code> to the <code>format_error_template</code>, which is shown if the response provided by a model is malformed.</li> </ul> </li> <li>(Optional) Including a demonstration that uses a command is helpful to showcase proper use + increases the frequency with which the agent uses the command. If you'd like to add a demonstration...<ul> <li>Create a demonstration manually (i.e. <code>python run.py --model human_thought ...</code>) or automatically (i.e. <code>python run_replay --traj_path ...</code>)</li> <li>Add/Update the demonstration to the <code>demonstrations</code> argument.</li> <li>Update <code>demonstration_template</code> to control how the demonstration is displayed to the agent.</li> </ul> </li> </ul> <p>Config files</p> <p>If you're not familiar with how SWE-agent configuration files work, we recommend checking out the <code>config</code> documentation.</p> <p>Next, run your configuration and see how your agent uses the commands! <pre><code>python run.py --config_file config/[your config].yaml ...\n</code></pre></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/config/","title":"Configuration","text":"<p>This page contains details describing how to write your own configurations to control how agents can interact with the <code>SWEEnv</code> environment.</p> <p>A configuration is represented as a single <code>.yaml</code> file, specified by the <code>--config</code> flag in the command line interface, allowing you to...</p> <ul> <li>Define the commands that agents may use to traverse + modify a codebase (see here for more details)</li> <li>Write prompts that are deterministically/conditionally shown to the agent over the course of a single trajectory.</li> <li>Control the input/output interface that sits between the agent and <code>SWEEnv</code>.</li> </ul> <p>Default config files</p> <p>Our default config files are in the <code>config/</code> directory.</p>"},{"location":"config/config/#configuration-file-fields","title":"Configuration File Fields","text":"<p>The configuration is a <code>.yaml</code> file that consists of several fields. They are fully represented in this following outline:</p> <pre><code># Prompt Templates: Control how observations of environment are shown to agent\nsystem_template: | # .yaml syntax for multi-line string value\n  First `system` message shown to agent\ninstance_template: |- # .yaml syntax for multi-line string value w/ no new line\n  Instance prompt, contains task instance-specific content\nnext_step_template: |-\n  Format template of per-turn observation (Contains standard output from agent's action)\nnext_step_no_output_template: |-\n  Format template of observation when there is no standard output from the agent's action\nformat_error_template: |-\n  Format template of error message (Used when agent's action causes an error)\ndemonstration_template: |\n  Format template for showing a demonstration to the agent\ndemonstrations:\n- `trajectories/&lt;username&gt;/&lt;experiment folder&gt;/*.traj`\n- File is a demonstration of how to solve a task. This could an agent generated trajectory.\n- You can include 1+ demonstrations\n\n# Environment States: Define features of the SWEEnv environment\nenv_variables:\n# Default variables for SWEEnv at the beginning of each instance\n  CURRENT_FILE: 0\n  CURRENT_LINE:\n  OVERLAP:\n  SEARCH_FILES:\n  SEARCH_INDEX:\n  SEARCH_RESULTS:\n  WINDOW_SIZE:\n  START_INDEX:\n  END_INDEX:\n  START_CURSOR:\n  END_CUROSR:\n  START_CURSORS_MARK:\n  END_CURSOR_MARK:\nstate_command: |\n# `state_command` allows you to update state variables to reflect any aspect of the environment (e.g. current working directory)\n  name: state\n  code: |\n    state() { echo '{\"pwd\": \"'$PWD'\"}';\n\n# Action Interface: Define how an agent interacts with the SWEEnv environment\ncommand_files:\n- path/to/bash_file.sh\n- Each file contains a list of commands implemented in bash\n- You can include 1+ command files\nparse_command: Reference to functionality for defining command documentation\nhistory_processor: Reference to functionality for controlling agent's message history\nparse_function: Parser run on agent output\n</code></pre> <p>In the <code>config/</code> directory, we recommend looking at...</p> <ul> <li><code>configs/</code> for examples of properly formatted configuration files. Each configuration differs in its set of commands, input/output format, demonstrations, etc.</li> <li><code>commands/</code> for the bash implementations of the custom commands that SWE-agent uses to navigate + edit the codebase. More information here.</li> </ul> <p>Relative paths</p> <p>Relative paths in config files are resolved to the <code>SWE_AGENT_CONFIG_ROOT</code> environment variable (if set) or the SWE-agent repository root.</p>"},{"location":"config/config/#how-a-configuration-file-is-processed","title":"How a Configuration File is Processed","text":"<p>Some notes on processing that occurs on config fields when SWE-agent is run:</p> <ul> <li>Commands specified in <code>command_files</code> will be parsed into a single block of documentation text that can be referenced as <code>{command_docs}</code>.</li> <li><code>env_variables</code> are the default variables for the bash environment at the beginning of each instance.</li> <li><code>state_command</code> is used to extract state information from the bash environment (formatted as json) to be used in the templates given to the agent.</li> </ul> <p>Possible variables that can be used in templates are: - <code>{command_docs}</code> (an automatically compiled collection of available commands + their docstrings) - any variable given in <code>env_variables</code> (same spelling), e.g., <code>{WINDOW_SIZE}</code> - any variable extracted as json as part of the <code>state_command</code> function - the last observation <code>{observation}</code> - ... this list will grow as we implement more features!</p>"},{"location":"config/config/#template-workflow","title":"Template Workflow","text":"<p>The following diagram illustrates where each template is shown within a single episode of solving one task instance.</p> <p></p> <p>One of three templates can be shown per turn:</p> <ul> <li>\"Next Step\" (<code>next_step_template</code>): Displayed if the model's action successfully runs. The output and a prompt for the next action is shown</li> <li>\"Next Step (No Output)\" (<code>next_step_no_output_template</code>): Displayed if the model's action successfully runs, but does not produce any standard output (e.g. <code>rm</code>, <code>cd</code>)</li> <li>\"Format Error\" (<code>format_error_template</code>): Displayed if the model's response is malformed. Over the next two turns...</li> <li>If one of the model's next response is correct, the message history is updated such that the \"Format Error\" turn is not kept. The episode continues.</li> <li>If the model's next two responses are both malformed, the episode terminates.</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/demonstrations/","title":"Changing the demonstrations","text":"<p>An important way to show LMs how to use commands and interact with the environment is through providing a demonstration - which is basically a completed trajectory that the LM can learn from.</p> <p>For simplicity we only ingest demonstrations in the from of a trajectory file. However, since trajectory files are usually JSON, you can convert them to yaml using the <code>make_demos/convert_traj_to_demo.py</code> script to be more human-readable and easier to edit.</p> <p>Demo (yaml) files are stored in the <code>make_demos/demos</code> directory by default and consist primarily of the sequence of actions that an LM would need to take to complete a task. It's important that your demo have the proper format to be parsed by SWE-agent and your config.</p>"},{"location":"config/demonstrations/#converting-an-existing-trajectory-into-a-demonstration","title":"Converting an existing trajectory into a demonstration","text":"<p>Here's how you can make a demo from an existing trajectory file:</p> <ol> <li>Find a basic trajectory that you already like and want to use as the basis for your demo.    For instance, consider the <code>.traj</code> files in the <code>trajectories/demonstrations/</code> folder.</li> <li>Run <code>python convert_traj_to_demo.py &lt;path to trajectory file.traj&gt;</code> to convert the trajectory to a demo.    This demo will be saved as a readable yaml file in the <code>make_demos/demos</code> directory.</li> <li>Edit the demo by hand to make it work for your particular use case and configuration.</li> <li>(Optional) Run <code>python run_replay.py --traj_path &lt;path to demo&gt; --config_file &lt;path to config file&gt;</code> to execute the actions of the demo, have the system generate the execution output, and ensure that it works as expected.</li> <li>Inspect the resulting trajectory to ensure it was executed correctly.</li> </ol>"},{"location":"config/demonstrations/#manually-creating-a-custom-trajectory","title":"Manually creating a custom trajectory","text":"<p>You can also manually generate a trajectory by running the agent with <code>--model_name human</code> (which allows you to enter the commands that would usually be suggested by the LM) and then convert that trajectory into a demonstration as above. To edit text in <code>human</code> mode:</p> <ol> <li>Run the command <code>edit edit_start_line:edit_end_line</code></li> <li>Write the text you want to insert. Feel free to write the text across multiple lines.</li> <li>Press <code>return</code> then write <code>end_of_edit</code> and then press <code>return</code> again to submit the edit.</li> </ol> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"config/env/","title":"Environment variables","text":"<p>This page details all environment variables that are currently in use by SWE-agent.</p> <ul> <li>All API keys (for LMs and GitHub) can be set as an environment variable. See here for more information.</li> <li>[Experimental] <code>SWE_AGENT_EXPERIMENTAL_COMMUNICATE</code> switches to a faster way of communicating with running docker   shell sessions. This has not been tested on SWE-bench, so it might sometimes break.   However, it can be very useful when running on single issues.</li> <li><code>SWE_AGENT_CONFIG_ROOT</code>: Used to resolve relative paths in the config</li> </ul>"},{"location":"dev/contribute/","title":"Contribute to SWE-agent","text":"<p>Formatting change</p> <p>We've recently added automated formatting to our code base. If you are dealing with merge-conflicts when opening a PR or updating your fork, please first install <code>pre-commit</code> and run <code>pre-commit run --all-files</code> and try again.</p> <p>The easiest way to contribute is to give us feedback.</p> <ul> <li>Something isn't working? Open a bug report.   Rule of thumb: If you're running something and you get some error messages, this is the issue type for you.</li> <li>You have a concrete question? Open a question issue.</li> <li>You are missing something? Open a feature request issue</li> <li>Open-ended discussion? Talk on discord. Note that all actionable items should be an issue though.</li> </ul> <p>Wanna do more and actually contribute code? Great! Please see the following sections for tips and guidelines!</p>"},{"location":"dev/contribute/#development-repository-set-up","title":"Development repository set-up","text":"<p>Please install the repository from source, following our usual instructions but add the <code>[dev]</code> option to the <code>pip</code> command (you can just run the command again):</p> <pre><code>pip install -e '.[dev]'\n</code></pre> <p>Then, make sure to set up <code>pre-commit</code>:</p> <pre><code># cd to our repo root\npre-commit install\n</code></pre> <p><code>pre-commit</code> will check for formatting and basic syntax errors before your commits.</p> <p>Autofixes</p> <p>Most problems (including formatting) will be automatically fixed. Therefore, if <code>pre-commit</code>/<code>git commit</code> fails on its first run, simply try running it a second time.</p> <p>Some more autofixes can be enabled with the <code>--unsafe-fixes</code> option from <code>ruff</code>:</p> <pre><code>pipx run ruff check --fix --unsafe-fixes\n</code></pre>"},{"location":"dev/contribute/#running-tests","title":"Running tests","text":"<p>We provide a lot of tests that can be very helpful for rapid development. Run them with</p> <pre><code>pytest\n</code></pre> <p>Some of the tests might be slower than others. You can exclude them with</p> <pre><code>pytest -m \"not slow\"\n</code></pre>"},{"location":"dev/contribute/#tips-for-pull-requests","title":"Tips for pull requests","text":"<ul> <li>If you see a lot of formatting-related merge conflicts, please see here.</li> <li>Please open separate PRs for separate issues. This makes it easier to incorporate part of your changes.</li> <li>It might be good to open an issue and discuss first before investing time on an experimental feature.</li> <li>Don't know where to get started? Look for issues marked \ud83d\udc4b good first issue or \ud83d\ude4f help wanted</li> <li>When changing the behavior of the agent, we need to have some indication that it actually improves the success rate of SWE-agent.   However, if you make the behavior optional without complicating SWE-agent (for example by providing new commands),   we might be less strict.</li> <li>Please add simple unit tests or integration tests wherever possible. Take a look in the tests directory   for inspiration. We emphasize simple easy-tow-rite tests that get a lot of coverage.</li> </ul>"},{"location":"dev/contribute/#building-the-documentation","title":"Building the documentation","text":"<p>Simply run</p> <pre><code># cd repo root\nmkdocs serve\n</code></pre> <p>and point your browser to port 8000 or click one of the links in the output.</p>"},{"location":"dev/contribute/#diving-into-the-code","title":"Diving into the code","text":"<ul> <li> <p> Code structure and reference</p> <p>Read the reference for more information on our code.</p> <p> Read more</p> </li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"dev/formatting_conflicts/","title":"Formatting conflicts","text":"<p>On May 28th, 2024, we introduced automated formatting with <code>ruff-format</code> and <code>pre-commit</code>. This changed almost every file in the project. If you forked or branched off before these changes and now try to synchronize your fork/branch with <code>princeton-nlp/SWE-agent:main</code>, you will see a lot of merge conflicts.</p> <p>To solve this, you need to apply the same formatting to your code. Here's how you can do it.</p> <p>First let's add the official remote (if it exists, you've probably already added it and you can ignore the warning).</p> <pre><code>git remote add upstream https://github.com/princeton-nlp/SWE-agent.git\ngit fetch upstream\n</code></pre> <p>Now, you need the updated <code>pyproject.toml</code> and <code>.pre-commit-config.yaml</code> files. We can get them from <code>princeton-nlp/SWE-agent:main</code>:</p> <pre><code>git checkout upstream/main -- .pre-commit-config.yaml pyproject.toml\ngit commit -m \"Update formatting instructions\" --no-verify\n</code></pre> <p>Let's assume that your changes are on branch <code>FEATURE_BRANCH</code>, for example, if you've committed to <code>main</code>:</p> <pre><code>export FEATURE_BRANCH=\"main\"\n</code></pre> <p>Next we create a copy of this branch (so we don't further modify it):</p> <pre><code>git branch \"${FEATURE_BRANCH}\" \"${FEATURE_BRANCH}_REBASED\"\n</code></pre> <p>And now comes the tricky bit: We rebase your changes on top of <code>upstream/mean</code>, while applying the formatting fixes at every step:</p> <pre><code>git rebase upstream/main \"${FEATURE_BRANCH}_REBASED\" \\\n  -Xtheirs \\\n  --exec 'git reset --soft HEAD^; pre-commit run; pipx run ruff check --fix --unsafe-fixes; git add -u; git commit -C HEAD@{1} --no-verify'\n</code></pre> <p>Understanding the last command</p> <p>Here's what is happening:</p> <ul> <li><code>git rebase upstream/main \"${FEATURE_BRANCH}_REBASED\"</code> applies every commit from <code>\"${FEATURE_BRANCH}_REBASED\"</code> on top of <code>upstream/main</code>.</li> <li><code>-Xtheirs</code> tells git to always take your changes for merge conflicts   (rather than the format changes).</li> <li>After every commit, the command from <code>--exec</code> is being called.<ul> <li><code>git reset --soft HEAD^</code> undos the <code>git commit</code> action (while leaving the   changes staged),</li> <li>then we apply the formatting, and</li> <li>finally we commit the   formatted changes again.</li> </ul> </li> </ul> <p>Still merge conflicts?</p> <p>It's possible that there are non-formatting-related merge conflicts that you are encountering. In this case, <code>git rebase</code> will stop every time it cannot resolve the conflict. Simply fix the merge conflicts as you would normally do (edit the file, commit once done), and then run <code>git rebase --continue</code>.</p> <p>You can now open a PR from <code>${FEATURE_BRANCH}_REBASED</code> or make it your new default branch.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"installation/","title":"Setting up SWE-agent","text":"<ul> <li> <p> All in browser</p> <p>Run SWE-agent using GitHub codespaces. All necessary packages will be pre-installedn in an in-browser VSCode environment.</p> <p> Get started</p> </li> <li> <p> Install from source</p> <p>Install SWE-agent from source using <code>pip</code>.</p> <p> Get started</p> </li> <li> <p> Run in docker</p> <p>Pull a docker container and directly run SWE-agent. This is our fallback solution if the local installation does not work for you.</p> <p> Get started</p> </li> <li> <p> Changelog</p> <p>See what's new in SWE-agent</p> <p> Read the changelog</p> </li> </ul>"},{"location":"installation/changelog/","title":"Changelog","text":""},{"location":"installation/changelog/#050-2024-05-28","title":"0.5.0 (2024-05-28)","text":"<p>All new commits</p> <p>\u2728 The big news is our brand new documentation \u2728</p> <p>Secondly, @ollmer added a new flag <code>--cache_task_images</code> that will significantly speed up SWE-agent when running on the same environment/repository multiple times (no more waiting for cloning and installation!)</p>"},{"location":"installation/changelog/#breaking-changes","title":"Breaking changes","text":"<ul> <li>We have reformatted our codebase. If you create a PR based on a previous commit, make sure you install our <code>pre-commit</code> hook to avoid merge-conflicts because of formatting. See our docs for more information.</li> <li>Remove direct imports in <code>__init__.py</code> (you can no longer <code>from sweagent import Agent</code> by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/436</li> </ul>"},{"location":"installation/changelog/#added","title":"Added","text":"<ul> <li>Running the web UI is now supported when running swe-agent completely in docker</li> <li>Speed up evaluation by caching task environments as docker images by @ollmer in https://github.com/princeton-nlp/SWE-agent/pull/317</li> </ul>"},{"location":"installation/changelog/#improved","title":"Improved","text":"<ul> <li>Add gpt-4o model by @raymyers in https://github.com/princeton-nlp/SWE-agent/pull/344</li> <li>Web: Allow to specify commit hash by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/358</li> <li>Add default environment_setup config by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/351</li> <li>Enh: Suppress openai logging; improve formatting of stats by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/416</li> <li>Remove signal dependency by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/428</li> <li>Do not use select if running on Windows by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/429</li> <li>Use custom Config class to support env and keys.cfg (this allows passing keys as environment variables) by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/430</li> </ul>"},{"location":"installation/changelog/#fixes","title":"Fixes","text":"<ul> <li>Web: Fix script_path input by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/334</li> <li>Fix: Don't print patch msg for exit_cost patch by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/343</li> <li>Fix: Do not request job control in bash by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/345</li> <li>Fix: --base_commit not used for gh urls by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/346</li> <li>Fix: Separate data path/traj dir cause exception by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/348</li> <li>Add docker-py lower bound by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/406</li> <li>Fix: IndexError when replaying incomplete trajectories by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/410</li> </ul>"},{"location":"installation/changelog/#040-2024-05-09","title":"0.4.0 (2024-05-09)","text":"<p>All new commits</p>"},{"location":"installation/changelog/#added_1","title":"Added","text":"<p>We\u2019re excited to launch the SWE-agent web UI! Specify a bug, press start and watch SWE-agent do the magic.</p>"},{"location":"installation/changelog/#030-2024-05-02","title":"0.3.0 (2024-05-02)","text":""},{"location":"installation/changelog/#added_2","title":"Added","text":"<ul> <li>Run SWE-agent in the cloud using GitHub Codespaces</li> <li>Add GPT4-turbo model by @zgrannan in https://github.com/princeton-nlp/SWE-agent/pull/252</li> <li>feat: Amazon Bedrock support (Claude models) by @JGalego in https://github.com/princeton-nlp/SWE-agent/pull/207</li> </ul>"},{"location":"installation/changelog/#fixed","title":"Fixed","text":"<ul> <li>Better error handling for --open_pr by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/239</li> <li>Fixed a potential error by @DanjieTang in https://github.com/princeton-nlp/SWE-agent/pull/242</li> <li>fix: TARGETARCH not set on some OS/docker setups by @mspronesti in https://github.com/princeton-nlp/SWE-agent/pull/249</li> <li>Pass Python version to get_environment_yml by @waterson in https://github.com/princeton-nlp/SWE-agent/pull/271</li> <li>Fix Together model validation error by @mikanfactory in https://github.com/princeton-nlp/SWE-agent/pull/236</li> <li>Doc: Avoid invalid github token by @klieret in https://github.com/princeton-nlp/SWE-agent/pull/292</li> </ul>"},{"location":"installation/changelog/#020-2024-04-15","title":"0.2.0 (2024-04-15)","text":"<p>All new commits</p>"},{"location":"installation/changelog/#added_3","title":"Added","text":"<ul> <li>Allow to run on local repos (new flag: <code>--repo_path</code>) in https://github.com/princeton-nlp/SWE-agent/pull/193</li> <li>Patch files are now saved separately to a patch directory in https://github.com/princeton-nlp/SWE-agent/pull/126</li> <li>Allow to supply custom installation commands when running on gh issues or locally (<code>--environment_setup</code>) in https://github.com/princeton-nlp/SWE-agent/pull/153</li> <li>Allow to specify openapi base url in <code>keys.cfg</code> in https://github.com/princeton-nlp/SWE-agent/pull/118</li> </ul>"},{"location":"installation/changelog/#improved_1","title":"Improved","text":"<ul> <li>Improve error handling of docker issues in https://github.com/princeton-nlp/SWE-agent/pull/165</li> <li>Make github token fully optional in https://github.com/princeton-nlp/SWE-agent/pull/189</li> </ul>"},{"location":"installation/changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Fix opening PR from fork in https://github.com/princeton-nlp/SWE-agent/pull/229</li> <li>Fix: Choosing TogetherAI models in https://github.com/princeton-nlp/SWE-agent/pull/130</li> </ul>"},{"location":"installation/codespaces/","title":"Running SWE-agent in your browser","text":"<p>Running SWE-agent in your browser is the easiest way to try out our project.</p> <ol> <li>Click </li> <li>Add your API keys to <code>keys.cfg</code> (find the file in the left sidebar and fill out the template). More information on the keys here.</li> <li>Make sure to wait until the <code>postCreateCommand</code> in the terminal window at the bottom is finished</li> <li>Enter your SWE-agent command, see using the web interface or using the command line.</li> </ol>"},{"location":"installation/codespaces/#running-the-web-ui","title":"Running the Web UI","text":"<p>Go to the terminal and enter</p> <pre><code>./start_web_ui.sh\n</code></pre> <p>After a while, you should see a popup offering you to forward port <code>3000</code>. Click <code>Open in Browser</code>.</p> <p></p> <p>If you instead only see the offer to forward port <code>8000</code>, do not click it (this is the port that's being used by the backend).</p> <p>Instead, click on the <code>Ports</code> tab, and click on the globe next to port <code>3000</code>:</p> <p></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"installation/docker/","title":"Fallback: Usage with docker","text":"<p>Instead of installing SWE-agent from source, you can also run the software directly using Docker.</p> <ol> <li>Install Docker, then start Docker locally.</li> <li>Run <code>docker pull sweagent/swe-agent:latest</code></li> <li>Add your API tokens to a file <code>keys.cfg</code> as explained here or pass them as    environment variables.</li> </ol>"},{"location":"installation/docker/#running-the-command-line-interface","title":"Running the command line interface","text":"<p>Assuming that you create <code>keys.cfg</code> in the current directory, run</p> <pre><code>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v $(pwd)/keys.cfg:/app/keys.cfg \\\n  sweagent/swe-agent-run:latest \\\n  python run.py --image_name=sweagent/swe-agent:latest \\\n  --model_name gpt4 \\\n  --data_path https://github.com/pvlib/pvlib-python/issues/1603 \\\n  --config_file config/default_from_url.yaml  --skip_existing=False\n</code></pre> <p>Windows</p> <p>If you're using docker on Windows, use <code>-v //var/run/docker.sock:/var/run/docker.sock</code> (double slash) to escape it (more information).</p> <p>If you instead want to pass the keys as environment variables, use</p> <pre><code>docker run --rm -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -e GITHUB_TOKEN=\"yourgithubtoken\" \\\n  -e OPENAI_API_KEY=\"youropenaikey\" \\\n  sweagent/swe-agent-run:latest \\\n  # rest of the command above\n</code></pre> <p>Getting updates</p> <p>Even though the image <code>sweagent/swe-agent:latest</code> has the tag <code>latest</code>, it is not automatically updated every time you run <code>docker run</code>. Instead, you need to manually run</p> <pre><code>docker pull sweagent/swe-agent-run:latest\ndocker pull sweagent/swe-agent:latest\n</code></pre> <p>periodically.</p> <p>Retrieving generated files</p> <p>The optional <code>--rm</code> flag removes the docker container after the command has terminated. Therefore, to retrieve files (like generated patch files) from the container, please remove this flag.</p>"},{"location":"installation/docker/#running-the-web-server","title":"Running the web server","text":"<p>Tip</p> <p>Please also read the previous section for tips on passing environment variables and staying up to date.</p> <p>To run the web server, make sure to forward port 3000:</p> <pre><code>docker run -p 3000:3000 -it -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v $(pwd)/keys.cfg:/app/keys.cfg \\\n  sweagent/swe-agent-run:latest bash start_web_ui.sh\n</code></pre> <p>More tips</p> <p>See the installation issues section for more help if you run into trouble.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"installation/keys/","title":"Adding your API keys","text":"<p>In order to access the LM of your choice (and to access private GitHub repositories), you need to supply the corresponding keys.</p> <p>There are two options to do this:</p> <ol> <li>Set the corresponding environment variables.</li> <li>Create a <code>keys.cfg</code> file at the root of this repository.</li> </ol> <p>The following <code>keys.cfg</code> example shows you how the keys are named:</p> <pre><code># Remove the comment '#' in front of the line for all keys that you have set\n# GITHUB_TOKEN: 'GitHub Token for access to private repos'\n# OPENAI_API_KEY: 'OpenAI API Key Here if using OpenAI Model'\n# ANTHROPIC_API_KEY: 'Anthropic API Key Here if using Anthropic Model'\n# TOGETHER_API_KEY: 'Together API Key Here if using Together Model'\n# AZURE_OPENAI_API_KEY: 'Azure OpenAI API Key Here if using Azure OpenAI Model'\n# AZURE_OPENAI_ENDPOINT: 'Azure OpenAI Endpoint Here if using Azure OpenAI Model'\n# AZURE_OPENAI_DEPLOYMENT: 'Azure OpenAI Deployment Here if using Azure OpenAI Model'\n# AZURE_OPENAI_API_VERSION: 'Azure OpenAI API Version Here if using Azure OpenAI Model'\n# OPENAI_API_BASE_URL: 'LM base URL here if using Local or alternative api Endpoint'\n</code></pre> <p>See the following links for tutorials on obtaining Anthropic, OpenAI, and Github tokens.</p>"},{"location":"installation/source/","title":"Installation from source","text":"<p>Installation from source is the preferred way to set up SWE-agent on your machine.</p> <p>Issues on Windows</p> <p>Expect some issues with Windows (we're working on them). In the meantime, use Docker.</p> <ol> <li>Install Docker, then start Docker locally.</li> <li>If you plan on using the web-based GUI: Install <code>nodejs</code>.</li> <li>Clone the repository, for example with     <pre><code>git clone https://github.com/princeton-nlp/SWE-agent.git\n</code></pre></li> <li>Run     <pre><code>pip install --editable .\n</code></pre>     at the repository root (as with any python setup, it's recommended to use conda or virtual environments to manage dependencies). Error about editable install? Please update pip<sup>1</sup>.</li> <li>Run     <pre><code>docker pull sweagent/swe-agent:latest\n</code></pre> Alternatively, you can run <code>./setup.sh</code> to create your own <code>swe-agent</code> docker image.</li> <li>Set up your LM API keys as explained here.</li> </ol> <p>Docker issues</p> <p>If you run into docker issues, see the installation tips section for more help.</p> <p>Updating</p> <p>SWE-agent is still in active development. Features and enhancement are added often. To make sure you are on the latest version, periodically run <code>git pull</code> (there is no need to redo the <code>pip install</code>). You might also want to run <code>git pull sweagent/swe-agent:latest</code> periodically (though changes to the container are more rare).</p> <p>Development setup</p> <p>Want to modify SWE-agent? Great! There are a few extra steps and tips: Please check our contribution guide.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul> <ol> <li> <p>You can update <code>pip</code> with <code>pip install --upgrade pip</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"installation/tips/","title":"More installation tips","text":"<p>If you seem to be having issues with running docker</p> <ul> <li>Make sure that you allow the use of the Docker socket. In Docker desktop, click Settings &gt; Advanced &gt; Allow the default Docker socket to be used (requires password)</li> <li>If your docker installation uses a different socket, you might have to symlink them, see this command for example</li> <li>If you are using any containers from dockerhub (i.e., you ran <code>docker pull ...</code> or you are running <code>docker run ...</code>), please make sure that you are using the latest   versions. Just because an image has the <code>latest</code> tag (e.g., <code>sweagent/swe-agent-run:latest</code>) does not mean that it will auto-update. Please run   <code>docker pull sweagent/swe-agent-run:latest</code> to make sure you actually have the most recent version!</li> </ul> <p>Any remaining issues? Please open a GitHub issue!</p>"},{"location":"reference/","title":"Code structure and reference","text":"<p>This section is in development</p> <p>Some submodules are missing. We also do not strive for completeness, but provide this as an easy entry point for people who want to start reading the code. Also note that SWE-agent is still developed very actively, so the python implementation details are still changing. See the changelog for more information.</p> <p>The core:</p> <ul> <li>The <code>sweagent/agent/</code>  submodule implements the agent.<ul> <li>Read about the <code>Agent</code> class</li> <li>Explore the code</li> </ul> </li> <li>The <code>sweagent/environment/</code> submodule handles the communication with the docker container where we execute code.<ul> <li>Read about the <code>SWEEnv</code> class</li> <li>Explore the code</li> </ul> </li> </ul> <p>More subolders</p> <ul> <li>See the <code>scripts/</code> folder for other useful scripts and details.</li> <li>See the <code>config/</code> folder for details about how you can define your own configuration!</li> <li>See the <code>trajectories/</code> folder for details about the output of <code>run.py</code>.</li> <li>See the <code>evaluation/</code> folder for details about how evaluation works.</li> </ul>"},{"location":"reference/agent/","title":"The agent class","text":""},{"location":"reference/agent/#sweagent.agent.agents.Agent","title":"<code>Agent</code>","text":"<p>Agent handles the behaviour of the model and how it interacts with the environment.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>class Agent:\n    \"\"\"Agent handles the behaviour of the model and how it interacts with the environment.\"\"\"\n\n    def __init__(self, name: str, args: AgentArguments):\n        self.name = name\n        self.model = get_model(args.model, args.config._commands + args.config.subroutine_types)\n        self.config = args.config\n        assert self.config is not None  # mypy\n        self.system_args = {\n            \"command_docs\": self.config.command_docs,\n            **self.config.env_variables,\n        }\n        self.instance_args = None\n        self._parse_command_patterns()\n        self.history = []\n        self.last_container_id = None\n        self.hooks = []\n\n    def add_hook(self, hook: AgentHook):\n        hook.on_init()\n        self.hooks.append(hook)\n\n    def _append_history(self, item: dict):\n        for hook in self.hooks:\n            hook.on_query_message_added(**item)\n        self.history.append(item)\n\n    def setup(self, instance_args, init_model_stats=None) -&gt; None:\n        \"\"\"Setup the agent for a new instance.\"\"\"\n        assert self.config is not None  # mypy\n        self.model.reset_stats(init_model_stats)\n        self.instance_args = instance_args\n\n        system_msg = self.config.system_template.format(**self.system_args)\n        logger.info(f\"SYSTEM ({self.name})\\n{system_msg}\")\n\n        self.history: list[dict[str, Any]] = []\n        self._append_history({\"role\": \"system\", \"content\": system_msg, \"agent\": self.name})\n\n        if \"history_to_messages\" in dir(self.model):\n            for demonstration_path in self.config.demonstrations:\n                if self.config.demonstration_template is None and not self.config.put_demos_in_history:\n                    msg = \"Cannot use demonstrations without a demonstration template or put_demos_in_history=True\"\n                    raise ValueError(msg)\n\n                # Load history\n                logger.info(f\"DEMONSTRATION: {demonstration_path}\")\n                demo_history = json.loads(Path(demonstration_path).read_text())[\"history\"]\n                demo_history = [\n                    entry\n                    for entry in demo_history\n                    if (\"agent\" not in entry) or (\"agent\" in entry and entry[\"agent\"] == self.name)\n                ]\n\n                if self.config.put_demos_in_history:\n                    if self.config.demonstration_template is not None:\n                        logger.warning(\"Demonstration template is ignored for put_demos_in_history=True\")\n                    # Add demonstration to history directly as separate messages\n                    for entry in demo_history:\n                        if entry[\"role\"] != \"system\":\n                            entry[\"is_demo\"] = True\n                            self._append_history(entry)\n                else:\n                    # Add demonstration as single message to history\n                    demo_message = self.model.history_to_messages(\n                        demo_history,\n                        is_demonstration=True,\n                    )\n                    demonstration = self.config.demonstration_template.format(demonstration=demo_message)\n                    self._append_history(\n                        {\n                            \"agent\": self.name,\n                            \"content\": demonstration,\n                            \"is_demo\": True,\n                            \"role\": \"user\",\n                        },\n                    )\n\n    @property\n    def state_command(self) -&gt; str:\n        \"\"\"Return the bash command that will be used to extract the environment state.\"\"\"\n        return self.config.state_command.name\n\n    @property\n    def local_history(self) -&gt; list[dict[str, str]]:\n        \"\"\"Return the history of the agent since the last reset.\"\"\"\n        return self.config.history_processor([entry for entry in self.history if entry[\"agent\"] == self.name])\n\n    def save_trajectory(self, trajectory, log_path: Path, env_name: str, info: dict[str, Any]):\n        log_dict = {\n            \"environment\": env_name,\n            \"trajectory\": trajectory,\n            \"history\": self.history,\n            \"info\": info,\n        }\n        log_path.write_text(json.dumps(log_dict, indent=2))\n        logger.info(f\"Saved trajectory to {log_path}\")\n\n    def _get_first_match(self, action: str, pattern_type: str) -&gt; re.Match | None:\n        \"\"\"Return the first match of a command pattern in the action string.\"\"\"\n        assert self.config is not None  # mypy\n        if pattern_type == \"subroutine\":\n            patterns = {k: v for k, v in self.subroutine_patterns.items()}\n        elif pattern_type == \"multi_line\":\n            patterns = {\n                k: v\n                for k, v in self.command_patterns.items()\n                if k in self.config.multi_line_command_endings or k == self.config.submit_command\n            }\n            patterns += {\n                k: v for k, v in self.subroutine_patterns.items() if k in self.config.multi_line_command_endings\n            }\n        elif pattern_type == \"multi_line_no_subroutines\":\n            patterns = {k: v for k, v in self.command_patterns.items() if k in self.config.multi_line_command_endings}\n        else:\n            msg = f\"Unknown pattern type: {pattern_type}\"\n            raise ValueError(msg)\n        matches = list()\n        for _, pat in patterns.items():\n            match = pat.search(action)\n            if match:\n                matches.append(match)\n        if len(matches) == 0:\n            return None\n        matches = sorted(matches, key=lambda x: x.start())\n        return matches[0]\n\n    def _guard_multiline_input(self, action: str) -&gt; str:\n        \"\"\"Split action by multiline commands, then append the first line in each multiline command with \"&lt;&lt; '{end_name}'\".\n        Multiline commands (which are specified by an end_name) are commands that span multiple lines and are terminated by a specific end_name.\n\n        Their multi-line argument is sent using a heredoc, which is a way to send a multi-line string to a command in bash.\n        \"\"\"\n        parsed_action = list()\n        rem_action = action\n        while rem_action.strip():\n            first_match = self._get_first_match(rem_action, \"multi_line_no_subroutines\")\n            if first_match:\n                pre_action = rem_action[: first_match.start()]\n                match_action = rem_action[first_match.start() : first_match.end()]\n                rem_action = rem_action[first_match.end() :]\n                if pre_action.strip():\n                    parsed_action.append(pre_action)\n                if match_action.strip():\n                    eof = first_match.group(3).strip()\n                    if not match_action.split(\"\\n\")[0].strip().endswith(f\"&lt;&lt; '{eof}'\"):\n                        guarded_command = match_action[first_match.start() :]\n                        first_line = guarded_command.split(\"\\n\")[0]\n                        guarded_command = guarded_command.replace(first_line, first_line + f\" &lt;&lt; '{eof}'\", 1)\n                        parsed_action.append(guarded_command)\n                    else:\n                        parsed_action.append(match_action)\n            else:\n                parsed_action.append(rem_action)\n                rem_action = \"\"\n        return \"\\n\".join(parsed_action)\n\n    def split_actions(self, action: str, pattern_type=\"subroutine\") -&gt; list[dict[str, Any]]:\n        \"\"\"Split an action into a list of actions in a greedy manner, each of which is a subroutine call or a single command.\"\"\"\n        parsed_action = list()\n        rem_action = action\n        while rem_action.strip():\n            first_match = self._get_first_match(rem_action, pattern_type)\n            if first_match:\n                pre_action = rem_action[: first_match.start()]\n                match_action = rem_action[first_match.start() : first_match.end()]\n                rem_action = rem_action[first_match.end() :]\n                if pre_action.strip():\n                    parsed_action.append({\"agent\": self.name, \"action\": pre_action, \"cmd_name\": None})\n                if match_action.strip():\n                    if match_action.split()[0] == self.config.submit_command:\n                        parsed_action.append(\n                            {\n                                \"agent\": self.name,\n                                \"action\": match_action,\n                                \"cmd_name\": first_match.group(1),\n                            },\n                        )  # submit command is not a subroutine\n                    else:\n                        parsed_action.append(\n                            {\n                                \"agent\": first_match.group(1),\n                                \"args\": first_match.group(2),\n                                \"action\": match_action,\n                                \"cmd_name\": first_match.group(1),\n                            },\n                        )\n            else:\n                parsed_action.append({\"agent\": self.name, \"action\": rem_action, \"cmd_name\": None})\n                rem_action = \"\"\n        return parsed_action\n\n    def _parse_command_patterns(self):\n        assert self.config is not None  # mypy\n        self.command_patterns = dict()\n        for command in self.config._commands:\n            if command.end_name is not None:\n                pat = re.compile(\n                    rf\"^\\s*({command.name})\\s*(.*?)^({command.end_name})\\s*$\",\n                    re.DOTALL | re.MULTILINE,\n                )\n                self.command_patterns[command.name] = pat\n            else:\n                pat = re.compile(rf\"^\\s*({command.name})\\s*(.*?)$\", re.MULTILINE)\n                self.command_patterns[command.name] = pat\n        self.subroutine_patterns = dict()\n        for _, subroutine in self.config._subroutines.items():\n            if subroutine.end_name is None:\n                pat = re.compile(rf\"^\\s*({subroutine.name})\\s*(.*?)$\", re.MULTILINE)\n                self.subroutine_patterns[subroutine.name,] = pat\n            else:\n                pat = re.compile(\n                    rf\"^\\s*({subroutine.name})\\s*(.*?)^({subroutine.end_name})\\s*$\",\n                    re.DOTALL | re.MULTILINE,\n                )\n                self.subroutine_patterns[subroutine.name] = pat\n        if hasattr(self.config, \"submit_command_end_name\"):\n            submit_pat = re.compile(\n                rf\"^\\s*({self.config.submit_command})\\s*(.*?)^({self.config.submit_command_end_name})\\s*$\",\n                re.DOTALL | re.MULTILINE,\n            )\n        else:\n            submit_pat = re.compile(rf\"^\\s*({self.config.submit_command})(\\s*)$\", re.MULTILINE)  # group 2 is nothing\n        self.subroutine_patterns[self.config.submit_command] = submit_pat\n        self.command_patterns[self.config.submit_command] = submit_pat\n\n    def forward(self, observation: str, available_actions: list[str], state: str) -&gt; tuple[str, str, str]:\n        thought, action, output = self.forward_with_error_check(observation, state)\n\n        self._append_history(\n            {\n                \"role\": \"assistant\",\n                \"content\": output,\n                \"thought\": thought,\n                \"action\": action,\n                \"agent\": self.name,\n            },\n        )\n\n        logger.info(f\"\ud83d\udcad THOUGHT ({self.name})\\n{thought}\")\n        logger.info(f\"\ud83c\udfac ACTION ({self.name})\\n{action}\")\n\n        return thought, action, output\n\n    def forward_model(self, observation: str, state: str) -&gt; str:\n        \"\"\"Query the model with the current state and observation with the appropriate template.\n\n        Returns the model output.\"\"\"\n        assert self.config is not None  # mypy\n\n        state_vars = json.loads(state)\n\n        templates: list[str] = []\n        # Determine observation template based on what prior observation was\n        if self.history[-1][\"role\"] == \"system\" or self.history[-1].get(\"is_demo\", False):\n            # Show instance template if prev. obs. was initial system message\n            templates = [self.config.instance_template]\n            if self.config.strategy_template is not None:\n                templates.append(self.config.strategy_template)\n        elif observation is None or observation.strip() == \"\":\n            # Show no output template if observation content was empty\n            templates = [self.config.next_step_no_output_template]\n        else:\n            # Show standard output template if there is observation content\n            templates = [self.config.next_step_template]\n\n        # Populate selected template(s) with information (e.g., issue, arguments, state)\n        messages = []\n        for template in templates:\n            messages.append(\n                template.format(\n                    **self.instance_args,\n                    **self.system_args,\n                    **state_vars,\n                    observation=(observation if observation is not None else \"\"),\n                ),\n            )\n\n        message = \"\\n\".join(messages)\n\n        logger.info(f\"\ud83e\udd16 MODEL INPUT\\n{message}\")\n        self._append_history({\"role\": \"user\", \"content\": message, \"agent\": self.name})\n\n        for hook in self.hooks:\n            hook.on_model_query(query=self.local_history, agent=self.name)\n        return self.model.query(self.local_history)\n\n    def retry_after_format_fail(self, output):\n        \"\"\"Ask the model to correct (without committing to persistent history) after a malformatted model output\"\"\"\n        format_error_template = self.config.format_error_template\n\n        logger.warning(f\"MALFORMED OUTPUT\\n{output}\")\n        logger.warning(f\"FORMAT ERROR\\n{format_error_template}\")\n\n        temp_history = self.local_history + [\n            {\"role\": \"assistant\", \"content\": output, \"agent\": self.name},\n            {\"role\": \"user\", \"content\": format_error_template, \"agent\": self.name},\n        ]\n        return self.model.query(temp_history)\n\n    def retry_after_blocklist_fail(self, output, action):\n        \"\"\"Ask the model to correct (without committing to persistent history) after a disallowed command\"\"\"\n        name = action.strip().split()[0]\n        blocklist_error_message = self.config.blocklist_error_template.format(name=name)\n\n        logger.warning(f\"BLOCKLISTED OUTPUT\\n{output}\")\n        logger.warning(f\"BLOCKLIST ERROR\\n{blocklist_error_message}\")\n\n        temp_history = self.local_history + [\n            {\"role\": \"assistant\", \"content\": output, \"agent\": self.name},\n            {\"role\": \"user\", \"content\": blocklist_error_message, \"agent\": self.name},\n        ]\n        return self.model.query(temp_history)\n\n    def should_block_action(self, action):\n        \"\"\"Check if the command should be blocked.\"\"\"\n        names = action.strip().split()\n        if len(names) == 0:\n            return False\n        name = names[0]\n        if name in self.config.blocklist:\n            return True\n        if name in self.config.blocklist_standalone and name == action.strip():\n            return True\n        return False\n\n    def check_format_and_requery(\n        self,\n        output: str,\n    ) -&gt; tuple[str, str, str]:\n        \"\"\"Query the model with the current state and observation with the appropriate template.\n\n        Try to parse the output into a thought and action. Retry if the output is malformatted or the action is blocked.\n\n        Returns the thought, action, and raw model output.\n        \"\"\"\n        # Condition for handling outputs with no thought (just action)\n        if self.model.args.model_name == \"human\":\n            return \"\", output, output\n        elif self.model.args.model_name == \"human_thought\":\n            thought, action = ParseFunction.get(\"ThoughtActionParser\")(\n                output,\n                self.config._commands + self.config.subroutine_types,\n                strict=False,\n            )\n            return thought, action, output\n\n        format_fails = blocklist_fails = 0\n\n        while format_fails + blocklist_fails &lt;= 2:\n            try:\n                thought, action = self.config.parse_function(\n                    output,\n                    self.config._commands + self.config.subroutine_types,\n                    strict=False,\n                )\n            except KeyboardInterrupt:\n                raise\n            except FormatError:\n                format_fails += 1\n                output = self.retry_after_format_fail(output)\n                continue\n            if self.should_block_action(action):\n                blocklist_fails += 1\n                output = self.retry_after_blocklist_fail(output, action)\n            else:\n                return thought, action, output\n        logger.warning(f\"Malformat limit reached: \\n{output}\")\n        return \"Exit due to format error\", \"exit_format\", output\n\n    def forward_with_error_check(self, observation: str, state: str) -&gt; tuple[str, str, str]:\n        \"\"\"Wrapper around `self.forward_model` that handles errors and retries\n        due to format errors or blocked actions.\n        \"\"\"\n        try:\n            output = self.forward_model(observation, state)\n        except KeyboardInterrupt:\n            raise\n        except RuntimeError as e:\n            logger.warning(f\"Runtime error: {e}\")\n            return (\n                f\"Exit due to runtime error: {e}\",\n                \"exit_error\",\n                f\"exit due to runtime error: {e}\",\n            )\n        except ContextWindowExceededError:\n            logger.warning(\"Context window exceeded\")\n            return \"Exit due to context window\", \"exit_context\", \"Exit due to context window\"\n        except CostLimitExceededError:\n            logger.warning(\"Cost limit exceeded\")\n            return \"Exit due to cost limit\", \"exit_cost\", \"Exit due to cost limit\"\n        except RetryError as e:\n            logger.warning(f\"Retry error: {e}\")\n            return (\n                f\"Exit due to retry error: {e}\",\n                \"exit_api\",\n                f\"exit due to retry error: {e}\",\n            )\n        return self.check_format_and_requery(output)\n\n    def init_environment_vars(self, env):\n        self.set_environment_vars(env, self.config.env_variables)\n\n    def set_environment_vars(self, env, env_variables):\n        assert self.config is not None  # mypy\n        commands_to_execute = (\n            [self.config.state_command.code]\n            +\n            # [code for code in self.config.util_functions] +\n            # [command.code for command in self.config._commands] +\n            [f\"{k}={v}\" for k, v in env_variables.items()]\n        )\n        commands = \"\\n\".join(commands_to_execute)\n        try:\n            output = env.communicate(commands)\n            if env.returncode != 0:\n                msg = f\"Nonzero return code: {env.returncode}\\nOutput: {output}\"\n                raise RuntimeError(msg)\n        except KeyboardInterrupt:\n            raise\n        except Exception as e:\n            logger.warning(\"Failed to set environment variables\")\n            raise e\n        command_files = list()\n        for file in self.config.command_files:\n            datum = dict()\n            contents = open(file).read()\n            datum[\"contents\"] = contents\n            filename = Path(file).name\n            if not contents.strip().startswith(\"#!\"):\n                if filename.endswith(\".sh\"):\n                    # files are sourced, so they are not executable\n                    datum[\"name\"] = Path(file).name\n                    datum[\"type\"] = \"source_file\"\n                elif filename.startswith(\"_\"):\n                    # files are sourced, so they are not executable\n                    datum[\"name\"] = Path(file).name\n                    datum[\"type\"] = \"utility\"\n                else:\n                    msg = (\n                        f\"Non-shell script file {file} does not start with shebang.\\n\"\n                        \"Either add a shebang (#!) or change the file extension to .sh if you want to source it.\\n\"\n                        \"You can override this behavior by adding an underscore to the file name (e.g. _utils.py).\"\n                    )\n                    raise ValueError(msg)\n            else:\n                # scripts are made executable\n                datum[\"name\"] = Path(file).name.rsplit(\".\", 1)[0]\n                datum[\"type\"] = \"script\"\n            command_files.append(datum)\n        env.add_commands(command_files)\n\n    def get_environment_vars(self, env):\n        assert self.config is not None  # mypy\n        env_vars = dict()\n        for var in self.config.env_variables:\n            env_vars[var] = env.communicate(f\"echo ${var}\").strip()\n        return env_vars\n\n    def call_subroutine(self, agent_name, sub_action, env):\n        assert self.config is not None  # mypy\n        env_vars = self.get_environment_vars(env)\n        cwd = env.communicate(\"pwd -P\").strip()\n        init_observation = self.config._subroutines[agent_name].init_observation\n        if init_observation is not None:\n            obs, _, _, _ = env.step(init_observation.format(args=sub_action[\"args\"]))\n        else:\n            obs = None\n        if env.returncode != 0:\n            self._append_history({\"role\": \"user\", \"content\": obs, \"agent\": agent_name})\n            msg = f\"Nonzero return code: {env.returncode} for init_observation in {agent_name}.\\n{obs}\"\n            raise RuntimeError(msg)\n        return_type = self.config._subroutines[agent_name].return_type\n        sub_agent = Agent(agent_name, self.config._subroutines[agent_name].agent_args)\n        sub_agent_output = sub_agent.run(\n            {\"issue\": sub_action[\"args\"]},\n            env,\n            observation=obs,\n            return_type=return_type,\n            init_model_stats=self.model.stats,\n        )\n        self.history += sub_agent.history\n        self.set_environment_vars(env, env_vars)\n        env.communicate(f\"cd {cwd}\")\n        self.model.stats.replace(sub_agent.model.stats)\n        return sub_agent_output\n\n    def run(\n        self,\n        setup_args: dict[str, Any],\n        env: SWEEnv,\n        observation: str | None = None,\n        traj_dir: Path | None = None,\n        return_type: str | None = \"info\",\n        init_model_stats: APIStats | None = None,\n    ):\n        \"\"\"\n        Run the agent on an environment.\n        Return the final value of the specified return type.\n        \"\"\"\n        done = False\n        # mypy checks\n        assert env.container_obj is not None\n        assert env.record is not None\n        assert self.config is not None\n\n        if env.container_obj.id != self.last_container_id:\n            logger.info(f\"Initializing agent settings for container {env.container_obj.id}\")\n            self.init_environment_vars(env)\n            self.last_container_id = env.container_obj.id\n        # Re-initialize primary\n        self.setup(setup_args, init_model_stats)\n\n        for hook in self.hooks:\n            hook.on_run_start()\n\n        # Run action/observation loop\n        trajectory = []\n        info = {}\n        traj_log_path = traj_dir / (env.record[\"instance_id\"] + \".traj\")\n        logger.info(\"Trajectory will be saved to %s\", traj_log_path)\n        while not done:\n            for hook in self.hooks:\n                hook.on_step_start()\n            state = env.communicate(self.state_command) if self.state_command else None\n            thought, action, output = self.forward(observation, env.get_available_actions(), state)\n            for hook in self.hooks:\n                hook.on_actions_generated(thought=thought, action=action, output=output)\n            observations = list()\n            run_action = self._guard_multiline_input(action)\n            for sub_action in self.split_actions(run_action):\n                if sub_action[\"agent\"] == self.name or sub_action[\"cmd_name\"] == self.config.submit_command:\n                    for hook in self.hooks:\n                        hook.on_sub_action_started(sub_action=sub_action)\n                    obs, _, done, info = env.step(sub_action[\"action\"])\n                    for hook in self.hooks:\n                        hook.on_sub_action_executed(obs=obs, done=done)\n                    observations.append(obs)\n                    if sub_action[\"cmd_name\"] == self.config.submit_command:\n                        done = True\n                    if done:\n                        break\n                else:\n                    agent_name = sub_action[\"agent\"]\n                    sub_agent_output = self.call_subroutine(agent_name, sub_action, env)\n                    observations.append(sub_agent_output)\n\n            observation = \"\\n\".join([obs for obs in observations if obs is not None])\n\n            trajectory_step = TrajectoryStep(\n                {\n                    \"action\": action,\n                    \"observation\": observation,\n                    \"response\": output,\n                    \"state\": state,\n                    \"thought\": thought,\n                },\n            )\n            trajectory.append(trajectory_step)\n            model_stats: APIStats = self.model.stats\n            info[\"model_stats\"] = model_stats.to_dict()\n            if traj_dir:\n                self.save_trajectory(trajectory, traj_log_path, env_name=env.name, info=info)\n            for hook in self.hooks:\n                hook.on_step_done(trajectory_step=trajectory_step, model_stats=model_stats)\n\n        for hook in self.hooks:\n            hook.on_run_done()\n\n        logger.info(\"Trajectory saved to %s\", traj_log_path)\n\n        if return_type == \"info\":\n            return info\n        if return_type == \"info_trajectory\":\n            return info, trajectory\n        return trajectory[-1][return_type]\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.local_history","title":"<code>local_history: list[dict[str, str]]</code>  <code>property</code>","text":"<p>Return the history of the agent since the last reset.</p>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.state_command","title":"<code>state_command: str</code>  <code>property</code>","text":"<p>Return the bash command that will be used to extract the environment state.</p>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.check_format_and_requery","title":"<code>check_format_and_requery(output)</code>","text":"<p>Query the model with the current state and observation with the appropriate template.</p> <p>Try to parse the output into a thought and action. Retry if the output is malformatted or the action is blocked.</p> <p>Returns the thought, action, and raw model output.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def check_format_and_requery(\n    self,\n    output: str,\n) -&gt; tuple[str, str, str]:\n    \"\"\"Query the model with the current state and observation with the appropriate template.\n\n    Try to parse the output into a thought and action. Retry if the output is malformatted or the action is blocked.\n\n    Returns the thought, action, and raw model output.\n    \"\"\"\n    # Condition for handling outputs with no thought (just action)\n    if self.model.args.model_name == \"human\":\n        return \"\", output, output\n    elif self.model.args.model_name == \"human_thought\":\n        thought, action = ParseFunction.get(\"ThoughtActionParser\")(\n            output,\n            self.config._commands + self.config.subroutine_types,\n            strict=False,\n        )\n        return thought, action, output\n\n    format_fails = blocklist_fails = 0\n\n    while format_fails + blocklist_fails &lt;= 2:\n        try:\n            thought, action = self.config.parse_function(\n                output,\n                self.config._commands + self.config.subroutine_types,\n                strict=False,\n            )\n        except KeyboardInterrupt:\n            raise\n        except FormatError:\n            format_fails += 1\n            output = self.retry_after_format_fail(output)\n            continue\n        if self.should_block_action(action):\n            blocklist_fails += 1\n            output = self.retry_after_blocklist_fail(output, action)\n        else:\n            return thought, action, output\n    logger.warning(f\"Malformat limit reached: \\n{output}\")\n    return \"Exit due to format error\", \"exit_format\", output\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.forward_model","title":"<code>forward_model(observation, state)</code>","text":"<p>Query the model with the current state and observation with the appropriate template.</p> <p>Returns the model output.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def forward_model(self, observation: str, state: str) -&gt; str:\n    \"\"\"Query the model with the current state and observation with the appropriate template.\n\n    Returns the model output.\"\"\"\n    assert self.config is not None  # mypy\n\n    state_vars = json.loads(state)\n\n    templates: list[str] = []\n    # Determine observation template based on what prior observation was\n    if self.history[-1][\"role\"] == \"system\" or self.history[-1].get(\"is_demo\", False):\n        # Show instance template if prev. obs. was initial system message\n        templates = [self.config.instance_template]\n        if self.config.strategy_template is not None:\n            templates.append(self.config.strategy_template)\n    elif observation is None or observation.strip() == \"\":\n        # Show no output template if observation content was empty\n        templates = [self.config.next_step_no_output_template]\n    else:\n        # Show standard output template if there is observation content\n        templates = [self.config.next_step_template]\n\n    # Populate selected template(s) with information (e.g., issue, arguments, state)\n    messages = []\n    for template in templates:\n        messages.append(\n            template.format(\n                **self.instance_args,\n                **self.system_args,\n                **state_vars,\n                observation=(observation if observation is not None else \"\"),\n            ),\n        )\n\n    message = \"\\n\".join(messages)\n\n    logger.info(f\"\ud83e\udd16 MODEL INPUT\\n{message}\")\n    self._append_history({\"role\": \"user\", \"content\": message, \"agent\": self.name})\n\n    for hook in self.hooks:\n        hook.on_model_query(query=self.local_history, agent=self.name)\n    return self.model.query(self.local_history)\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.forward_with_error_check","title":"<code>forward_with_error_check(observation, state)</code>","text":"<p>Wrapper around <code>self.forward_model</code> that handles errors and retries due to format errors or blocked actions.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def forward_with_error_check(self, observation: str, state: str) -&gt; tuple[str, str, str]:\n    \"\"\"Wrapper around `self.forward_model` that handles errors and retries\n    due to format errors or blocked actions.\n    \"\"\"\n    try:\n        output = self.forward_model(observation, state)\n    except KeyboardInterrupt:\n        raise\n    except RuntimeError as e:\n        logger.warning(f\"Runtime error: {e}\")\n        return (\n            f\"Exit due to runtime error: {e}\",\n            \"exit_error\",\n            f\"exit due to runtime error: {e}\",\n        )\n    except ContextWindowExceededError:\n        logger.warning(\"Context window exceeded\")\n        return \"Exit due to context window\", \"exit_context\", \"Exit due to context window\"\n    except CostLimitExceededError:\n        logger.warning(\"Cost limit exceeded\")\n        return \"Exit due to cost limit\", \"exit_cost\", \"Exit due to cost limit\"\n    except RetryError as e:\n        logger.warning(f\"Retry error: {e}\")\n        return (\n            f\"Exit due to retry error: {e}\",\n            \"exit_api\",\n            f\"exit due to retry error: {e}\",\n        )\n    return self.check_format_and_requery(output)\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.retry_after_blocklist_fail","title":"<code>retry_after_blocklist_fail(output, action)</code>","text":"<p>Ask the model to correct (without committing to persistent history) after a disallowed command</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def retry_after_blocklist_fail(self, output, action):\n    \"\"\"Ask the model to correct (without committing to persistent history) after a disallowed command\"\"\"\n    name = action.strip().split()[0]\n    blocklist_error_message = self.config.blocklist_error_template.format(name=name)\n\n    logger.warning(f\"BLOCKLISTED OUTPUT\\n{output}\")\n    logger.warning(f\"BLOCKLIST ERROR\\n{blocklist_error_message}\")\n\n    temp_history = self.local_history + [\n        {\"role\": \"assistant\", \"content\": output, \"agent\": self.name},\n        {\"role\": \"user\", \"content\": blocklist_error_message, \"agent\": self.name},\n    ]\n    return self.model.query(temp_history)\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.retry_after_format_fail","title":"<code>retry_after_format_fail(output)</code>","text":"<p>Ask the model to correct (without committing to persistent history) after a malformatted model output</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def retry_after_format_fail(self, output):\n    \"\"\"Ask the model to correct (without committing to persistent history) after a malformatted model output\"\"\"\n    format_error_template = self.config.format_error_template\n\n    logger.warning(f\"MALFORMED OUTPUT\\n{output}\")\n    logger.warning(f\"FORMAT ERROR\\n{format_error_template}\")\n\n    temp_history = self.local_history + [\n        {\"role\": \"assistant\", \"content\": output, \"agent\": self.name},\n        {\"role\": \"user\", \"content\": format_error_template, \"agent\": self.name},\n    ]\n    return self.model.query(temp_history)\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.run","title":"<code>run(setup_args, env, observation=None, traj_dir=None, return_type='info', init_model_stats=None)</code>","text":"<p>Run the agent on an environment. Return the final value of the specified return type.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def run(\n    self,\n    setup_args: dict[str, Any],\n    env: SWEEnv,\n    observation: str | None = None,\n    traj_dir: Path | None = None,\n    return_type: str | None = \"info\",\n    init_model_stats: APIStats | None = None,\n):\n    \"\"\"\n    Run the agent on an environment.\n    Return the final value of the specified return type.\n    \"\"\"\n    done = False\n    # mypy checks\n    assert env.container_obj is not None\n    assert env.record is not None\n    assert self.config is not None\n\n    if env.container_obj.id != self.last_container_id:\n        logger.info(f\"Initializing agent settings for container {env.container_obj.id}\")\n        self.init_environment_vars(env)\n        self.last_container_id = env.container_obj.id\n    # Re-initialize primary\n    self.setup(setup_args, init_model_stats)\n\n    for hook in self.hooks:\n        hook.on_run_start()\n\n    # Run action/observation loop\n    trajectory = []\n    info = {}\n    traj_log_path = traj_dir / (env.record[\"instance_id\"] + \".traj\")\n    logger.info(\"Trajectory will be saved to %s\", traj_log_path)\n    while not done:\n        for hook in self.hooks:\n            hook.on_step_start()\n        state = env.communicate(self.state_command) if self.state_command else None\n        thought, action, output = self.forward(observation, env.get_available_actions(), state)\n        for hook in self.hooks:\n            hook.on_actions_generated(thought=thought, action=action, output=output)\n        observations = list()\n        run_action = self._guard_multiline_input(action)\n        for sub_action in self.split_actions(run_action):\n            if sub_action[\"agent\"] == self.name or sub_action[\"cmd_name\"] == self.config.submit_command:\n                for hook in self.hooks:\n                    hook.on_sub_action_started(sub_action=sub_action)\n                obs, _, done, info = env.step(sub_action[\"action\"])\n                for hook in self.hooks:\n                    hook.on_sub_action_executed(obs=obs, done=done)\n                observations.append(obs)\n                if sub_action[\"cmd_name\"] == self.config.submit_command:\n                    done = True\n                if done:\n                    break\n            else:\n                agent_name = sub_action[\"agent\"]\n                sub_agent_output = self.call_subroutine(agent_name, sub_action, env)\n                observations.append(sub_agent_output)\n\n        observation = \"\\n\".join([obs for obs in observations if obs is not None])\n\n        trajectory_step = TrajectoryStep(\n            {\n                \"action\": action,\n                \"observation\": observation,\n                \"response\": output,\n                \"state\": state,\n                \"thought\": thought,\n            },\n        )\n        trajectory.append(trajectory_step)\n        model_stats: APIStats = self.model.stats\n        info[\"model_stats\"] = model_stats.to_dict()\n        if traj_dir:\n            self.save_trajectory(trajectory, traj_log_path, env_name=env.name, info=info)\n        for hook in self.hooks:\n            hook.on_step_done(trajectory_step=trajectory_step, model_stats=model_stats)\n\n    for hook in self.hooks:\n        hook.on_run_done()\n\n    logger.info(\"Trajectory saved to %s\", traj_log_path)\n\n    if return_type == \"info\":\n        return info\n    if return_type == \"info_trajectory\":\n        return info, trajectory\n    return trajectory[-1][return_type]\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.setup","title":"<code>setup(instance_args, init_model_stats=None)</code>","text":"<p>Setup the agent for a new instance.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def setup(self, instance_args, init_model_stats=None) -&gt; None:\n    \"\"\"Setup the agent for a new instance.\"\"\"\n    assert self.config is not None  # mypy\n    self.model.reset_stats(init_model_stats)\n    self.instance_args = instance_args\n\n    system_msg = self.config.system_template.format(**self.system_args)\n    logger.info(f\"SYSTEM ({self.name})\\n{system_msg}\")\n\n    self.history: list[dict[str, Any]] = []\n    self._append_history({\"role\": \"system\", \"content\": system_msg, \"agent\": self.name})\n\n    if \"history_to_messages\" in dir(self.model):\n        for demonstration_path in self.config.demonstrations:\n            if self.config.demonstration_template is None and not self.config.put_demos_in_history:\n                msg = \"Cannot use demonstrations without a demonstration template or put_demos_in_history=True\"\n                raise ValueError(msg)\n\n            # Load history\n            logger.info(f\"DEMONSTRATION: {demonstration_path}\")\n            demo_history = json.loads(Path(demonstration_path).read_text())[\"history\"]\n            demo_history = [\n                entry\n                for entry in demo_history\n                if (\"agent\" not in entry) or (\"agent\" in entry and entry[\"agent\"] == self.name)\n            ]\n\n            if self.config.put_demos_in_history:\n                if self.config.demonstration_template is not None:\n                    logger.warning(\"Demonstration template is ignored for put_demos_in_history=True\")\n                # Add demonstration to history directly as separate messages\n                for entry in demo_history:\n                    if entry[\"role\"] != \"system\":\n                        entry[\"is_demo\"] = True\n                        self._append_history(entry)\n            else:\n                # Add demonstration as single message to history\n                demo_message = self.model.history_to_messages(\n                    demo_history,\n                    is_demonstration=True,\n                )\n                demonstration = self.config.demonstration_template.format(demonstration=demo_message)\n                self._append_history(\n                    {\n                        \"agent\": self.name,\n                        \"content\": demonstration,\n                        \"is_demo\": True,\n                        \"role\": \"user\",\n                    },\n                )\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.should_block_action","title":"<code>should_block_action(action)</code>","text":"<p>Check if the command should be blocked.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def should_block_action(self, action):\n    \"\"\"Check if the command should be blocked.\"\"\"\n    names = action.strip().split()\n    if len(names) == 0:\n        return False\n    name = names[0]\n    if name in self.config.blocklist:\n        return True\n    if name in self.config.blocklist_standalone and name == action.strip():\n        return True\n    return False\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.Agent.split_actions","title":"<code>split_actions(action, pattern_type='subroutine')</code>","text":"<p>Split an action into a list of actions in a greedy manner, each of which is a subroutine call or a single command.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def split_actions(self, action: str, pattern_type=\"subroutine\") -&gt; list[dict[str, Any]]:\n    \"\"\"Split an action into a list of actions in a greedy manner, each of which is a subroutine call or a single command.\"\"\"\n    parsed_action = list()\n    rem_action = action\n    while rem_action.strip():\n        first_match = self._get_first_match(rem_action, pattern_type)\n        if first_match:\n            pre_action = rem_action[: first_match.start()]\n            match_action = rem_action[first_match.start() : first_match.end()]\n            rem_action = rem_action[first_match.end() :]\n            if pre_action.strip():\n                parsed_action.append({\"agent\": self.name, \"action\": pre_action, \"cmd_name\": None})\n            if match_action.strip():\n                if match_action.split()[0] == self.config.submit_command:\n                    parsed_action.append(\n                        {\n                            \"agent\": self.name,\n                            \"action\": match_action,\n                            \"cmd_name\": first_match.group(1),\n                        },\n                    )  # submit command is not a subroutine\n                else:\n                    parsed_action.append(\n                        {\n                            \"agent\": first_match.group(1),\n                            \"args\": first_match.group(2),\n                            \"action\": match_action,\n                            \"cmd_name\": first_match.group(1),\n                        },\n                    )\n        else:\n            parsed_action.append({\"agent\": self.name, \"action\": rem_action, \"cmd_name\": None})\n            rem_action = \"\"\n    return parsed_action\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.AgentArguments","title":"<code>AgentArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>FlattenedAccess</code>, <code>FrozenSerializable</code></p> <p>Configure the agent's behaviour (templates, parse functions, blocklists, ...).</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>@dataclass(frozen=True)\nclass AgentArguments(FlattenedAccess, FrozenSerializable):\n    \"\"\"Configure the agent's behaviour (templates, parse functions, blocklists, ...).\"\"\"\n\n    model: ModelArguments = None\n\n    # Policy can only be set via config yaml file from command line\n    config_file: Path | None = None\n    config: AgentConfig | None = field(default=None, cmd=False)\n\n    def __post_init__(self):\n        if self.config is None and self.config_file is not None:\n            # If unassigned, we load the config from the file to store its contents with the overall arguments\n            config = AgentConfig.load_yaml(self.config_file)\n            object.__setattr__(self, \"config\", config)\n        assert self.config is not None  # mypy\n        for subroutine in getattr(self.config, \"subroutines\", {}).values():\n            model_args = subroutine.model\n            object.__setattr__(\n                model_args,\n                \"per_instance_cost_limit\",\n                self.model.per_instance_cost_limit,\n            )\n            object.__setattr__(model_args, \"total_cost_limit\", self.model.total_cost_limit)\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.AgentHook","title":"<code>AgentHook</code>","text":"Source code in <code>sweagent/agent/agents.py</code> <pre><code>class AgentHook:\n    def on_init(self): ...\n\n    def on_run_start(\n        self,\n    ): ...\n\n    def on_step_start(self): ...\n\n    def on_actions_generated(self, *, thought: str, action: str, output: str): ...\n\n    def on_sub_action_started(self, *, sub_action: str): ...\n\n    def on_sub_action_executed(self, *, obs: str, done: bool): ...\n\n    def on_step_done(self, *, trajectory_step: TrajectoryStep, model_stats: APIStats): ...\n\n    def on_run_done(self): ...\n\n    def on_model_query(self, *, query: str, agent: str):\n        \"\"\"Actually query the model with the complete history.\"\"\"\n\n    def on_query_message_added(\n        self,\n        *,\n        role: str,\n        content: str,\n        agent: str,\n        is_demo: bool = False,\n        thought: str = \"\",\n        action: str = \"\",\n    ): ...\n</code></pre>"},{"location":"reference/agent/#sweagent.agent.agents.AgentHook.on_model_query","title":"<code>on_model_query(*, query, agent)</code>","text":"<p>Actually query the model with the complete history.</p> Source code in <code>sweagent/agent/agents.py</code> <pre><code>def on_model_query(self, *, query: str, agent: str):\n    \"\"\"Actually query the model with the complete history.\"\"\"\n</code></pre>"},{"location":"reference/env/","title":"The environment class","text":""},{"location":"reference/env/#sweagent.environment.swe_env.EnvironmentArguments","title":"<code>EnvironmentArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>FrozenSerializable</code></p> <p>Configure data sources and setup instructions for the environment in which we solve the tasks.</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>@dataclass(frozen=True)\nclass EnvironmentArguments(FrozenSerializable):\n    \"\"\"Configure data sources and setup instructions for the environment in which we solve the tasks.\"\"\"\n\n    # Source of issue statement/problem statement. To run over a batch of issues: Path to a data file\n    # (`json`, `jsonl`) or directory. To run over single issue: github issue url or path to markdown file\n    # with problem statement or problem statement as text prefixed with `text://`.\n    data_path: str\n    # Name of the docker image to use for the environment. Defaults to sweagent/swe-agent:latest\n    image_name: str = \"sweagent/swe-agent:latest\"\n    # When running over SWE-bench issues: Specify the split to use.\n    split: str = \"dev\"\n    # Specify a branch name or a commit hash to checkout before running the task.\n    # Only used when running over a single problem statement/issue.\n    base_commit: str | None = None\n    # Use a persistent container with this name\n    container_name: str | None = None\n    # Try to install the environment before running the task.\n    install_environment: bool = True\n    # No effect, kept for backwards compatibility.\n    timeout: int | None = None\n    # Enable environment logger.\n    verbose: bool = False\n    # Do not use attempt to use a repository mirror from https://github.com/swe-bench.\n    no_mirror: bool = False\n    # Cache task images to speed up task initialization. This means that the environment will be saved as a\n    # docker image for every repository and base commit combination. This uses quite a bit of disk space\n    # but speeds up task initialization significantly when running over multiple issues from the same repository\n    # (or using different models for the same issues).\n    cache_task_images: bool = False\n    # Custom environment setup. Currently only used when data_path points to a single issue.\n    # This needs to be either a string pointing to a yaml file (with yaml, yml file extension)\n    # or a shell script (with sh extension).\n    # See https://github.com/princeton-nlp/SWE-agent/pull/153 for more information\n    environment_setup: str | None = None\n    # Only used when running on single issue. Path to local repository or github repository.\n    repo_path: str = \"\"\n\n    def __post_init__(self):\n        if self.timeout is not None:\n            logger.warning(\"The 'timeout' argument is deprecated and has no effect.\")\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv","title":"<code>SWEEnv</code>","text":"<p>               Bases: <code>Env</code></p> <p>Gym environment for SWE-bench. This class should handle all communication with the docker container.</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>class SWEEnv(gym.Env):\n    \"\"\"Gym environment for SWE-bench. This class should handle all communication with the docker container.\"\"\"\n\n    name = \"swe_main\"\n    cached_image_prefix = \"swe-agent-task-env-\"\n\n    def __init__(self, args: EnvironmentArguments):\n        super().__init__()\n        self.args = args\n        self.base_commit = None\n        self.communicate_output = None\n        self.container_name = args.container_name\n        self.install_environment = args.install_environment\n        self.logger = logger\n        self.persistent = args.container_name is not None\n        self.returncode = None\n        if not self.args.verbose:\n            self.logger.disabled = True\n\n        #: The commit hash of the swe-agent repository\n        self.commit_sha = None\n        try:\n            repo = Repo(REPO_ROOT, search_parent_directories=True)\n            self.commit_sha = repo.head.object.hexsha\n        except KeyboardInterrupt:\n            raise\n        except:\n            logger.warning(\"Failed to get commit hash for this repo\")\n\n        self._github_token: str = keys_config.get(\"GITHUB_TOKEN\", \"\")  # type: ignore\n\n        # Load Task Instances\n        self.data_path = self.args.data_path\n        self.data = get_instances(\n            self.data_path,\n            self.args.base_commit,\n            self.args.split,\n            token=self._github_token,\n            repo_path=self.args.repo_path,\n        )\n        #: Instance we're currently processing. Gets set in self.reset.\n        self.record = None\n        self.logger.info(f\"\ud83d\udcbd Loaded dataset from {self.data_path}\")\n\n        # Establish connection with execution container\n        self.image_name = args.image_name\n        self._reset_container()\n\n        self.idx = 0\n        self.clean_multi_line_functions = lambda x: x\n        self.hooks = []\n\n    def _get_cached_task_image_name(self) -&gt; str:\n        assert self.record is not None\n        inputs: list[str] = [\n            self.record[\"repo\"],\n            self.record[\"base_commit\"],\n            self.args.environment_setup or \"no_setup\",\n        ]\n        tag = hashlib.sha256(\"\".join(inputs).encode()).hexdigest()[:50]\n        return f\"{self.cached_image_prefix}{tag}\"\n\n    def add_hook(self, hook: EnvHook):\n        hook.on_init()\n        self.hooks.append(hook)\n\n    @property\n    def _repo_name(self) -&gt; str:\n        \"\"\"Name of the local copy of the repository\"\"\"\n        assert self.record is not None\n        return self.record[\"repo\"].replace(\"/\", \"__\")\n\n    def _copy_repo(self) -&gt; str:\n        \"\"\"Clone/copy repository/codebase in container\n        Returns:\n            folder name of clone\n        \"\"\"\n        assert self.record is not None  # mypy\n        for hook in self.hooks:\n            hook.on_copy_repo_started(repo_type=self.record[\"repo_type\"], repo_path=self.record[\"repo\"])\n        if self.record[\"repo_type\"] == \"local\":\n            copy_anything_to_container(\n                self.container_obj,\n                self.record[\"repo\"].removeprefix(\"local://\"),\n                \"/\" + self._repo_name,\n            )\n            self.communicate_with_handling(\n                input=f\"chown -R root:root {self._repo_name}\",\n                error_msg=\"Failed to change permissions on copied repository\",\n            )\n            return self._repo_name\n        assert self.record[\"repo_type\"] == \"github\"\n        token_prefix = \"\"\n        if self._github_token:\n            token_prefix = f\"{self._github_token}@\"\n        # fixme: This if statement is brittle and should probably be replaced with better logic\n        if not self.args.no_mirror and self.record[\"problem_statement_source\"] == \"swe-bench\":\n            self.logger.info(f\"{self._repo_name} not found in container, cloning...\")\n            self.communicate_with_handling(\n                input=f\"git clone https://{token_prefix}github.com/swe-bench/{self._repo_name}.git\",\n                error_msg=\"Failed to clone repository from mirror\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n            return self._repo_name\n        else:\n            logger.info(\"Trying to clone from non-mirror...\")\n            self.communicate_with_handling(\n                input=f\"git clone https://{token_prefix}github.com/{self.record['repo']}.git {self._repo_name}\",\n                error_msg=\"Failed to clone repository from non-mirror\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n            return self._repo_name\n\n    def reset(self, index: int | None = None, apply_test_patch: bool = False) -&gt; tuple[str | None, dict]:\n        \"\"\"\n        Function to reset container between each task instance.\n        * Clones instance's repository\n        * Cleans repository of prior modifications\n        * Resets environment variables\n        * Check out base commit\n\n        Arguments:\n            index (`int`) - index of task instance to reset to\n        Returns:\n            observation (`str`) - output from container\n            info (`dict`) - additional information (e.g. debugging information)\n        \"\"\"\n        info = {}\n        info[\"commit_sha\"] = self.commit_sha\n\n        # Get task instance\n        self.idx = index if index is not None else self.idx\n        self.record = self.data[self.idx]\n        self.idx += 1\n\n        # Set query, gold command\n        self.base_commit = self.record[\"base_commit\"]\n        self.query = self.record[\"problem_statement\"]\n        self.reward = None\n\n        ### Reset Container ###\n\n        if self.args.cache_task_images:\n            cached_image = self._get_cached_task_image_name()\n            if image_exists(cached_image):\n                logger.info(f\"Restore environment from cached image {cached_image}\")\n                self.close()  # stop current container\n                self._init_container(cached_image=cached_image)\n                self.communicate(\"export $(xargs &lt;/.env)\")\n                envs = self.communicate(\"env\")\n                logger.debug(f\"Environment variables restored from the image:\\n{envs}\\n\")\n                if apply_test_patch:\n                    self._apply_test_patch()\n                return None, info\n            else:\n                logger.info(f\"Cached image {cached_image} not found, rebuilding task environment...\")\n\n        # Clone repository if not already cloned\n        self.communicate(input=\"cd /\")\n        folders = self.communicate(input=\"ls\").split(\"\\n\")\n        if self._repo_name not in folders:\n            self._copy_repo()\n\n        # Clean repository of any modifications + Checkout base commit\n        for cmd in [\n            \"echo -n &gt; /root/files_to_edit.txt\",\n            f\"cd {self._repo_name}\",\n            \"export ROOT=$(pwd -P)\",\n            \"git status\",\n            \"git restore .\",\n            f\"git reset --hard {self.base_commit}\",\n            \"git clean -fdxq\",\n        ]:\n            self.communicate_with_handling(\n                input=cmd,\n                error_msg=\"Failed to clean repository\",\n            )\n\n        # Reset environment variables\n        for cmd in [\n            'export CURRENT_FILE=\"\"',\n            \"export CURRENT_LINE=0\",\n            \"export SEARCH_RESULTS=()\",\n            \"export SEARCH_FILES=()\",\n            \"export SEARCH_INDEX=0\",\n        ]:\n            self.communicate_with_handling(\n                input=cmd,\n                error_msg=\"Failed to reset environment variables\",\n            )\n\n        # Set up environment\n        self.communicate_with_handling(\n            \"source /root/miniconda3/etc/profile.d/conda.sh\",\n            error_msg=\"Failed to source conda\",\n        )\n\n        system = self.communicate(\"uname -s\").strip().lower()\n        arch = self.communicate(\"uname -m\").strip().lower()\n        if system == \"linux\" and arch == \"x86_64\":\n            self.communicate_with_handling(\n                \"apt update; apt install build-essential -y\",\n                error_msg=\"Failed to install build-essential\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n\n        # Call install environment helper function if specified\n        if self.install_environment:\n            self.install_env()\n        # Install mypy for linting purposes\n        self.communicate_with_handling(\"pip install flake8\", error_msg=\"Failed to install flake8 (lint library)\")\n\n        if self.args.cache_task_images:\n            envs = self.communicate(\"env\")\n            logger.debug(f\"Environment variables to save:\\n{envs}\\n\")\n            self.communicate(\"env &gt;&gt; /.env\")\n            assert self.container_obj is not None  # mypy\n            self.container_obj.commit(cached_image)\n            logger.info(f\"Container with environment {self.container_obj.id} cached as image {cached_image}\")\n\n        if apply_test_patch:\n            self._apply_test_patch()\n        # Write any metadata to info if necessary\n        return None, info\n\n    def _apply_test_patch(self):\n        \"\"\"\n        Apply test patch for oracle setting\n        \"\"\"\n        assert self.record is not None\n        path_to_patch = \"test.patch\"\n        with open(path_to_patch, \"w\") as f:\n            f.write(self.record[\"test_patch\"])\n        subprocess.run(\n            f\"docker cp {path_to_patch} {self.container_name}:/root/test.patch\",\n            shell=True,\n            check=False,\n        )\n        self.communicate_with_handling(\n            input=\"git apply /root/test.patch\",\n            error_msg=\"Failed to apply test patch correctly\",\n        )\n        os.remove(path_to_patch)\n\n    def step(self, action: str) -&gt; tuple[str | None, int, bool, dict]:\n        \"\"\"\n        Runs given action in environment and returns corresponding output\n\n        Args:\n            action (`str`) - command to run in bash shell\n\n        Returns:\n            observation (`str`) - output from container\n            reward (`float`) - value between 0 and 1 quantifying correctness of output + environment state\n            done (`bool`) - whether task is over\n            info (`dict`) - additional information (e.g. debugging information)\n        \"\"\"\n        info = {}\n\n        observation = \"\"\n        # Handle special actions\n        if action.strip() == \"skip\":\n            observation = \"Skipped\"\n            info[\"exit_status\"] = \"skipped\"\n            return observation, 0, True, info\n        if action in {\"exit_context\", \"exit_cost\", \"exit_error\", \"exit_format\", \"exit_api\"}:\n            try:\n                observation = self.communicate(input=\"submit\")\n                submission = self.get_submission(observation)\n                assert submission is not None and submission.strip() != \"\", AssertionError(\"No submission found.\")\n                self.logger.info(f\"Found submission: {submission}\")\n                info[\"exit_status\"] = f\"submitted ({action})\"\n                info[\"submission\"] = submission\n                observation = \"Exited (autosubmitted)\"\n                logger.info(\"Exiting with autosubmission\")\n                return observation, 0, True, info\n            except KeyboardInterrupt:\n                raise\n            except:\n                observation = \"Exited\"\n                info[\"exit_status\"] = action\n                return observation, 0, True, info\n\n        # Attempt to run action in container\n        observation = \"\"\n        try:\n            observation = self.communicate(input=action, timeout_duration=25)\n        except TimeoutError:\n            try:\n                self.interrupt()\n                observation += \"\\nEXECUTION TIMED OUT\"\n            except RuntimeError as e:\n                observation += \"\\nEXECUTION TIMED OUT AND INTERRUPT FAILED. RESTARTING PROCESS.\"\n                info[\"exit_status\"] = \"early_exit\"\n                logger.warning(f\"Failed to interrupt container: {e}\\nRESTARTING PROCESS.\")\n                self.reset_container()\n                return observation, 0, True, info\n        except RuntimeError as e:\n            observation += \"\\nCOMMAND FAILED TO EXECUTE. RESTARTING PROCESS.\"\n            info[\"exit_status\"] = \"early_exit\"\n            logger.warning(f\"Failed to execute command: {e}\\nRESTARTING PROCESS.\")\n            self.reset_container()\n            return observation, 0, True, info\n        except BrokenPipeError as e:\n            observation += \"\\nBROKEN PIPE ERROR. RESTARTING PROCESS.\"\n            info[\"exit_status\"] = \"early_exit\"\n            logger.error(f\"Broken pipe error: {e}\\nRESTARTING PROCESS.\")\n            self.reset_container()\n            return observation, 0, True, info\n        except Exception:\n            observation += \"\\nEXECUTION FAILED OR COMMAND MALFORMED\"\n\n        # Record submission and end episode if `submit` keyword found\n        submission = self.get_submission(observation)\n        if submission is not None:\n            self.logger.info(f\"Found submission: {submission}\")\n            info[\"exit_status\"] = \"submitted\"\n            info[\"submission\"] = submission if submission.strip() != \"\" else None\n            observation = submission if submission.strip() != \"\" else None\n            return observation, 0, True, info\n        return observation, 0, False, info\n\n    def close(self):\n        \"\"\"\n        Handle environment shutdown\n        \"\"\"\n        self.logger.info(\"Beginning environment shutdown...\")\n        try:\n            self.communicate(input=\"exit\")\n        except KeyboardInterrupt:\n            raise\n        except:\n            pass\n        assert self.container is not None\n        assert self.container_obj is not None\n        self.container.terminate()\n        if self.persistent:\n            if self.container_obj.status not in {\"paused\", \"exited\"}:\n                self.container_obj.pause()\n                self.logger.info(\"Agent container paused\")\n            else:\n                self.logger.info(f\"Agent container status: {self.container_obj.status}\")\n        else:\n            try:\n                self.container_obj.remove(force=True)\n            except KeyboardInterrupt:\n                raise\n            except:\n                pass\n            self.logger.info(\"Agent container stopped\")\n        for hook in self.hooks:\n            hook.on_close()\n\n    # MARK: Helper functions #\n\n    def _reset_container(self) -&gt; None:\n        if hasattr(self, \"container\"):\n            try:\n                self.container.terminate()\n            except KeyboardInterrupt:\n                raise\n            except:\n                pass\n        self._init_container()\n        self._init_scripts()\n\n    def reset_container(self) -&gt; None:\n        self.close()\n        self.container = None\n        self.container_obj = None\n        self._reset_container()\n\n    @staticmethod\n    def _get_container_name(image_name: str) -&gt; str:\n        \"\"\"Return name of container\"\"\"\n        process_id = str(os.getpid())\n        current_time = str(datetime.datetime.now())\n        unique_string = current_time + process_id\n        hash_object = hashlib.sha256(unique_string.encode())\n        image_name_sanitized = image_name.replace(\"/\", \"-\")\n        image_name_sanitized = image_name_sanitized.replace(\":\", \"-\")\n        return f\"{image_name_sanitized}-{hash_object.hexdigest()[:10]}\"\n\n    def _init_container(self, cached_image: str | None = None) -&gt; None:\n        \"\"\"\n        Handles container initialization. Defines container name and creates it.\n        If cached_image is provided, it will use that image name instead of the default.\n        \"\"\"\n        image_name = self.image_name\n        if cached_image is not None:\n            image_name = cached_image\n            logger.info(f\"Using cached image: {image_name}\")\n        if self.persistent:\n            assert self.container_name is not None\n        else:\n            # Make sure that we get a new container name just in case removing didn't work.\n            # Might be a fix for https://github.com/princeton-nlp/SWE-agent/issues/451\n            self.container_name = self._get_container_name(image_name)\n        self.container, self.parent_pids = get_container(self.container_name, image_name, persistent=self.persistent)\n        try:\n            client = docker.from_env(timeout=600)\n        except docker.errors.DockerException as e:\n            if \"Error while fetching server API version\" in str(e):\n                msg = \"Docker is not running. Please start Docker and try again.\"\n                raise RuntimeError(msg) from e\n        try:\n            self.container_obj = client.containers.get(self.container_name)\n        except docker.errors.NotFound:\n            logger.debug(\"Couldn't find container. Let's wait and retry.\")\n            time.sleep(3)\n            self.container_obj = client.containers.get(self.container_name)\n        self.logger.info(\"\ud83c\udf31 Environment Initialized\")\n\n    def _init_scripts(self):\n        \"\"\"\n        Initialize custom commands within container\n        \"\"\"\n        self.communicate_with_handling(\n            \"source /root/.bashrc\",\n            error_msg=\"Failed to source .bashrc\",\n        )\n        self.communicate_with_handling(\n            \"mkdir -p /root/commands\",\n            error_msg=\"Failed to create commands directory\",\n        )\n        self.communicate_with_handling(\n            \"touch /root/commands/__init__.py\",\n            error_msg=\"Failed to create __init__.py\",\n        )\n        self.communicate_with_handling(\n            \"export PATH=$PATH:/root/commands\",\n            error_msg=\"Failed to add commands directory to PATH\",\n        )\n\n    def _communicate_experimental(\n        self,\n        input: str,\n        timeout_duration=25,\n    ) -&gt; str:\n        \"\"\"Experimental version of `_communicate`\"\"\"\n\n        command_suffix = f\"echo {PROCESS_DONE_MARKER_START}$?{PROCESS_DONE_MARKER_END}\\n\"\n        try:\n            self.returncode = None\n            cmd = input if input.endswith(\"\\n\") else input + \"\\n\"\n            cmd += command_suffix\n            os.write(self.container.stdin.fileno(), cmd.encode())\n            time.sleep(0.03)\n            self.container.stdin.flush()\n        except BrokenPipeError:\n            traceback.print_exc()\n            self.logger.error(\"Failed to communicate with container. Check docker logs for more information.\")\n            msg = \"Failed to communicate with container\"\n            raise RuntimeError(msg)\n\n        buffer, exit_code = read_with_timeout_experimental(self.container, timeout_duration)\n        self.returncode = int(exit_code)\n        return buffer\n\n    def _communicate(\n        self,\n        input: str,\n        timeout_duration=25,\n    ) -&gt; str:\n        if \"SWE_AGENT_EXPERIMENTAL_COMMUNICATE\" in keys_config:\n            return self._communicate_experimental(input, timeout_duration)\n        try:\n            self.returncode = None\n            cmd = input if input.endswith(\"\\n\") else input + \"\\n\"\n            os.write(self.container.stdin.fileno(), cmd.encode())\n            time.sleep(0.1)\n            self.container.stdin.flush()\n        except BrokenPipeError:\n            traceback.print_exc()\n            self.logger.error(\"Failed to communicate with container. Check docker logs for more information.\")\n            msg = \"Failed to communicate with container\"\n            raise RuntimeError(msg)\n        try:\n            buffer = read_with_timeout(self.container, self.get_pids, timeout_duration)\n            self.container.stdin.write(\"echo $?\\n\")\n            time.sleep(0.1)\n            self.container.stdin.flush()\n            exit_code = read_with_timeout(self.container, self.get_pids, 5).strip()\n        except Exception as e:\n            self.logger.error(f\"Read with timeout failed on input:\\n---\\n{input}\\n---\")\n            raise e\n        if not exit_code.isdigit():\n            msg = f\"Container crashed. Failed to get exit code. Output:\\n---\\n{buffer}\\n---\"\n            raise RuntimeError(msg)\n        self.returncode = int(exit_code)\n        return buffer\n\n    def _check_syntax(self, input: str):\n        \"\"\"\n        Saves environment variables to file\n        \"\"\"\n        output = self._communicate(f\"/bin/bash -n &lt;&lt;'EOF'\\n{input}\\nEOF\\n\")\n        return output, self.returncode == 0\n\n    def communicate(\n        self,\n        input: str,\n        timeout_duration=25,\n    ) -&gt; str:\n        \"\"\"\n        Sends input to container and returns output\n\n        Args:\n            input (`str`) - input to send to container\n\n        Returns:\n            output (`str`) - output from container\n        \"\"\"\n        if input.strip() != \"exit\":\n            output, valid = self._check_syntax(input)\n            if not valid:\n                return output  # shows syntax errors\n            output = self._communicate(\n                input,\n                timeout_duration=timeout_duration,\n            )\n            self.communicate_output = output\n            return output\n        else:\n            self.container.terminate()\n            self.returncode = 0\n            self.communicate_output = \"\"\n            return \"\"\n\n    def communicate_with_handling(self, input: str, error_msg: str, timeout_duration=25) -&gt; str:\n        \"\"\"\n        Wrapper for communicate function that raises error if return code is non-zero\n        \"\"\"\n        logs = self.communicate(input, timeout_duration=timeout_duration)\n        if self.returncode != 0:\n            self.logger.error(f\"{error_msg}: {logs}\")\n            self.close()\n            msg = f\"{error_msg}: {logs}\"\n            raise RuntimeError(msg)\n        return logs\n\n    def get_available_actions(self) -&gt; list[str]:\n        \"\"\"\n        Returns list of available actions in current environment state\n        \"\"\"\n        return []\n\n    def get_pids(self, all_pids=False) -&gt; list[str]:\n        \"\"\"\n        Gets list of processes running inside docker container\n        \"\"\"\n        pids = self.container_obj.exec_run(\"ps -eo pid,comm --no-headers\").output.decode().split(\"\\n\")\n        pids = [x.split() for x in pids if x]\n        if not all_pids:\n            pids = [x for x in pids if x[1] != \"ps\" and x[0] not in self.parent_pids]\n        return pids\n\n    def get_submission(self, output: str) -&gt; str:\n        \"\"\"\n        Function for extracting diff patch submission at the end of an episode.\n\n        Args:\n            output (`str`) - `submit` observation\n        Returns:\n            submission (`str`) - diff patch submission\n        \"\"\"\n        pattern = r\"\\&lt;\\&lt;SUBMISSION\\|\\|(.*)\\|\\|SUBMISSION\\&gt;\\&gt;\"\n        match = re.search(pattern, output, re.DOTALL)\n        if match is None:\n            return None\n        return match.group(1)\n\n    def run_shell_script(self, script_path: Path, *, location: str) -&gt; None:\n        \"\"\"Run custom script supplied by user at `script_path`\n\n        Args:\n            location: location of script file 'host' or 'container'\n        \"\"\"\n        if location == \"host\":\n            return self._run_shell_script_host(script_path)\n        elif location == \"container\":\n            raise NotImplementedError\n        msg = f\"Invalid 'location': {location}\"\n        raise ValueError(msg)\n\n    def _run_shell_script_host(self, script_path: Path) -&gt; None:\n        \"\"\"Run shell script file (located on host) in container\"\"\"\n        if not script_path.is_file():\n            msg = f\"Script not found at {script_path}\"\n            raise FileNotFoundError(msg)\n        shell_commands = Path(script_path).read_text().splitlines()\n        for i, cmd in enumerate(shell_commands):\n            self.communicate_with_handling(\n                cmd,\n                error_msg=f\"Failed to execute line {i}.\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n\n    def install_env(self) -&gt; None:\n        \"\"\"\n        Creates conda environment and installs third party dependencies to allow code execution\n        \"\"\"\n        assert self.record is not None  # mypy\n        if (\n            self.record[\"problem_statement_source\"] != \"swe-bench\" or self.record[\"repo_type\"] == \"local\"\n        ) and self.args.environment_setup is None:\n            logger.warning(\n                \"install_environment is set to True, but the data path is a GitHub URL \"\n                \"without an environment config file (environment_config key/flag). \"\n                \"Skipping conda environment installation.\",\n            )\n            return\n        for hook in self.hooks:\n            hook.on_install_env_started()\n        if self.args.environment_setup is not None:\n            assert isinstance(self.args.environment_setup, (str, os.PathLike))\n            if Path(self.args.environment_setup).suffix in [\".yml\", \".yaml\"]:\n                try:\n                    install_configs = yaml.safe_load(Path(self.args.environment_setup).read_text())\n                except Exception as e:\n                    msg = \"Environment config file needs to be a yaml file\"\n                    raise ValueError(msg) from e\n            elif Path(self.args.environment_setup).suffix == \".sh\":\n                self.run_shell_script(Path(self.args.environment_setup), location=\"host\")\n                return\n            else:\n                msg = \"Environment config file needs to be a yaml file or shell script\"\n                raise ValueError(msg)\n        else:\n            try:\n                install_configs = MAP_VERSION_TO_INSTALL[self.record[\"repo\"]][str(self.record[\"version\"])]\n            except KeyError as e:\n                msg = (\n                    \"Tried to look up install configs in swe-bench, but failed. \"\n                    \"You can set a custom environment config with the environment_config key/flag.\"\n                )\n                raise ValueError(msg) from e\n        # Create environment if does not exist yet\n        env_name = f\"{self._repo_name}__{self.record['version']}\"\n        env_check = self.communicate(f\"conda env list | grep {env_name}\", timeout_duration=LONG_TIMEOUT)\n        if env_check.strip() == \"\":\n            self.logger.info(f\"{env_name} conda env not found, creating...\")\n            packages = install_configs.get(\"packages\", \"\")\n            if packages == \"requirements.txt\":\n                # Create conda environment\n                self.communicate_with_handling(\n                    f\"conda create -n {env_name} python={install_configs['python']} -y\",\n                    error_msg=\"Failed to create conda environment\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n                # Write reqs to requirements.txt in docker container\n                content_reqs = get_requirements(self.record)\n                copy_file_to_container(self.container_obj, content_reqs, PATH_TO_REQS)\n                # Create conda environment + install reqs\n                self.communicate_with_handling(\n                    f\"conda activate {env_name}\",\n                    error_msg=\"Failed to activate conda environment\",\n                )\n                self.communicate_with_handling(\n                    f\"pip install -r {PATH_TO_REQS}\",\n                    error_msg=\"Failed to install requirements.txt\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n                self.communicate(f\"rm {PATH_TO_REQS}\")\n            elif packages == \"environment.yml\":\n                # Write environment.yml to file\n                if install_configs.get(\"no_use_env\", False):\n                    content_env_yml = get_environment_yml(self.record, env_name)\n                else:\n                    content_env_yml = get_environment_yml(\n                        self.record,\n                        env_name,\n                        python_version=install_configs[\"python\"],\n                    )\n                copy_file_to_container(self.container_obj, content_env_yml, PATH_TO_ENV_YML)\n                if install_configs.get(\"no_use_env\", False):\n                    # Create conda environment\n                    self.communicate_with_handling(\n                        f\"conda create -c conda-forge -n {env_name} python={install_configs['python']} -y\",\n                        error_msg=\"Failed to create conda environment\",\n                        timeout_duration=LONG_TIMEOUT,\n                    )\n                    # Install packages\n                    self.communicate_with_handling(\n                        f\"conda env update -f {PATH_TO_ENV_YML}\",\n                        error_msg=\"Failed to install environment.yml\",\n                        timeout_duration=LONG_TIMEOUT,\n                    )\n                else:\n                    # Create environment + install packages\n                    self.communicate_with_handling(\n                        f\"conda env create --file {PATH_TO_ENV_YML}\",\n                        error_msg=\"Failed to create conda environment with environment.yml\",\n                        timeout_duration=LONG_TIMEOUT,\n                    )\n                self.communicate(f\"rm {PATH_TO_ENV_YML}\")\n            else:\n                # Create environment + install packages\n                self.communicate_with_handling(\n                    f\"conda create -n {env_name} python={install_configs['python']} {packages} -y\",\n                    error_msg=\"Failed to create conda environment\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n            # Install extra pip packages if specified\n            if install_configs.get(\"pip_packages\", False):\n                self.communicate_with_handling(\n                    f\"source activate {env_name} &amp;&amp; pip install {' '.join(install_configs['pip_packages'])}\",\n                    error_msg=\"Failed to install pip packages\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n\n        # Activate environment\n        self.communicate_with_handling(f\"conda activate {env_name}\", error_msg=\"Failed to activate conda environment\")\n\n        # Install repo at base commit\n        if install_configs.get(\"pre_install\", False):\n            self.logger.info(\"Running pre-install commands...\")\n            for pre_install_cmd in install_configs[\"pre_install\"]:\n                self.communicate_with_handling(\n                    pre_install_cmd,\n                    error_msg=\"Pre-install commands failed to execute successfully\",\n                )\n        self.logger.info(f\"Installing {self._repo_name} at base commit...\")\n        if install_configs.get(\"install\", False):\n            install_cmd = install_configs[\"install\"]\n            self.communicate_with_handling(\n                install_cmd,\n                error_msg=\"Install command failed to execute successfully\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n        if install_configs.get(\"post_install\", False):\n            self.logger.info(\"Running post-install commands...\")\n            for post_install_cmd in install_configs[\"post_install\"]:\n                self.communicate_with_handling(\n                    post_install_cmd,\n                    error_msg=\"Post-install commands failed to execute successfully\",\n                )\n\n    def add_commands(self, commands: list[dict]) -&gt; None:\n        \"\"\"\n        Adds custom commands to container\n        \"\"\"\n        for command in commands:\n            name = command[\"name\"]\n            contents = command[\"contents\"]\n            copy_file_to_container(self.container_obj, contents, f\"/root/commands/{name}\")\n            if command[\"type\"] == \"source_file\":\n                self.communicate_with_handling(\n                    f\"source /root/commands/{name}\",\n                    error_msg=(\n                        f\"Failed to source {name}. If you meant to make a script,\"\n                        \" start the file with a shebang (e.g. #!/usr/bin/env python).\"\n                    ),\n                )\n            elif command[\"type\"] == \"script\":\n                self.communicate_with_handling(\n                    f\"chmod +x /root/commands/{name}\",\n                    error_msg=f\"Failed to chmod {name}\",\n                )\n            elif command[\"type\"] == \"utility\":\n                # nothing to do for utility scripts\n                pass\n            else:\n                msg = f\"Invalid command type: {command['type']}\"\n                raise ValueError(msg)\n\n    def interrupt(self):\n        \"\"\"\n        Send interrupt signal to container and exhaust stdout buffer with a communicate call\n        \"\"\"\n        pids = self.get_pids()\n        for pid, cmd in pids:\n            if pid not in self.parent_pids and cmd != \"ps\":\n                self.container_obj.exec_run(f\"kill -9 {pid}\")\n        try:\n            _ = read_with_timeout(self.container, self.get_pids, 20)\n        except TimeoutError:\n            pass\n        try:\n            output = self.communicate(input=\"echo 'interrupted'\", timeout_duration=5)\n            assert output.strip().endswith(\"interrupted\"), \"container health check failed\"\n        except TimeoutError:\n            msg = \"Failed to interrupt container\"\n            raise RuntimeError(msg)\n\n    def open_pr(self, *, trajectory, _dry_run: bool = False):\n        \"\"\"Create PR to repository\n\n        Args:\n            trajectory: Trajectory of actions taken by the agent\n            _dry_run: Whether to actually push anything or just simulate it\n        \"\"\"\n        logger.info(\"Opening PR\")\n        # TODO: have better way of handling this\n        # Adding random string suffix to avoid name conflicts if we had a previously failed run\n        issue_url = self.args.data_path\n        try:\n            issue = get_gh_issue_data(issue_url, token=self._github_token)\n        except InvalidGithubURL as e:\n            msg = \"Data path must be a github issue URL if --open_pr is set.\"\n            raise ValueError(msg) from e\n        branch_name = f\"swe-agent-fix-#{issue.number}-\" + str(random.random())[2:10]\n\n        self.communicate_with_handling(\n            input=\"rm -f model.patch\",\n            error_msg=\"Failed to remove model patch\",\n            timeout_duration=10,\n        )\n        self.communicate_with_handling(\n            input=f\"git checkout -b {branch_name}\",\n            error_msg=\"Failed to switch to new branch\",\n            timeout_duration=10,\n        )\n        self.communicate_with_handling(\n            input=\"git add .\",\n            error_msg=\"Failed to add commits\",\n            timeout_duration=10,\n        )\n        dry_run_flag = \"--allow-empty\" if _dry_run else \"\"\n        self.communicate_with_handling(\n            input=f\"git commit -m 'Fix: {issue.title}' -m 'Closes #{issue.number}' {dry_run_flag}\",\n            error_msg=\"Failed to commit changes\",\n            timeout_duration=10,\n        )\n\n        owner, repo, _ = parse_gh_issue_url(issue_url)\n        # If `--repo_path` was specified with a different github URL, then the record will contain\n        # the forking user\n        assert self.record is not None\n        if self.record[\"repo_type\"] != \"github\":\n            # We already validated that `--data_path` is a github issue URL\n            # so this is the only case where we can reach here\n            msg = \"--repo_path must point to a github URL if --open_pr is set\"\n            raise ValueError(msg)\n        forker, _ = self.record[\"repo\"].split(\"/\")\n        head = branch_name\n        remote = \"origin\"\n        if forker != owner:\n            head = f\"{forker}:{branch_name}\"\n            token_prefix = \"\"\n            if self._github_token:\n                token_prefix = f\"{self._github_token}@\"\n            fork_url = f\"https://{token_prefix}github.com/{forker}/{repo}.git\"\n            logger.debug(f\"Using fork: {fork_url}\")\n            self.communicate_with_handling(\n                input=f\"git remote add fork {fork_url}\",\n                error_msg=\"Failed to create new git remote\",\n                timeout_duration=10,\n            )\n            remote = \"fork\"\n        dry_run_prefix = \"echo \" if _dry_run else \"\"\n        self.communicate_with_handling(\n            input=f\"{dry_run_prefix} git push {remote} {branch_name}\",\n            error_msg=(\n                \"Failed to push branch to remote. Please check your token and permissions. \"\n                \"You might want to push to a fork with the push_gh_repo_url option.\"\n            ),\n            timeout_duration=10,\n        )\n        body = (\n            f\"This is a PR opened by AI tool [SWE Agent](https://github.com/princeton-nlp/SWE-agent/) \"\n            f\"to close [#{issue.number}]({issue_url}) ({issue.title}).\\n\\nCloses #{issue.number}.\"\n        )\n        body += \"\\n\\n\" + format_trajectory_markdown(trajectory)\n        api = GhApi(token=self._github_token)\n        if not _dry_run:\n            pr_info = api.pulls.create(\n                owner=owner,\n                repo=repo,\n                title=f\"SWE-agent[bot] PR to fix: {issue.title}\",\n                head=head,\n                base=\"main\",\n                body=body,\n                draft=True,\n            )\n            logger.info(\n                f\"\ud83c\udf89 PR created as a draft at {pr_info.html_url}. Please review it carefully, push \"\n                \"any required changes onto the branch and then click \"\n                \"'Ready for Review' to bring it to the attention of the maintainers.\",\n            )\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.add_commands","title":"<code>add_commands(commands)</code>","text":"<p>Adds custom commands to container</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def add_commands(self, commands: list[dict]) -&gt; None:\n    \"\"\"\n    Adds custom commands to container\n    \"\"\"\n    for command in commands:\n        name = command[\"name\"]\n        contents = command[\"contents\"]\n        copy_file_to_container(self.container_obj, contents, f\"/root/commands/{name}\")\n        if command[\"type\"] == \"source_file\":\n            self.communicate_with_handling(\n                f\"source /root/commands/{name}\",\n                error_msg=(\n                    f\"Failed to source {name}. If you meant to make a script,\"\n                    \" start the file with a shebang (e.g. #!/usr/bin/env python).\"\n                ),\n            )\n        elif command[\"type\"] == \"script\":\n            self.communicate_with_handling(\n                f\"chmod +x /root/commands/{name}\",\n                error_msg=f\"Failed to chmod {name}\",\n            )\n        elif command[\"type\"] == \"utility\":\n            # nothing to do for utility scripts\n            pass\n        else:\n            msg = f\"Invalid command type: {command['type']}\"\n            raise ValueError(msg)\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.close","title":"<code>close()</code>","text":"<p>Handle environment shutdown</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def close(self):\n    \"\"\"\n    Handle environment shutdown\n    \"\"\"\n    self.logger.info(\"Beginning environment shutdown...\")\n    try:\n        self.communicate(input=\"exit\")\n    except KeyboardInterrupt:\n        raise\n    except:\n        pass\n    assert self.container is not None\n    assert self.container_obj is not None\n    self.container.terminate()\n    if self.persistent:\n        if self.container_obj.status not in {\"paused\", \"exited\"}:\n            self.container_obj.pause()\n            self.logger.info(\"Agent container paused\")\n        else:\n            self.logger.info(f\"Agent container status: {self.container_obj.status}\")\n    else:\n        try:\n            self.container_obj.remove(force=True)\n        except KeyboardInterrupt:\n            raise\n        except:\n            pass\n        self.logger.info(\"Agent container stopped\")\n    for hook in self.hooks:\n        hook.on_close()\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.communicate","title":"<code>communicate(input, timeout_duration=25)</code>","text":"<p>Sends input to container and returns output</p> <p>Returns:</p> Type Description <code>str</code> <p>output (<code>str</code>) - output from container</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def communicate(\n    self,\n    input: str,\n    timeout_duration=25,\n) -&gt; str:\n    \"\"\"\n    Sends input to container and returns output\n\n    Args:\n        input (`str`) - input to send to container\n\n    Returns:\n        output (`str`) - output from container\n    \"\"\"\n    if input.strip() != \"exit\":\n        output, valid = self._check_syntax(input)\n        if not valid:\n            return output  # shows syntax errors\n        output = self._communicate(\n            input,\n            timeout_duration=timeout_duration,\n        )\n        self.communicate_output = output\n        return output\n    else:\n        self.container.terminate()\n        self.returncode = 0\n        self.communicate_output = \"\"\n        return \"\"\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.communicate_with_handling","title":"<code>communicate_with_handling(input, error_msg, timeout_duration=25)</code>","text":"<p>Wrapper for communicate function that raises error if return code is non-zero</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def communicate_with_handling(self, input: str, error_msg: str, timeout_duration=25) -&gt; str:\n    \"\"\"\n    Wrapper for communicate function that raises error if return code is non-zero\n    \"\"\"\n    logs = self.communicate(input, timeout_duration=timeout_duration)\n    if self.returncode != 0:\n        self.logger.error(f\"{error_msg}: {logs}\")\n        self.close()\n        msg = f\"{error_msg}: {logs}\"\n        raise RuntimeError(msg)\n    return logs\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.get_available_actions","title":"<code>get_available_actions()</code>","text":"<p>Returns list of available actions in current environment state</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def get_available_actions(self) -&gt; list[str]:\n    \"\"\"\n    Returns list of available actions in current environment state\n    \"\"\"\n    return []\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.get_pids","title":"<code>get_pids(all_pids=False)</code>","text":"<p>Gets list of processes running inside docker container</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def get_pids(self, all_pids=False) -&gt; list[str]:\n    \"\"\"\n    Gets list of processes running inside docker container\n    \"\"\"\n    pids = self.container_obj.exec_run(\"ps -eo pid,comm --no-headers\").output.decode().split(\"\\n\")\n    pids = [x.split() for x in pids if x]\n    if not all_pids:\n        pids = [x for x in pids if x[1] != \"ps\" and x[0] not in self.parent_pids]\n    return pids\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.get_submission","title":"<code>get_submission(output)</code>","text":"<p>Function for extracting diff patch submission at the end of an episode.</p> <p>Returns:     submission (<code>str</code>) - diff patch submission</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def get_submission(self, output: str) -&gt; str:\n    \"\"\"\n    Function for extracting diff patch submission at the end of an episode.\n\n    Args:\n        output (`str`) - `submit` observation\n    Returns:\n        submission (`str`) - diff patch submission\n    \"\"\"\n    pattern = r\"\\&lt;\\&lt;SUBMISSION\\|\\|(.*)\\|\\|SUBMISSION\\&gt;\\&gt;\"\n    match = re.search(pattern, output, re.DOTALL)\n    if match is None:\n        return None\n    return match.group(1)\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.install_env","title":"<code>install_env()</code>","text":"<p>Creates conda environment and installs third party dependencies to allow code execution</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def install_env(self) -&gt; None:\n    \"\"\"\n    Creates conda environment and installs third party dependencies to allow code execution\n    \"\"\"\n    assert self.record is not None  # mypy\n    if (\n        self.record[\"problem_statement_source\"] != \"swe-bench\" or self.record[\"repo_type\"] == \"local\"\n    ) and self.args.environment_setup is None:\n        logger.warning(\n            \"install_environment is set to True, but the data path is a GitHub URL \"\n            \"without an environment config file (environment_config key/flag). \"\n            \"Skipping conda environment installation.\",\n        )\n        return\n    for hook in self.hooks:\n        hook.on_install_env_started()\n    if self.args.environment_setup is not None:\n        assert isinstance(self.args.environment_setup, (str, os.PathLike))\n        if Path(self.args.environment_setup).suffix in [\".yml\", \".yaml\"]:\n            try:\n                install_configs = yaml.safe_load(Path(self.args.environment_setup).read_text())\n            except Exception as e:\n                msg = \"Environment config file needs to be a yaml file\"\n                raise ValueError(msg) from e\n        elif Path(self.args.environment_setup).suffix == \".sh\":\n            self.run_shell_script(Path(self.args.environment_setup), location=\"host\")\n            return\n        else:\n            msg = \"Environment config file needs to be a yaml file or shell script\"\n            raise ValueError(msg)\n    else:\n        try:\n            install_configs = MAP_VERSION_TO_INSTALL[self.record[\"repo\"]][str(self.record[\"version\"])]\n        except KeyError as e:\n            msg = (\n                \"Tried to look up install configs in swe-bench, but failed. \"\n                \"You can set a custom environment config with the environment_config key/flag.\"\n            )\n            raise ValueError(msg) from e\n    # Create environment if does not exist yet\n    env_name = f\"{self._repo_name}__{self.record['version']}\"\n    env_check = self.communicate(f\"conda env list | grep {env_name}\", timeout_duration=LONG_TIMEOUT)\n    if env_check.strip() == \"\":\n        self.logger.info(f\"{env_name} conda env not found, creating...\")\n        packages = install_configs.get(\"packages\", \"\")\n        if packages == \"requirements.txt\":\n            # Create conda environment\n            self.communicate_with_handling(\n                f\"conda create -n {env_name} python={install_configs['python']} -y\",\n                error_msg=\"Failed to create conda environment\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n            # Write reqs to requirements.txt in docker container\n            content_reqs = get_requirements(self.record)\n            copy_file_to_container(self.container_obj, content_reqs, PATH_TO_REQS)\n            # Create conda environment + install reqs\n            self.communicate_with_handling(\n                f\"conda activate {env_name}\",\n                error_msg=\"Failed to activate conda environment\",\n            )\n            self.communicate_with_handling(\n                f\"pip install -r {PATH_TO_REQS}\",\n                error_msg=\"Failed to install requirements.txt\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n            self.communicate(f\"rm {PATH_TO_REQS}\")\n        elif packages == \"environment.yml\":\n            # Write environment.yml to file\n            if install_configs.get(\"no_use_env\", False):\n                content_env_yml = get_environment_yml(self.record, env_name)\n            else:\n                content_env_yml = get_environment_yml(\n                    self.record,\n                    env_name,\n                    python_version=install_configs[\"python\"],\n                )\n            copy_file_to_container(self.container_obj, content_env_yml, PATH_TO_ENV_YML)\n            if install_configs.get(\"no_use_env\", False):\n                # Create conda environment\n                self.communicate_with_handling(\n                    f\"conda create -c conda-forge -n {env_name} python={install_configs['python']} -y\",\n                    error_msg=\"Failed to create conda environment\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n                # Install packages\n                self.communicate_with_handling(\n                    f\"conda env update -f {PATH_TO_ENV_YML}\",\n                    error_msg=\"Failed to install environment.yml\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n            else:\n                # Create environment + install packages\n                self.communicate_with_handling(\n                    f\"conda env create --file {PATH_TO_ENV_YML}\",\n                    error_msg=\"Failed to create conda environment with environment.yml\",\n                    timeout_duration=LONG_TIMEOUT,\n                )\n            self.communicate(f\"rm {PATH_TO_ENV_YML}\")\n        else:\n            # Create environment + install packages\n            self.communicate_with_handling(\n                f\"conda create -n {env_name} python={install_configs['python']} {packages} -y\",\n                error_msg=\"Failed to create conda environment\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n        # Install extra pip packages if specified\n        if install_configs.get(\"pip_packages\", False):\n            self.communicate_with_handling(\n                f\"source activate {env_name} &amp;&amp; pip install {' '.join(install_configs['pip_packages'])}\",\n                error_msg=\"Failed to install pip packages\",\n                timeout_duration=LONG_TIMEOUT,\n            )\n\n    # Activate environment\n    self.communicate_with_handling(f\"conda activate {env_name}\", error_msg=\"Failed to activate conda environment\")\n\n    # Install repo at base commit\n    if install_configs.get(\"pre_install\", False):\n        self.logger.info(\"Running pre-install commands...\")\n        for pre_install_cmd in install_configs[\"pre_install\"]:\n            self.communicate_with_handling(\n                pre_install_cmd,\n                error_msg=\"Pre-install commands failed to execute successfully\",\n            )\n    self.logger.info(f\"Installing {self._repo_name} at base commit...\")\n    if install_configs.get(\"install\", False):\n        install_cmd = install_configs[\"install\"]\n        self.communicate_with_handling(\n            install_cmd,\n            error_msg=\"Install command failed to execute successfully\",\n            timeout_duration=LONG_TIMEOUT,\n        )\n    if install_configs.get(\"post_install\", False):\n        self.logger.info(\"Running post-install commands...\")\n        for post_install_cmd in install_configs[\"post_install\"]:\n            self.communicate_with_handling(\n                post_install_cmd,\n                error_msg=\"Post-install commands failed to execute successfully\",\n            )\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.interrupt","title":"<code>interrupt()</code>","text":"<p>Send interrupt signal to container and exhaust stdout buffer with a communicate call</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def interrupt(self):\n    \"\"\"\n    Send interrupt signal to container and exhaust stdout buffer with a communicate call\n    \"\"\"\n    pids = self.get_pids()\n    for pid, cmd in pids:\n        if pid not in self.parent_pids and cmd != \"ps\":\n            self.container_obj.exec_run(f\"kill -9 {pid}\")\n    try:\n        _ = read_with_timeout(self.container, self.get_pids, 20)\n    except TimeoutError:\n        pass\n    try:\n        output = self.communicate(input=\"echo 'interrupted'\", timeout_duration=5)\n        assert output.strip().endswith(\"interrupted\"), \"container health check failed\"\n    except TimeoutError:\n        msg = \"Failed to interrupt container\"\n        raise RuntimeError(msg)\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.open_pr","title":"<code>open_pr(*, trajectory, _dry_run=False)</code>","text":"<p>Create PR to repository</p> <p>Parameters:</p> Name Type Description Default <code>trajectory</code> <p>Trajectory of actions taken by the agent</p> required <code>_dry_run</code> <code>bool</code> <p>Whether to actually push anything or just simulate it</p> <code>False</code> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def open_pr(self, *, trajectory, _dry_run: bool = False):\n    \"\"\"Create PR to repository\n\n    Args:\n        trajectory: Trajectory of actions taken by the agent\n        _dry_run: Whether to actually push anything or just simulate it\n    \"\"\"\n    logger.info(\"Opening PR\")\n    # TODO: have better way of handling this\n    # Adding random string suffix to avoid name conflicts if we had a previously failed run\n    issue_url = self.args.data_path\n    try:\n        issue = get_gh_issue_data(issue_url, token=self._github_token)\n    except InvalidGithubURL as e:\n        msg = \"Data path must be a github issue URL if --open_pr is set.\"\n        raise ValueError(msg) from e\n    branch_name = f\"swe-agent-fix-#{issue.number}-\" + str(random.random())[2:10]\n\n    self.communicate_with_handling(\n        input=\"rm -f model.patch\",\n        error_msg=\"Failed to remove model patch\",\n        timeout_duration=10,\n    )\n    self.communicate_with_handling(\n        input=f\"git checkout -b {branch_name}\",\n        error_msg=\"Failed to switch to new branch\",\n        timeout_duration=10,\n    )\n    self.communicate_with_handling(\n        input=\"git add .\",\n        error_msg=\"Failed to add commits\",\n        timeout_duration=10,\n    )\n    dry_run_flag = \"--allow-empty\" if _dry_run else \"\"\n    self.communicate_with_handling(\n        input=f\"git commit -m 'Fix: {issue.title}' -m 'Closes #{issue.number}' {dry_run_flag}\",\n        error_msg=\"Failed to commit changes\",\n        timeout_duration=10,\n    )\n\n    owner, repo, _ = parse_gh_issue_url(issue_url)\n    # If `--repo_path` was specified with a different github URL, then the record will contain\n    # the forking user\n    assert self.record is not None\n    if self.record[\"repo_type\"] != \"github\":\n        # We already validated that `--data_path` is a github issue URL\n        # so this is the only case where we can reach here\n        msg = \"--repo_path must point to a github URL if --open_pr is set\"\n        raise ValueError(msg)\n    forker, _ = self.record[\"repo\"].split(\"/\")\n    head = branch_name\n    remote = \"origin\"\n    if forker != owner:\n        head = f\"{forker}:{branch_name}\"\n        token_prefix = \"\"\n        if self._github_token:\n            token_prefix = f\"{self._github_token}@\"\n        fork_url = f\"https://{token_prefix}github.com/{forker}/{repo}.git\"\n        logger.debug(f\"Using fork: {fork_url}\")\n        self.communicate_with_handling(\n            input=f\"git remote add fork {fork_url}\",\n            error_msg=\"Failed to create new git remote\",\n            timeout_duration=10,\n        )\n        remote = \"fork\"\n    dry_run_prefix = \"echo \" if _dry_run else \"\"\n    self.communicate_with_handling(\n        input=f\"{dry_run_prefix} git push {remote} {branch_name}\",\n        error_msg=(\n            \"Failed to push branch to remote. Please check your token and permissions. \"\n            \"You might want to push to a fork with the push_gh_repo_url option.\"\n        ),\n        timeout_duration=10,\n    )\n    body = (\n        f\"This is a PR opened by AI tool [SWE Agent](https://github.com/princeton-nlp/SWE-agent/) \"\n        f\"to close [#{issue.number}]({issue_url}) ({issue.title}).\\n\\nCloses #{issue.number}.\"\n    )\n    body += \"\\n\\n\" + format_trajectory_markdown(trajectory)\n    api = GhApi(token=self._github_token)\n    if not _dry_run:\n        pr_info = api.pulls.create(\n            owner=owner,\n            repo=repo,\n            title=f\"SWE-agent[bot] PR to fix: {issue.title}\",\n            head=head,\n            base=\"main\",\n            body=body,\n            draft=True,\n        )\n        logger.info(\n            f\"\ud83c\udf89 PR created as a draft at {pr_info.html_url}. Please review it carefully, push \"\n            \"any required changes onto the branch and then click \"\n            \"'Ready for Review' to bring it to the attention of the maintainers.\",\n        )\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.reset","title":"<code>reset(index=None, apply_test_patch=False)</code>","text":"<p>Function to reset container between each task instance. * Clones instance's repository * Cleans repository of prior modifications * Resets environment variables * Check out base commit</p> <p>Returns:     observation (<code>str</code>) - output from container     info (<code>dict</code>) - additional information (e.g. debugging information)</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def reset(self, index: int | None = None, apply_test_patch: bool = False) -&gt; tuple[str | None, dict]:\n    \"\"\"\n    Function to reset container between each task instance.\n    * Clones instance's repository\n    * Cleans repository of prior modifications\n    * Resets environment variables\n    * Check out base commit\n\n    Arguments:\n        index (`int`) - index of task instance to reset to\n    Returns:\n        observation (`str`) - output from container\n        info (`dict`) - additional information (e.g. debugging information)\n    \"\"\"\n    info = {}\n    info[\"commit_sha\"] = self.commit_sha\n\n    # Get task instance\n    self.idx = index if index is not None else self.idx\n    self.record = self.data[self.idx]\n    self.idx += 1\n\n    # Set query, gold command\n    self.base_commit = self.record[\"base_commit\"]\n    self.query = self.record[\"problem_statement\"]\n    self.reward = None\n\n    ### Reset Container ###\n\n    if self.args.cache_task_images:\n        cached_image = self._get_cached_task_image_name()\n        if image_exists(cached_image):\n            logger.info(f\"Restore environment from cached image {cached_image}\")\n            self.close()  # stop current container\n            self._init_container(cached_image=cached_image)\n            self.communicate(\"export $(xargs &lt;/.env)\")\n            envs = self.communicate(\"env\")\n            logger.debug(f\"Environment variables restored from the image:\\n{envs}\\n\")\n            if apply_test_patch:\n                self._apply_test_patch()\n            return None, info\n        else:\n            logger.info(f\"Cached image {cached_image} not found, rebuilding task environment...\")\n\n    # Clone repository if not already cloned\n    self.communicate(input=\"cd /\")\n    folders = self.communicate(input=\"ls\").split(\"\\n\")\n    if self._repo_name not in folders:\n        self._copy_repo()\n\n    # Clean repository of any modifications + Checkout base commit\n    for cmd in [\n        \"echo -n &gt; /root/files_to_edit.txt\",\n        f\"cd {self._repo_name}\",\n        \"export ROOT=$(pwd -P)\",\n        \"git status\",\n        \"git restore .\",\n        f\"git reset --hard {self.base_commit}\",\n        \"git clean -fdxq\",\n    ]:\n        self.communicate_with_handling(\n            input=cmd,\n            error_msg=\"Failed to clean repository\",\n        )\n\n    # Reset environment variables\n    for cmd in [\n        'export CURRENT_FILE=\"\"',\n        \"export CURRENT_LINE=0\",\n        \"export SEARCH_RESULTS=()\",\n        \"export SEARCH_FILES=()\",\n        \"export SEARCH_INDEX=0\",\n    ]:\n        self.communicate_with_handling(\n            input=cmd,\n            error_msg=\"Failed to reset environment variables\",\n        )\n\n    # Set up environment\n    self.communicate_with_handling(\n        \"source /root/miniconda3/etc/profile.d/conda.sh\",\n        error_msg=\"Failed to source conda\",\n    )\n\n    system = self.communicate(\"uname -s\").strip().lower()\n    arch = self.communicate(\"uname -m\").strip().lower()\n    if system == \"linux\" and arch == \"x86_64\":\n        self.communicate_with_handling(\n            \"apt update; apt install build-essential -y\",\n            error_msg=\"Failed to install build-essential\",\n            timeout_duration=LONG_TIMEOUT,\n        )\n\n    # Call install environment helper function if specified\n    if self.install_environment:\n        self.install_env()\n    # Install mypy for linting purposes\n    self.communicate_with_handling(\"pip install flake8\", error_msg=\"Failed to install flake8 (lint library)\")\n\n    if self.args.cache_task_images:\n        envs = self.communicate(\"env\")\n        logger.debug(f\"Environment variables to save:\\n{envs}\\n\")\n        self.communicate(\"env &gt;&gt; /.env\")\n        assert self.container_obj is not None  # mypy\n        self.container_obj.commit(cached_image)\n        logger.info(f\"Container with environment {self.container_obj.id} cached as image {cached_image}\")\n\n    if apply_test_patch:\n        self._apply_test_patch()\n    # Write any metadata to info if necessary\n    return None, info\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.run_shell_script","title":"<code>run_shell_script(script_path, *, location)</code>","text":"<p>Run custom script supplied by user at <code>script_path</code></p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>location of script file 'host' or 'container'</p> required Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def run_shell_script(self, script_path: Path, *, location: str) -&gt; None:\n    \"\"\"Run custom script supplied by user at `script_path`\n\n    Args:\n        location: location of script file 'host' or 'container'\n    \"\"\"\n    if location == \"host\":\n        return self._run_shell_script_host(script_path)\n    elif location == \"container\":\n        raise NotImplementedError\n    msg = f\"Invalid 'location': {location}\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"reference/env/#sweagent.environment.swe_env.SWEEnv.step","title":"<code>step(action)</code>","text":"<p>Runs given action in environment and returns corresponding output</p> <p>Returns:</p> Type Description <code>str | None</code> <p>observation (<code>str</code>) - output from container</p> <code>int</code> <p>reward (<code>float</code>) - value between 0 and 1 quantifying correctness of output + environment state</p> <code>bool</code> <p>done (<code>bool</code>) - whether task is over</p> <code>dict</code> <p>info (<code>dict</code>) - additional information (e.g. debugging information)</p> Source code in <code>sweagent/environment/swe_env.py</code> <pre><code>def step(self, action: str) -&gt; tuple[str | None, int, bool, dict]:\n    \"\"\"\n    Runs given action in environment and returns corresponding output\n\n    Args:\n        action (`str`) - command to run in bash shell\n\n    Returns:\n        observation (`str`) - output from container\n        reward (`float`) - value between 0 and 1 quantifying correctness of output + environment state\n        done (`bool`) - whether task is over\n        info (`dict`) - additional information (e.g. debugging information)\n    \"\"\"\n    info = {}\n\n    observation = \"\"\n    # Handle special actions\n    if action.strip() == \"skip\":\n        observation = \"Skipped\"\n        info[\"exit_status\"] = \"skipped\"\n        return observation, 0, True, info\n    if action in {\"exit_context\", \"exit_cost\", \"exit_error\", \"exit_format\", \"exit_api\"}:\n        try:\n            observation = self.communicate(input=\"submit\")\n            submission = self.get_submission(observation)\n            assert submission is not None and submission.strip() != \"\", AssertionError(\"No submission found.\")\n            self.logger.info(f\"Found submission: {submission}\")\n            info[\"exit_status\"] = f\"submitted ({action})\"\n            info[\"submission\"] = submission\n            observation = \"Exited (autosubmitted)\"\n            logger.info(\"Exiting with autosubmission\")\n            return observation, 0, True, info\n        except KeyboardInterrupt:\n            raise\n        except:\n            observation = \"Exited\"\n            info[\"exit_status\"] = action\n            return observation, 0, True, info\n\n    # Attempt to run action in container\n    observation = \"\"\n    try:\n        observation = self.communicate(input=action, timeout_duration=25)\n    except TimeoutError:\n        try:\n            self.interrupt()\n            observation += \"\\nEXECUTION TIMED OUT\"\n        except RuntimeError as e:\n            observation += \"\\nEXECUTION TIMED OUT AND INTERRUPT FAILED. RESTARTING PROCESS.\"\n            info[\"exit_status\"] = \"early_exit\"\n            logger.warning(f\"Failed to interrupt container: {e}\\nRESTARTING PROCESS.\")\n            self.reset_container()\n            return observation, 0, True, info\n    except RuntimeError as e:\n        observation += \"\\nCOMMAND FAILED TO EXECUTE. RESTARTING PROCESS.\"\n        info[\"exit_status\"] = \"early_exit\"\n        logger.warning(f\"Failed to execute command: {e}\\nRESTARTING PROCESS.\")\n        self.reset_container()\n        return observation, 0, True, info\n    except BrokenPipeError as e:\n        observation += \"\\nBROKEN PIPE ERROR. RESTARTING PROCESS.\"\n        info[\"exit_status\"] = \"early_exit\"\n        logger.error(f\"Broken pipe error: {e}\\nRESTARTING PROCESS.\")\n        self.reset_container()\n        return observation, 0, True, info\n    except Exception:\n        observation += \"\\nEXECUTION FAILED OR COMMAND MALFORMED\"\n\n    # Record submission and end episode if `submit` keyword found\n    submission = self.get_submission(observation)\n    if submission is not None:\n        self.logger.info(f\"Found submission: {submission}\")\n        info[\"exit_status\"] = \"submitted\"\n        info[\"submission\"] = submission if submission.strip() != \"\" else None\n        observation = submission if submission.strip() != \"\" else None\n        return observation, 0, True, info\n    return observation, 0, False, info\n</code></pre>"},{"location":"reference/env_utils/","title":"Environment utils","text":""},{"location":"reference/env_utils/#sweagent.environment.utils.InstanceBuilder","title":"<code>InstanceBuilder</code>","text":"Source code in <code>sweagent/environment/utils.py</code> <pre><code>class InstanceBuilder:\n    def __init__(self, token: str | None = None):\n        \"\"\"This helper class is used to build the data for an instance object,\n        retrieving problem statements from github issues or local files and setting\n        repo paths from github urls or local paths.\n        \"\"\"\n        # Args that will be passed to the Instance constructor\n        self.args = {}\n        self.token = token\n        self._instance_id_problem_suffix = \"\"\n\n    def set_problem_statement_from_gh_issue(self, issue_url: str):\n        owner, repo, issue_number = parse_gh_issue_url(issue_url)\n        self.args[\"problem_statement\"] = get_problem_statement_from_github_issue(\n            owner,\n            repo,\n            issue_number,\n            token=self.token,\n        )\n        self.args[\"instance_id\"] = f\"{owner}__{repo}-i{issue_number}\"\n        self.args[\"problem_statement_source\"] = \"online\"\n\n    def set_problem_statement_from_file(self, file_path: str):\n        self.set_problem_statement_from_text(Path(file_path).read_text())\n\n    def set_problem_statement_from_text(self, text: str):\n        self.args[\"problem_statement\"] = text\n        self.args[\"instance_id\"] = hashlib.sha256(self.args[\"problem_statement\"].encode()).hexdigest()[:6]\n        self.args[\"problem_statement_source\"] = \"local\"\n\n    def set_problem_statement(self, data_path: str):\n        \"\"\"Get problem statement for a single instance from a github issue url or a\n        path to a markdown or text file.\n        \"\"\"\n        if data_path.startswith(\"text://\"):\n            return self.set_problem_statement_from_text(data_path.removeprefix(\"text://\"))\n        if is_github_issue_url(data_path):\n            return self.set_problem_statement_from_gh_issue(data_path)\n        if Path(data_path).is_file():\n            return self.set_problem_statement_from_file(data_path)\n        msg = f\"Not sure how to get problem statement from {data_path=}.\"\n        raise ValueError(msg)\n\n    def set_repo_info_from_gh_url(self, url: str, base_commit: str | None = None):\n        owner, repo = parse_gh_repo_url(url)\n        self.args[\"repo\"] = f\"{owner}/{repo}\"\n        self.args[\"repo_type\"] = \"github\"\n        # Always get commit hash, because base_commit can also be branch or tag\n        api = GhApi(token=self.token)\n        self.args[\"base_commit\"] = get_commit(api, owner, repo, ref=base_commit).sha\n        if base_commit != self.args[\"base_commit\"]:\n            logger.info(f\"Base commit reference {base_commit} resolved to commit hash {self.args['base_commit']}\")\n        self.args[\"version\"] = self.args[\"base_commit\"][:7]\n\n    def set_repo_info_from_local_path(self, path: str, base_commit: str | None = None):\n        self.args[\"repo\"] = str(Path(path).resolve())\n        self.args[\"repo_type\"] = \"local\"\n        if base_commit:\n            self.args[\"base_commit\"] = base_commit\n        else:\n            try:\n                repo = Repo(path, search_parent_directories=True)\n            except InvalidGitRepositoryError as e:\n                msg = f\"Could not find git repository at {path=}.\"\n                raise ValueError(msg) from e\n            if repo.is_dirty():\n                msg = f\"Local git repository {path} is dirty. Please commit or stash changes.\"\n                raise ValueError(msg)\n            self.args[\"base_commit\"] = repo.head.object.hexsha\n        self.args[\"version\"] = self.args[\"base_commit\"][:7]\n\n    def set_repo_info(self, repo: str, base_commit: str | None = None):\n        if is_github_repo_url(repo):\n            self.set_repo_info_from_gh_url(repo, base_commit=base_commit)\n        elif Path(repo).is_dir():\n            self.set_repo_info_from_local_path(repo, base_commit=base_commit)\n        else:\n            msg = f\"Could not determine repo path from {repo=}.\"\n            raise ValueError(msg)\n\n    def set_from_dict(self, instance_dict: dict[str, Any]):\n        self.args |= instance_dict\n\n    def set_missing_fields(self):\n        # TODO: This field is only needed while swe_env is using some questionable logic\n        # to determine whether to clone from a mirror or not. This should be removed in the future.\n        # Values: 'swe-bench' (loaded from json/jsonl for swe-bench style inference),\n        # 'online' (loaded from github issue or similar) or 'local' (loaded from local file)\n        if \"problem_statement_source\" not in self.args:\n            self.args[\"problem_statement_source\"] = \"swe-bench\"\n        if \"repo_type\" not in self.args:\n            self.args[\"repo_type\"] = \"github\"\n\n    def validate(self):\n        required_fields = [\n            \"problem_statement\",\n            \"instance_id\",\n            \"repo\",\n            \"repo_type\",\n            \"base_commit\",\n            \"version\",\n            \"problem_statement_source\",\n        ]\n        if not all(x in self.args for x in required_fields):\n            missing = set(required_fields) - set(self.args.keys())\n            msg = f\"Missing required fields: {missing=}\"\n            raise ValueError(msg)\n        if self.args[\"repo_type\"] not in {\"github\", \"local\"}:\n            msg = f\"Invalid repo type: {self.args['repo_type']=}\"\n            raise ValueError(msg)\n        if self.args[\"repo_type\"] == \"github\" and self.args[\"repo\"].count(\"/\") != 1:\n            msg = f\"Invalid repo format for {self.args['repo_type']=}: {self.args['repo']=}\"\n            raise ValueError(msg)\n\n    def build(self) -&gt; dict[str, Any]:\n        self.set_missing_fields()\n        self.validate()\n        return self.args\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.InstanceBuilder.__init__","title":"<code>__init__(token=None)</code>","text":"<p>This helper class is used to build the data for an instance object, retrieving problem statements from github issues or local files and setting repo paths from github urls or local paths.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def __init__(self, token: str | None = None):\n    \"\"\"This helper class is used to build the data for an instance object,\n    retrieving problem statements from github issues or local files and setting\n    repo paths from github urls or local paths.\n    \"\"\"\n    # Args that will be passed to the Instance constructor\n    self.args = {}\n    self.token = token\n    self._instance_id_problem_suffix = \"\"\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.InstanceBuilder.set_problem_statement","title":"<code>set_problem_statement(data_path)</code>","text":"<p>Get problem statement for a single instance from a github issue url or a path to a markdown or text file.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def set_problem_statement(self, data_path: str):\n    \"\"\"Get problem statement for a single instance from a github issue url or a\n    path to a markdown or text file.\n    \"\"\"\n    if data_path.startswith(\"text://\"):\n        return self.set_problem_statement_from_text(data_path.removeprefix(\"text://\"))\n    if is_github_issue_url(data_path):\n        return self.set_problem_statement_from_gh_issue(data_path)\n    if Path(data_path).is_file():\n        return self.set_problem_statement_from_file(data_path)\n    msg = f\"Not sure how to get problem statement from {data_path=}.\"\n    raise ValueError(msg)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.copy_anything_to_container","title":"<code>copy_anything_to_container(container, host_path, container_path)</code>","text":"<p>Copy files or directories from host to container</p> <p>Note: Will need to set ownership on the copied files in the container.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def copy_anything_to_container(container, host_path: str, container_path: str) -&gt; None:\n    \"\"\"Copy files or directories from host to container\n\n    Note: Will need to set ownership on the copied files in the container.\n    \"\"\"\n    if not Path(host_path).exists():\n        msg = f\"Path {host_path} does not exist, cannot copy it to container.\"\n        raise FileNotFoundError(msg)\n    cmd = [\"docker\", \"cp\", host_path, f\"{container.id}:{container_path}\"]\n    logger.debug(f\"Copying {host_path} to container at {container_path} with command: {shlex.join(cmd)}\")\n    try:\n        subprocess.run(cmd, check=True)\n    except subprocess.CalledProcessError as e:\n        msg = f\"Error copying {host_path} to container at {container_path}: {e}\"\n        raise RuntimeError(msg) from e\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.copy_file_to_container","title":"<code>copy_file_to_container(container, contents, container_path)</code>","text":"<p>Copies a given string into a Docker container at a specified path.</p> <p>Args: - container: Docker SDK container object. - contents: The string to copy into the container. - container_path: The path inside the container where the string should be copied to.</p> <p>Returns: - None</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def copy_file_to_container(container, contents, container_path):\n    \"\"\"\n    Copies a given string into a Docker container at a specified path.\n\n    Args:\n    - container: Docker SDK container object.\n    - contents: The string to copy into the container.\n    - container_path: The path inside the container where the string should be copied to.\n\n    Returns:\n    - None\n    \"\"\"\n    temp_file_name = None\n\n    try:\n        # Create a temporary file\n        with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n            temp_file_name = temp_file.name\n            # Write the string to the temporary file and ensure it's written to disk\n            temp_file.write(contents.encode(\"utf-8\"))\n            temp_file.flush()\n            os.fsync(temp_file.fileno())\n\n        # Create a TAR archive in memory containing the temporary file\n        with tempfile.NamedTemporaryFile():\n            with open(temp_file_name, \"rb\") as temp_file:\n                # Prepare the TAR archive\n                with BytesIO() as tar_stream:\n                    with tarfile.open(fileobj=tar_stream, mode=\"w\") as tar:\n                        tar_info = tarfile.TarInfo(name=os.path.basename(container_path))\n                        tar_info.size = os.path.getsize(temp_file_name)\n                        tar.addfile(tarinfo=tar_info, fileobj=temp_file)\n                    tar_stream.seek(0)\n                    # Copy the TAR stream to the container\n                    container.put_archive(path=os.path.dirname(container_path), data=tar_stream.read())\n\n    except Exception as e:\n        logger.error(f\"An error occurred: {e}\")\n        logger.error(traceback.format_exc())\n    finally:\n        # Cleanup: Remove the temporary file if it was created\n        if temp_file_name and os.path.exists(temp_file_name):\n            os.remove(temp_file_name)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.format_trajectory_markdown","title":"<code>format_trajectory_markdown(trajectory)</code>","text":"<p>Format a trajectory as a markdown string for use in gh PR description.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def format_trajectory_markdown(trajectory: list[dict[str, str]]):\n    \"\"\"Format a trajectory as a markdown string for use in gh PR description.\"\"\"\n    prefix = [\n        \"&lt;details&gt;\",\n        \"&lt;summary&gt;Thought process ('trajectory') of SWE-agent (click to expand)&lt;/summary&gt;\",\n        \"\",\n        \"\",\n    ]\n    steps = []\n    for i, step in enumerate(trajectory):\n        step_strs = []\n        for key, value in step.items():\n            emoji = _MARKDOWN_TRAJECTORY_EMOJI_MAPPING.get(key, \"\")\n            if emoji:\n                emoji += \" \"\n            step_strs.append(f\"**{emoji}{key.capitalize()} ({i})**:\")\n            if key in [\"observation\", \"state\", \"action\"]:\n                step_strs.append(\"```\")\n                step_strs.append(remove_triple_backticks(value).strip())\n                step_strs.append(\"```\")\n            else:\n                step_strs.append(value.strip())\n        steps.append(\"\\n\".join(step_strs))\n    suffix = [\n        \"\",\n        \"&lt;/details&gt;\",\n    ]\n    return \"\\n\".join(prefix) + \"\\n\\n---\\n\\n\".join(steps) + \"\\n\".join(suffix)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_associated_commit_urls","title":"<code>get_associated_commit_urls(org, repo, issue_number, *, token='')</code>","text":"<p>Return the URLs of commits that would close an issue.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_associated_commit_urls(org: str, repo: str, issue_number: str, *, token: str = \"\") -&gt; list[str]:\n    \"\"\"Return the URLs of commits that would close an issue.\"\"\"\n    api = GhApi(token=token)\n    # Strangely the \"pull_request\" field of api.issues.get is often not set\n    # so we have to go through the events to check if there's a commit\n    events = api.issues.list_events(org, repo, issue_number)\n    commit_urls = []\n    for event in events:\n        if event.event != \"referenced\":\n            continue\n        if not event.commit_id:\n            continue\n        commit = api.repos.get_commit(org, repo, event.commit_id)\n        message = commit.commit.message\n        if f\"fixes #{issue_number}\" in message.lower() or f\"closes #{issue_number}\" in message.lower():\n            commit_urls.append(commit.html_url)\n    return commit_urls\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_commit","title":"<code>get_commit(api, owner, repo, ref=None)</code>","text":"<p>Get commit object from github api</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>GhApi</code> required <code>owner</code> <code>str</code> <p>Repo owner, e.g., \"princeton-nlp\"</p> required <code>repo</code> <code>str</code> <p>Repo, e.g., \"SWE-agent\"</p> required <code>ref</code> <code>str</code> <p>Branch, tag or commit hash</p> <code>None</code> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_commit(api: GhApi, owner: str, repo: str, ref: str | None = None):\n    \"\"\"Get commit object from github api\n\n    Args:\n        api (GhApi):\n        owner (str): Repo owner, e.g., \"princeton-nlp\"\n        repo (str): Repo, e.g., \"SWE-agent\"\n        ref (str, optional): Branch, tag or commit hash\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    if ref:\n        return api.repos.get_commit(owner, repo, ref)\n    return api.repos.list_commits(owner, repo)[0]\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_container","title":"<code>get_container(ctr_name, image_name, persistent=False)</code>","text":"<p>Get a container object for a given container name and image name</p> <p>Parameters:</p> Name Type Description Default <code>ctr_name</code> <code>str</code> <p>Name of container</p> required <code>image_name</code> <code>str</code> <p>Name of image</p> required <code>persistent</code> <code>bool</code> <p>Whether to use a persistent container or not</p> <code>False</code> <p>Returns:     Container object</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_container(ctr_name: str, image_name: str, persistent: bool = False) -&gt; tuple[subprocess.Popen, set]:\n    \"\"\"\n    Get a container object for a given container name and image name\n\n    Arguments:\n        ctr_name (str): Name of container\n        image_name (str): Name of image\n        persistent (bool): Whether to use a persistent container or not\n    Returns:\n        Container object\n    \"\"\"\n    if not image_exists(image_name):\n        msg = (\n            f\"Image {image_name} not found. Please ensure it is built and available. \"\n            \"Please double-check that you followed all installation/setup instructions from the \"\n            \"readme.\"\n        )\n        raise RuntimeError(msg)\n\n    if persistent:\n        return _get_persistent_container(ctr_name, image_name)\n    else:\n        return _get_non_persistent_container(ctr_name, image_name)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_data_path_name","title":"<code>get_data_path_name(data_path)</code>","text":"<p>if data_path is a file, return the file stem elif it's a github url, return the owner__repo_name</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_data_path_name(data_path: str) -&gt; str:\n    \"\"\"if data_path is a file, return the file stem\n    elif it's a github url, return the owner__repo_name\n    \"\"\"\n    if data_path.startswith(\"text://\"):\n        return hashlib.sha256(data_path.removeprefix(\"text://\").encode()).hexdigest()[:6]\n    match = GITHUB_ISSUE_URL_PATTERN.search(data_path)\n    if match:\n        owner, repo, _ = match.groups()\n        return f\"{owner}__{repo}\"\n    return Path(data_path).stem\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_gh_issue_data","title":"<code>get_gh_issue_data(issue_url, *, token='')</code>","text":"<p>Returns github issue data in the form of a dictionary. See https://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#get-an-issue for return format</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_gh_issue_data(issue_url: str, *, token: str = \"\"):\n    \"\"\"Returns github issue data in the form of a dictionary.\n    See https://docs.github.com/en/rest/issues/issues?apiVersion=2022-11-28#get-an-issue\n    for return format\n    \"\"\"\n    owner, repo, issue_number = parse_gh_issue_url(issue_url)\n    api = GhApi(token=token)\n    return api.issues.get(owner, repo, issue_number)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_instances","title":"<code>get_instances(file_path, base_commit=None, split=None, token=None, *, repo_path='')</code>","text":"<p>Getter function for handling json, jsonl files</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to file</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>List of instances as dictionaries</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_instances(\n    file_path: str,\n    base_commit: str | None = None,\n    split: str | None = None,\n    token: str | None = None,\n    *,\n    repo_path: str = \"\",\n) -&gt; list[dict[str, Any]]:\n    \"\"\"\n    Getter function for handling json, jsonl files\n\n    Args:\n        file_path (str): Path to file\n\n    Returns:\n        List of instances as dictionaries\n    \"\"\"\n\n    def instance_from_dict(instances):\n        ib = InstanceBuilder(token=token)\n        ib.set_from_dict(instances)\n        return ib.build()\n\n    def postproc_instance_list(instances):\n        if isinstance(instances, dict):\n            msg = \"Expected a list of instances, got a dictionary.\"\n            raise ValueError(msg)\n        return [instance_from_dict(x) for x in instances]\n\n    # The next if statement is very brittle logic to determine if we're processing a single instance\n    if (\n        file_path.startswith(\"text://\")\n        or (Path(file_path).is_file() and Path(file_path).suffix in [\".md\", \".txt\"])\n        or is_github_issue_url(file_path)\n    ):\n        ib = InstanceBuilder(token=token)\n        ib.set_problem_statement(file_path)\n        if repo_path:\n            ib.set_repo_info(repo_path, base_commit=base_commit)\n        elif is_github_repo_url(file_path):\n            ib.set_repo_info_from_gh_url(file_path, base_commit=base_commit)\n        else:\n            msg = f\"Could not determine repo path from {file_path=}, {repo_path=}\"\n            raise ValueError(msg)\n\n        return [ib.build()]\n\n    if base_commit:\n        msg = \"base_commit must be empty if running over multiple problem statements\"\n        raise ValueError(msg)\n\n    if repo_path:\n        msg = \"repo_path must be empty if running over multiple problem statements\"\n        raise ValueError(msg)\n\n    # If file_path is a directory, attempt load from disk\n    if os.path.isdir(file_path):\n        try:\n            dataset_or_dict = load_from_disk(file_path)\n            if isinstance(dataset_or_dict, dict):\n                return postproc_instance_list(dataset_or_dict[split])\n            return postproc_instance_list(dataset_or_dict)\n        except FileNotFoundError:\n            # Raised by load_from_disk if the directory is not a dataset directory\n            pass\n\n    # The next if statement is very brittle logic to determine if we're processing a single instance\n    if (\n        (Path(file_path).is_file() and Path(file_path).suffix in [\".md\", \".txt\"])\n        or is_github_issue_url(file_path)\n        or file_path.startswith(\"text://\")\n    ):\n        ib = InstanceBuilder(token=token)\n        ib.set_problem_statement(file_path)\n        if repo_path:\n            ib.set_repo_info(repo_path, base_commit=base_commit)\n        elif is_github_repo_url(file_path):\n            ib.set_repo_info_from_gh_url(file_path)\n        else:\n            msg = f\"Could not determine repo path from {file_path=}, {repo_path=}\"\n            raise ValueError(msg)\n\n        return [ib.build()]\n\n    if base_commit is not None:\n        msg = \"base_commit must be None if data_path is not a github issue url\"\n        raise ValueError(msg)\n\n    # If file_path is a file, load the file\n    if file_path.endswith(\".json\"):\n        return postproc_instance_list(json.load(open(file_path)))\n    if file_path.endswith(\".jsonl\"):\n        return postproc_instance_list([json.loads(x) for x in open(file_path).readlines()])\n\n    # Attempt load from HF datasets as a last resort\n    try:\n        return postproc_instance_list(load_dataset(file_path, split=split))\n    except:\n        msg = (\n            f\"Could not load instances from {file_path}. \"\n            \"Please ensure --data_path is a GitHub URL, a SWE-bench HuggingFace dataset, or a JSON/JSONL file.\"\n        )\n        raise ValueError(msg)\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.get_problem_statement_from_github_issue","title":"<code>get_problem_statement_from_github_issue(owner, repo, issue_number, *, token='')</code>","text":"<p>Return problem statement from github issue</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def get_problem_statement_from_github_issue(owner: str, repo: str, issue_number: str, *, token: str | None = \"\") -&gt; str:\n    \"\"\"Return problem statement from github issue\"\"\"\n    api = GhApi(token=token)\n    issue = api.issues.get(owner, repo, issue_number)\n    title = issue.title if issue.title else \"\"\n    body = issue.body if issue.body else \"\"\n    return f\"{title}\\n{body}\\n\"\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.image_exists","title":"<code>image_exists(image_name)</code>","text":"<p>Check that the image exists and give some better error messages.</p> <p>Parameters:</p> Name Type Description Default <code>image_name</code> <code>str</code> <p>Name of image</p> required <p>Returns:     bool: True if image exists</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def image_exists(image_name):\n    \"\"\"\n    Check that the image exists and give some better error messages.\n\n    Arguments:\n        image_name (str): Name of image\n    Returns:\n        bool: True if image exists\n    \"\"\"\n    try:\n        client = docker.from_env()\n    except docker.errors.DockerException as e:\n        docker_not_running = any(\n            (\n                \"connection aborted\" in str(e).lower(),\n                \"connection refused\" in str(e).lower(),\n                \"error while fetching server api version\" in str(e).lower(),\n            ),\n        )\n        if docker_not_running:\n            msg = (\n                \"Probably the Docker daemon is not running. Please start the Docker daemon and try again. \"\n                \"You might need to allow the use of the docker socket \"\n                \"(https://github.com/princeton-nlp/SWE-agent/issues/159) or symlink the socket \"\n                \"if it's at a non-standard location \"\n                \"(https://github.com/princeton-nlp/SWE-agent/issues/20#issuecomment-2047506005).\"\n            )\n            raise RuntimeError(msg) from e\n        raise\n    filterred_images = client.images.list(filters={\"reference\": image_name})\n    if len(filterred_images) == 0:\n        return False\n    elif len(filterred_images) &gt; 1:\n        RuntimeError(f\"Multiple images found for {image_name}, that's weird.\")\n    attrs = filterred_images[0].attrs\n    if attrs is not None:\n        logger.info(\n            f\"Found image {image_name} with tags: {attrs['RepoTags']}, created: {attrs['Created']} \"\n            f\"for {attrs['Os']} {attrs['Architecture']}.\",\n        )\n    return True\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.is_github_issue_url","title":"<code>is_github_issue_url(data_path)</code>","text":"<p>Check if data_path is an URL pointing to a github issue</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def is_github_issue_url(data_path: str) -&gt; bool:\n    \"\"\"Check if data_path is an URL pointing to a github issue\"\"\"\n    return GITHUB_ISSUE_URL_PATTERN.search(data_path) is not None\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.is_github_repo_url","title":"<code>is_github_repo_url(data_path)</code>","text":"<p>Check if data_path is an URL pointing to a github repository. Paths to issues or PRs will also match this pattern.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def is_github_repo_url(data_path: str) -&gt; bool:\n    \"\"\"Check if data_path is an URL pointing to a github repository.\n    Paths to issues or PRs will also match this pattern.\n    \"\"\"\n    return GITHUB_REPO_URL_PATTERN.search(data_path) is not None\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.parse_gh_issue_url","title":"<code>parse_gh_issue_url(issue_url)</code>","text":"<p>Return owner, repo, issue number from issue url</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def parse_gh_issue_url(issue_url: str) -&gt; tuple[str, str, str]:\n    \"\"\"Return owner, repo, issue number from issue url\"\"\"\n    match = GITHUB_ISSUE_URL_PATTERN.search(issue_url)\n    if not match:\n        msg = f\"Invalid GitHub issue URL: {issue_url}\"\n        raise InvalidGithubURL(msg)\n    res = match.groups()\n    assert len(res) == 3\n    return tuple(res)  # type: ignore\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.parse_gh_repo_url","title":"<code>parse_gh_repo_url(repo_url)</code>","text":"<p>Return owner, repo from repo url</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def parse_gh_repo_url(repo_url: str) -&gt; tuple[str, str]:\n    \"\"\"Return owner, repo from repo url\"\"\"\n    match = GITHUB_REPO_URL_PATTERN.search(repo_url)\n    if not match:\n        msg = f\"Invalid GitHub issue URL: {repo_url}\"\n        raise InvalidGithubURL(msg)\n    res = match.groups()\n    assert len(res) == 2\n    return tuple(res)  # type: ignore\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.read_with_timeout","title":"<code>read_with_timeout(container, pid_func, timeout_duration)</code>","text":"<p>Read data from a subprocess with a timeout. This function uses a file descriptor to read data from the subprocess in a non-blocking way.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>Popen</code> <p>The subprocess container.</p> required <code>pid_func</code> <code>function</code> <p>A function that returns a list of process IDs (except the PID of the main process).</p> required <code>timeout_duration</code> <code>int</code> <p>The timeout duration in seconds.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The data read from the subprocess, stripped of trailing newline characters.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the timeout duration is reached while reading from the subprocess.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def read_with_timeout(container, pid_func, timeout_duration):\n    \"\"\"\n    Read data from a subprocess with a timeout.\n    This function uses a file descriptor to read data from the subprocess in a non-blocking way.\n\n    Args:\n        container (subprocess.Popen): The subprocess container.\n        pid_func (function): A function that returns a list of process IDs (except the PID of the main process).\n        timeout_duration (int): The timeout duration in seconds.\n\n    Returns:\n        str: The data read from the subprocess, stripped of trailing newline characters.\n\n    Raises:\n        TimeoutError: If the timeout duration is reached while reading from the subprocess.\n    \"\"\"\n    buffer = b\"\"\n    fd = container.stdout.fileno()\n    end_time = time.time() + timeout_duration\n\n    # Select is not available on windows\n    is_windows = platform.system() == \"Windows\"\n    if not is_windows:\n        import select\n    else:\n        os.set_blocking(fd, False)\n\n    def ready_to_read(fd) -&gt; bool:\n        if is_windows:\n            # We can't do the extra check\n            return True\n        return bool(select.select([fd], [], [], 0.01)[0])\n\n    while time.time() &lt; end_time:\n        pids = pid_func()\n        if len(pids) &gt; 0:\n            # There are still PIDs running\n            time.sleep(0.05)\n            continue\n        if ready_to_read(fd):\n            data = os.read(fd, 4096)\n            if data:\n                buffer += data\n        else:\n            # No more data to read\n            break\n        time.sleep(0.05)  # Prevents CPU hogging\n\n    if container.poll() is not None:\n        msg = f\"Subprocess exited unexpectedly.\\nCurrent buffer: {buffer.decode()}\"\n        raise RuntimeError(msg)\n    if time.time() &gt;= end_time:\n        msg = f\"Timeout reached while reading from subprocess.\\nCurrent buffer: {buffer.decode()}\\nRunning PIDs: {pids}\"\n        raise TimeoutError(msg)\n    return buffer.decode()\n</code></pre>"},{"location":"reference/env_utils/#sweagent.environment.utils.read_with_timeout_experimental","title":"<code>read_with_timeout_experimental(container, timeout_duration)</code>","text":"<p>Read data from a subprocess with a timeout. This function uses a file descriptor to read data from the subprocess in a non-blocking way.</p> <p>NOTE: This is an experimental implementation that is faster than <code>read_with_timeout</code>, but has not been thoroughly tested.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>Popen</code> <p>The subprocess container.</p> required <code>timeout_duration</code> <code>int</code> <p>The timeout duration in seconds.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>The data read from the subprocess, stripped of trailing newline characters.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the timeout duration is reached while reading from the subprocess.</p> Source code in <code>sweagent/environment/utils.py</code> <pre><code>def read_with_timeout_experimental(container, timeout_duration):\n    \"\"\"\n    Read data from a subprocess with a timeout.\n    This function uses a file descriptor to read data from the subprocess in a non-blocking way.\n\n    NOTE: This is an experimental implementation that is faster than `read_with_timeout`, but\n    has not been thoroughly tested.\n\n    Args:\n        container (subprocess.Popen): The subprocess container.\n        timeout_duration (int): The timeout duration in seconds.\n\n    Returns:\n        str: The data read from the subprocess, stripped of trailing newline characters.\n\n    Raises:\n        TimeoutError: If the timeout duration is reached while reading from the subprocess.\n    \"\"\"\n    buffer = b\"\"\n    fd = container.stdout.fileno()\n    end_time = time.time() + timeout_duration\n\n    # Select is not available on windows\n    is_windows = platform.system() == \"Windows\"\n    if not is_windows:\n        import select\n    else:\n        os.set_blocking(fd, False)\n\n    def ready_to_read(fd) -&gt; bool:\n        if is_windows:\n            # We can't do the extra check\n            return True\n        return bool(select.select([fd], [], [], 0.01)[0])\n\n    while time.time() &lt; end_time:\n        if ready_to_read(fd):\n            try:\n                data = os.read(fd, 4096)\n            except BlockingIOError:\n                break\n            if data:\n                buffer += data\n        if PROCESS_DONE_MARKER_START in buffer.decode():\n            break\n        time.sleep(0.01)  # Prevents CPU hogging\n\n    if container.poll() is not None:\n        msg = f\"Subprocess exited unexpectedly.\\nCurrent buffer: {buffer.decode()}\"\n        raise RuntimeError(msg)\n    if time.time() &gt;= end_time:\n        msg = f\"Timeout reached while reading from subprocess.\\nCurrent buffer: {buffer.decode()}\"\n        raise TimeoutError(msg)\n    decoded = buffer.decode()\n    body = \"\\n\".join(line for line in decoded.splitlines() if not line.startswith(PROCESS_DONE_MARKER_START))\n    last_line = decoded.splitlines()[-1]\n    _results = PROCESS_DONE_REGEX.search(last_line)\n    if _results is None:\n        msg = f\"Could not find process done marker in last line: {last_line=}, {body=}\"\n        raise ValueError(msg)\n    exit_code = _results.group(1)\n    return body, exit_code\n</code></pre>"},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#sweagent.agent.models.AnthropicModel","title":"<code>AnthropicModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class AnthropicModel(BaseModel):\n    MODELS = {\n        \"claude-instant\": {\n            \"max_context\": 100_000,\n            \"cost_per_input_token\": 1.63e-06,\n            \"cost_per_output_token\": 5.51e-06,\n        },\n        \"claude-2.0\": {\n            \"max_context\": 100_000,\n            \"cost_per_input_token\": 1.102e-05,\n            \"cost_per_output_token\": 3.268e-05,\n        },\n        \"claude-2.1\": {\n            \"max_context\": 100_000,\n            \"cost_per_input_token\": 1.102e-05,\n            \"cost_per_output_token\": 3.268e-05,\n        },\n        \"claude-3-opus-20240229\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,  # Max tokens to generate for Claude 3 models\n            \"cost_per_input_token\": 1.5e-05,\n            \"cost_per_output_token\": 7.5e-05,\n        },\n        \"claude-3-sonnet-20240229\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 3e-06,\n            \"cost_per_output_token\": 1.5e-05,\n        },\n        \"claude-3-haiku-20240307\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 2.5e-07,\n            \"cost_per_output_token\": 1.25e-06,\n        },\n    }\n\n    SHORTCUTS = {\n        \"claude-2\": \"claude-2.1\",\n        \"claude-opus\": \"claude-3-opus-20240229\",\n        \"claude-sonnet\": \"claude-3-sonnet-20240229\",\n        \"claude-haiku\": \"claude-3-haiku-20240307\",\n    }\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n\n        # Set Anthropic key\n        self.api = Anthropic(api_key=keys_config[\"ANTHROPIC_API_KEY\"])\n\n    def history_to_messages(\n        self,\n        history: list[dict[str, str]],\n        is_demonstration: bool = False,\n    ) -&gt; str | list[dict[str, str]]:\n        \"\"\"\n        Create `prompt` by filtering out all keys except for role/content per `history` turn\n        Reference: https://docs.anthropic.com/claude/reference/complete_post\n        \"\"\"\n        return anthropic_history_to_messages(self, history, is_demonstration)\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=15),\n        reraise=True,\n        stop=stop_after_attempt(3),\n        retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n    )\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Query the Anthropic API with the given `history` and return the response.\n        \"\"\"\n        return anthropic_query(self, history)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.AnthropicModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>prompt</code> by filtering out all keys except for role/content per <code>history</code> turn Reference: https://docs.anthropic.com/claude/reference/complete_post</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(\n    self,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `prompt` by filtering out all keys except for role/content per `history` turn\n    Reference: https://docs.anthropic.com/claude/reference/complete_post\n    \"\"\"\n    return anthropic_history_to_messages(self, history, is_demonstration)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.AnthropicModel.query","title":"<code>query(history)</code>","text":"<p>Query the Anthropic API with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=15),\n    reraise=True,\n    stop=stop_after_attempt(3),\n    retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n)\ndef query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query the Anthropic API with the given `history` and return the response.\n    \"\"\"\n    return anthropic_query(self, history)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.BaseModel","title":"<code>BaseModel</code>","text":"Source code in <code>sweagent/agent/models.py</code> <pre><code>class BaseModel:\n    MODELS = {}\n    SHORTCUTS = {}\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        self.args = args\n        self.commands = commands\n        self.model_metadata = {}\n        self.stats = APIStats()\n\n        # Map `model_name` to API-compatible name `api_model`\n        self.api_model = (\n            self.SHORTCUTS[self.args.model_name] if self.args.model_name in self.SHORTCUTS else self.args.model_name\n        )\n\n        # Map model name to metadata (cost, context info)\n        MODELS = {\n            **{dest: self.MODELS[src] for dest, src in self.SHORTCUTS.items()},\n            **self.MODELS,\n        }\n        if args.model_name in MODELS:\n            self.model_metadata = MODELS[args.model_name]\n        elif args.model_name.startswith(\"ft:\"):\n            ft_model = args.model_name.split(\":\")[1]\n            self.model_metadata = MODELS[ft_model]\n        elif args.model_name.startswith(\"ollama:\"):\n            self.api_model = args.model_name.split(\"ollama:\", 1)[1]\n            self.model_metadata = self.MODELS[self.api_model]\n        elif args.model_name.startswith(\"azure:\"):\n            azure_model = args.model_name.split(\"azure:\", 1)[1]\n            self.model_metadata = MODELS[azure_model]\n        elif args.model_name.startswith(\"bedrock:\"):\n            self.api_model = args.model_name.split(\"bedrock:\", 1)[1]\n            self.model_metadata = MODELS[self.api_model]\n        else:\n            msg = f\"Unregistered model ({args.model_name}). Add model name to MODELS metadata to {self.__class__}\"\n            raise ValueError(msg)\n\n    def reset_stats(self, other: APIStats | None = None):\n        if other is None:\n            self.stats = APIStats(total_cost=self.stats.total_cost)\n            logger.info(\"Resetting model stats\")\n        else:\n            self.stats = other\n\n    def update_stats(self, input_tokens: int, output_tokens: int) -&gt; float:\n        \"\"\"\n        Calculates the cost of a response from the openai API.\n\n        Args:\n        input_tokens (int): The number of tokens in the prompt.\n        output_tokens (int): The number of tokens in the response.\n\n        Returns:\n        float: The cost of the response.\n        \"\"\"\n        # Calculate cost and update cost related fields\n        cost = (\n            self.model_metadata[\"cost_per_input_token\"] * input_tokens\n            + self.model_metadata[\"cost_per_output_token\"] * output_tokens\n        )\n        self.stats.total_cost += cost\n        self.stats.instance_cost += cost\n        self.stats.tokens_sent += input_tokens\n        self.stats.tokens_received += output_tokens\n        self.stats.api_calls += 1\n\n        # Log updated cost values to std. out.\n        logger.info(\n            f\"input_tokens={input_tokens:,}, \"\n            f\"output_tokens={output_tokens:,}, \"\n            f\"instance_cost={self.stats.instance_cost:.2f}, \"\n            f\"cost={cost:.2f}\",\n        )\n        logger.info(\n            f\"total_tokens_sent={self.stats.tokens_sent:,}, \"\n            f\"total_tokens_received={self.stats.tokens_received:,}, \"\n            f\"total_cost={self.stats.total_cost:.2f}, \"\n            f\"total_api_calls={self.stats.api_calls:,}\",\n        )\n\n        # Check whether total cost or instance cost limits have been exceeded\n        if 0 &lt; self.args.total_cost_limit &lt;= self.stats.total_cost:\n            logger.warning(f\"Cost {self.stats.total_cost:.2f} exceeds limit {self.args.total_cost_limit:.2f}\")\n            msg = \"Total cost limit exceeded\"\n            raise CostLimitExceededError(msg)\n\n        if 0 &lt; self.args.per_instance_cost_limit &lt;= self.stats.instance_cost:\n            logger.warning(f\"Cost {self.stats.instance_cost:.2f} exceeds limit {self.args.per_instance_cost_limit:.2f}\")\n            msg = \"Instance cost limit exceeded\"\n            raise CostLimitExceededError(msg)\n        return cost\n\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        msg = \"Use a subclass of BaseModel\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.BaseModel.update_stats","title":"<code>update_stats(input_tokens, output_tokens)</code>","text":"<p>Calculates the cost of a response from the openai API.</p> <p>Args: input_tokens (int): The number of tokens in the prompt. output_tokens (int): The number of tokens in the response.</p> <p>Returns: float: The cost of the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def update_stats(self, input_tokens: int, output_tokens: int) -&gt; float:\n    \"\"\"\n    Calculates the cost of a response from the openai API.\n\n    Args:\n    input_tokens (int): The number of tokens in the prompt.\n    output_tokens (int): The number of tokens in the response.\n\n    Returns:\n    float: The cost of the response.\n    \"\"\"\n    # Calculate cost and update cost related fields\n    cost = (\n        self.model_metadata[\"cost_per_input_token\"] * input_tokens\n        + self.model_metadata[\"cost_per_output_token\"] * output_tokens\n    )\n    self.stats.total_cost += cost\n    self.stats.instance_cost += cost\n    self.stats.tokens_sent += input_tokens\n    self.stats.tokens_received += output_tokens\n    self.stats.api_calls += 1\n\n    # Log updated cost values to std. out.\n    logger.info(\n        f\"input_tokens={input_tokens:,}, \"\n        f\"output_tokens={output_tokens:,}, \"\n        f\"instance_cost={self.stats.instance_cost:.2f}, \"\n        f\"cost={cost:.2f}\",\n    )\n    logger.info(\n        f\"total_tokens_sent={self.stats.tokens_sent:,}, \"\n        f\"total_tokens_received={self.stats.tokens_received:,}, \"\n        f\"total_cost={self.stats.total_cost:.2f}, \"\n        f\"total_api_calls={self.stats.api_calls:,}\",\n    )\n\n    # Check whether total cost or instance cost limits have been exceeded\n    if 0 &lt; self.args.total_cost_limit &lt;= self.stats.total_cost:\n        logger.warning(f\"Cost {self.stats.total_cost:.2f} exceeds limit {self.args.total_cost_limit:.2f}\")\n        msg = \"Total cost limit exceeded\"\n        raise CostLimitExceededError(msg)\n\n    if 0 &lt; self.args.per_instance_cost_limit &lt;= self.stats.instance_cost:\n        logger.warning(f\"Cost {self.stats.instance_cost:.2f} exceeds limit {self.args.per_instance_cost_limit:.2f}\")\n        msg = \"Instance cost limit exceeded\"\n        raise CostLimitExceededError(msg)\n    return cost\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.BedrockModel","title":"<code>BedrockModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class BedrockModel(BaseModel):\n    MODELS = {\n        \"anthropic.claude-instant-v1\": {\n            \"max_context\": 100_000,\n            \"max_tokens_to_sample\": 4096,\n            \"cost_per_input_token\": 8e-07,\n            \"cost_per_output_token\": 2.4e-06,\n        },\n        \"anthropic.claude-v2\": {\n            \"max_context\": 100_000,\n            \"max_tokens_to_sample\": 4096,\n            \"cost_per_input_token\": 8e-06,\n            \"cost_per_output_token\": 2.4e-05,\n        },\n        \"anthropic.claude-v2:1\": {\n            \"max_context\": 100_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 8e-06,\n            \"cost_per_output_token\": 2.4e-05,\n        },\n        \"anthropic.claude-3-opus-20240229-v1:0\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 1.5e-05,\n            \"cost_per_output_token\": 7.5e-05,\n        },\n        \"anthropic.claude-3-sonnet-20240229-v1:0\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 3e-06,\n            \"cost_per_output_token\": 1.5e-05,\n        },\n        \"anthropic.claude-3-haiku-20240307-v1:0\": {\n            \"max_context\": 200_000,\n            \"max_tokens\": 4096,\n            \"cost_per_input_token\": 2.5e-07,\n            \"cost_per_output_token\": 1.25e-06,\n        },\n    }\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n\n        # Extract provider from model ID\n        # https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html\n        self.model_provider = self.api_model.split(\".\")[0]\n        if self.model_provider == \"anthropic\":\n            # Note: this assumes AWS credentials are already configured.\n            # https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n            self.api = AnthropicBedrock()\n        elif self.model_provider in [\"ai21\", \"amazon\", \"cohere\", \"meta\", \"mistral\"]:\n            msg = f\"{self.api_model} is not supported!\"\n            raise NotImplementedError(msg)\n        else:\n            msg = f\"Provider {self.model_provider} is not supported by Amazon Bedrock!\"\n            raise ValueError(msg)\n\n    def history_to_messages(\n        self,\n        history: list[dict[str, str]],\n        is_demonstration: bool = False,\n    ) -&gt; str | list[dict[str, str]]:\n        \"\"\"\n        Create `prompt` from the history of messages\n        \"\"\"\n        if self.model_provider == \"anthropic\":\n            return anthropic_history_to_messages(self, history, is_demonstration)\n        else:\n            msg = f\"{self.api_model} is not supported!\"\n            raise NotImplementedError(msg)\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=15),\n        reraise=True,\n        stop=stop_after_attempt(3),\n        retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n    )\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Query Amazon Bedrock with the given `history` and return the response.\n        \"\"\"\n        if self.model_provider == \"anthropic\":\n            return anthropic_query(self, history)\n        else:\n            msg = f\"{self.api_model} is not supported!\"\n            raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.BedrockModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>prompt</code> from the history of messages</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(\n    self,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `prompt` from the history of messages\n    \"\"\"\n    if self.model_provider == \"anthropic\":\n        return anthropic_history_to_messages(self, history, is_demonstration)\n    else:\n        msg = f\"{self.api_model} is not supported!\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.BedrockModel.query","title":"<code>query(history)</code>","text":"<p>Query Amazon Bedrock with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=15),\n    reraise=True,\n    stop=stop_after_attempt(3),\n    retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n)\ndef query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query Amazon Bedrock with the given `history` and return the response.\n    \"\"\"\n    if self.model_provider == \"anthropic\":\n        return anthropic_query(self, history)\n    else:\n        msg = f\"{self.api_model} is not supported!\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.HumanModel","title":"<code>HumanModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class HumanModel(BaseModel):\n    MODELS = {\"human\": {}}\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n\n        # Determine which commands require multi-line input\n        self.multi_line_command_endings = {\n            command.name: command.end_name for command in commands if command.end_name is not None\n        }\n\n    def history_to_messages(\n        self,\n        history: list[dict[str, str]],\n        is_demonstration: bool = False,\n    ) -&gt; str | list[dict[str, str]]:\n        \"\"\"\n        Create `messages` by filtering out all keys except for role/content per `history` turn\n        \"\"\"\n        # Remove system messages if it is a demonstration\n        if is_demonstration:\n            history = [entry for entry in history if entry[\"role\"] != \"system\"]\n            return \"\\n\".join([entry[\"content\"] for entry in history])\n        # Return history components with just role, content fields\n        return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n\n    def query(self, history: list[dict[str, str]], action_prompt: str = \"&gt; \") -&gt; str:\n        \"\"\"\n        Logic for handling user input to pass to SWEEnv\n        \"\"\"\n        action = input(action_prompt)\n        command_name = action.split()[0] if action else \"\"\n\n        # Special handling for multi-line input actions (i.e. edit)\n        if command_name in self.multi_line_command_endings:\n            buffer = [action]\n            end_keyword = self.multi_line_command_endings[command_name]\n            while True:\n                action = input(\"... \")\n                buffer.append(action)\n                if action.rstrip() == end_keyword:\n                    # Continue reading input until terminating keyword inputted\n                    break\n            action = \"\\n\".join(buffer)\n        elif action.strip() == \"start_multiline_command\":  # do arbitrary multi-line input\n            buffer = []\n            while True:\n                action = input(\"... \")\n                if action.rstrip() == \"end_multiline_command\":\n                    break\n                buffer.append(action)\n            action = \"\\n\".join(buffer)\n        return action\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.HumanModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>messages</code> by filtering out all keys except for role/content per <code>history</code> turn</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(\n    self,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `messages` by filtering out all keys except for role/content per `history` turn\n    \"\"\"\n    # Remove system messages if it is a demonstration\n    if is_demonstration:\n        history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        return \"\\n\".join([entry[\"content\"] for entry in history])\n    # Return history components with just role, content fields\n    return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.HumanModel.query","title":"<code>query(history, action_prompt='&gt; ')</code>","text":"<p>Logic for handling user input to pass to SWEEnv</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def query(self, history: list[dict[str, str]], action_prompt: str = \"&gt; \") -&gt; str:\n    \"\"\"\n    Logic for handling user input to pass to SWEEnv\n    \"\"\"\n    action = input(action_prompt)\n    command_name = action.split()[0] if action else \"\"\n\n    # Special handling for multi-line input actions (i.e. edit)\n    if command_name in self.multi_line_command_endings:\n        buffer = [action]\n        end_keyword = self.multi_line_command_endings[command_name]\n        while True:\n            action = input(\"... \")\n            buffer.append(action)\n            if action.rstrip() == end_keyword:\n                # Continue reading input until terminating keyword inputted\n                break\n        action = \"\\n\".join(buffer)\n    elif action.strip() == \"start_multiline_command\":  # do arbitrary multi-line input\n        buffer = []\n        while True:\n            action = input(\"... \")\n            if action.rstrip() == \"end_multiline_command\":\n                break\n            buffer.append(action)\n        action = \"\\n\".join(buffer)\n    return action\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.HumanThoughtModel","title":"<code>HumanThoughtModel</code>","text":"<p>               Bases: <code>HumanModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class HumanThoughtModel(HumanModel):\n    MODELS = {\"human_thought\": {}}\n\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Logic for handling user input (both thought + action) to pass to SWEEnv\n        \"\"\"\n        thought_all = \"\"\n        thought = input(\"Thought (end w/ END_THOUGHT): \")\n        while True:\n            if \"END_THOUGHT\" in thought:\n                thought = thought.split(\"END_THOUGHT\")[0]\n                thought_all += thought\n                break\n            thought_all += thought\n            thought = input(\"... \")\n\n        action = super().query(history, action_prompt=\"Action: \")\n\n        return f\"{thought_all}\\n```\\n{action}\\n```\"\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.HumanThoughtModel.query","title":"<code>query(history)</code>","text":"<p>Logic for handling user input (both thought + action) to pass to SWEEnv</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Logic for handling user input (both thought + action) to pass to SWEEnv\n    \"\"\"\n    thought_all = \"\"\n    thought = input(\"Thought (end w/ END_THOUGHT): \")\n    while True:\n        if \"END_THOUGHT\" in thought:\n            thought = thought.split(\"END_THOUGHT\")[0]\n            thought_all += thought\n            break\n        thought_all += thought\n        thought = input(\"... \")\n\n    action = super().query(history, action_prompt=\"Action: \")\n\n    return f\"{thought_all}\\n```\\n{action}\\n```\"\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.InstantEmptySubmitTestModel","title":"<code>InstantEmptySubmitTestModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class InstantEmptySubmitTestModel(BaseModel):\n    MODELS = {\"instant_empty_submit\": {}}\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        \"\"\"This model immediately submits. Useful for testing purposes\"\"\"\n        super().__init__(args, commands)\n        self._action_idx = 0\n\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        # Need to at least do _something_ to submit\n        if self._action_idx == 0:\n            self._action_idx = 1\n            action = \"DISCUSSION\\nLet's reproduce the bug by creating a `reproduce.py` file.\\n\\n```\\ncreate reproduce.py\\n```\\n\"\n        elif self._action_idx == 1:\n            self._action_idx = 0\n            action = \"DISCUSSION\\nThe task should be resolved, so let's submit the patch.\\n\\n```\\nsubmit\\n```\\n\"\n        return action\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.InstantEmptySubmitTestModel.__init__","title":"<code>__init__(args, commands)</code>","text":"<p>This model immediately submits. Useful for testing purposes</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def __init__(self, args: ModelArguments, commands: list[Command]):\n    \"\"\"This model immediately submits. Useful for testing purposes\"\"\"\n    super().__init__(args, commands)\n    self._action_idx = 0\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.ModelArguments","title":"<code>ModelArguments</code>  <code>dataclass</code>","text":"<p>               Bases: <code>FrozenSerializable</code></p> <p>Arguments configuring the model and its behavior.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@dataclass(frozen=True)\nclass ModelArguments(FrozenSerializable):\n    \"\"\"Arguments configuring the model and its behavior.\"\"\"\n\n    # Name of the model to use\n    model_name: str\n    # Cost limit for every instance (task)\n    per_instance_cost_limit: float = 0.0\n    # Total cost limit\n    total_cost_limit: float = 0.0\n    # Sampling temperature\n    temperature: float = 1.0\n    # Sampling top-p\n    top_p: float = 1.0\n    # Path to replay file when using the replay model\n    replay_path: str = None\n    # Host URL when using Ollama model\n    host_url: str = \"localhost:11434\"\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OllamaModel","title":"<code>OllamaModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class OllamaModel(BaseModel):\n    MODELS = defaultdict(\n        lambda: {\n            \"max_context\": 128_000,\n            \"cost_per_input_token\": 0,\n            \"cost_per_output_token\": 0,\n        },\n    )\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n        from ollama import Client\n\n        self.client = Client(host=args.host_url)\n\n    def history_to_messages(\n        self,\n        history: list[dict[str, str]],\n        is_demonstration: bool = False,\n    ) -&gt; str | list[dict[str, str]]:\n        \"\"\"\n        Create `messages` by filtering out all keys except for role/content per `history` turn\n        \"\"\"\n        # Remove system messages if it is a demonstration\n        if is_demonstration:\n            history = [entry for entry in history if entry[\"role\"] != \"system\"]\n            return \"\\n\".join([entry[\"content\"] for entry in history])\n        # Return history components with just role, content fields\n        return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=15),\n        reraise=True,\n        stop=stop_after_attempt(3),\n        retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n    )\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Query the Ollama API with the given `history` and return the response.\n        \"\"\"\n        response = self.client.chat(\n            model=self.api_model,\n            messages=self.history_to_messages(history),\n            options={\n                \"temperature\": self.args.temperature,\n                \"top_p\": self.args.top_p,\n            },\n        )\n        # Calculate + update costs, return response\n        if \"prompt_eval_count\" in response:\n            input_tokens = response[\"prompt_eval_count\"]\n        else:\n            logger.warning(\n                \"Prompt eval count not found in response. Using 0. \"\n                \"This might be because the prompt has been cached. \"\n                \"See https://github.com/princeton-nlp/SWE-agent/issues/44 \"\n                \"and https://github.com/ollama/ollama/issues/3427.\",\n            )\n            input_tokens = 0\n        output_tokens = response[\"eval_count\"]\n        self.update_stats(input_tokens, output_tokens)\n        return response[\"message\"][\"content\"]\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OllamaModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>messages</code> by filtering out all keys except for role/content per <code>history</code> turn</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(\n    self,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `messages` by filtering out all keys except for role/content per `history` turn\n    \"\"\"\n    # Remove system messages if it is a demonstration\n    if is_demonstration:\n        history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        return \"\\n\".join([entry[\"content\"] for entry in history])\n    # Return history components with just role, content fields\n    return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OllamaModel.query","title":"<code>query(history)</code>","text":"<p>Query the Ollama API with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=15),\n    reraise=True,\n    stop=stop_after_attempt(3),\n    retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n)\ndef query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query the Ollama API with the given `history` and return the response.\n    \"\"\"\n    response = self.client.chat(\n        model=self.api_model,\n        messages=self.history_to_messages(history),\n        options={\n            \"temperature\": self.args.temperature,\n            \"top_p\": self.args.top_p,\n        },\n    )\n    # Calculate + update costs, return response\n    if \"prompt_eval_count\" in response:\n        input_tokens = response[\"prompt_eval_count\"]\n    else:\n        logger.warning(\n            \"Prompt eval count not found in response. Using 0. \"\n            \"This might be because the prompt has been cached. \"\n            \"See https://github.com/princeton-nlp/SWE-agent/issues/44 \"\n            \"and https://github.com/ollama/ollama/issues/3427.\",\n        )\n        input_tokens = 0\n    output_tokens = response[\"eval_count\"]\n    self.update_stats(input_tokens, output_tokens)\n    return response[\"message\"][\"content\"]\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OpenAIModel","title":"<code>OpenAIModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class OpenAIModel(BaseModel):\n    MODELS = {\n        \"gpt-3.5-turbo-0125\": {\n            \"max_context\": 16_385,\n            \"cost_per_input_token\": 5e-07,\n            \"cost_per_output_token\": 1.5e-06,\n        },\n        \"gpt-3.5-turbo-1106\": {\n            \"max_context\": 16_385,\n            \"cost_per_input_token\": 1.5e-06,\n            \"cost_per_output_token\": 2e-06,\n        },\n        \"gpt-3.5-turbo-16k-0613\": {\n            \"max_context\": 16_385,\n            \"cost_per_input_token\": 1.5e-06,\n            \"cost_per_output_token\": 2e-06,\n        },\n        \"gpt-4-32k-0613\": {\n            \"max_context\": 32_768,\n            \"cost_per_input_token\": 6e-05,\n            \"cost_per_output_token\": 0.00012,\n        },\n        \"gpt-4-0613\": {\n            \"max_context\": 8_192,\n            \"cost_per_input_token\": 3e-05,\n            \"cost_per_output_token\": 6e-05,\n        },\n        \"gpt-4-1106-preview\": {\n            \"max_context\": 128_000,\n            \"cost_per_input_token\": 1e-05,\n            \"cost_per_output_token\": 3e-05,\n        },\n        \"gpt-4-0125-preview\": {\n            \"max_context\": 128_000,\n            \"cost_per_input_token\": 1e-05,\n            \"cost_per_output_token\": 3e-05,\n        },\n        \"gpt-4-turbo-2024-04-09\": {\n            \"max_context\": 128_000,\n            \"cost_per_input_token\": 1e-05,\n            \"cost_per_output_token\": 3e-05,\n        },\n        \"gpt-4o-2024-05-13\": {\n            \"max_context\": 128_000,\n            \"cost_per_input_token\": 5e-06,\n            \"cost_per_output_token\": 15e-06,\n        },\n    }\n\n    SHORTCUTS = {\n        \"gpt3\": \"gpt-3.5-turbo-1106\",\n        \"gpt3-legacy\": \"gpt-3.5-turbo-16k-0613\",\n        \"gpt4\": \"gpt-4-1106-preview\",\n        \"gpt4-legacy\": \"gpt-4-0613\",\n        \"gpt4-0125\": \"gpt-4-0125-preview\",\n        \"gpt3-0125\": \"gpt-3.5-turbo-0125\",\n        \"gpt4-turbo\": \"gpt-4-turbo-2024-04-09\",\n        \"gpt4o\": \"gpt-4o-2024-05-13\",\n    }\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n\n        logging.getLogger(\"openai\").setLevel(logging.WARNING)\n        logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n\n        # Set OpenAI key\n        if self.args.model_name.startswith(\"azure\"):\n            self.api_model = keys_config[\"AZURE_OPENAI_DEPLOYMENT\"]\n            self.client = AzureOpenAI(\n                api_key=keys_config[\"AZURE_OPENAI_API_KEY\"],\n                azure_endpoint=keys_config[\"AZURE_OPENAI_ENDPOINT\"],\n                api_version=keys_config.get(\"AZURE_OPENAI_API_VERSION\", \"2024-02-01\"),\n            )\n        else:\n            api_base_url: str | None = keys_config.get(\"OPENAI_API_BASE_URL\", None)\n            self.client = OpenAI(api_key=keys_config[\"OPENAI_API_KEY\"], base_url=api_base_url)\n\n    def history_to_messages(\n        self,\n        history: list[dict[str, str]],\n        is_demonstration: bool = False,\n    ) -&gt; str | list[dict[str, str]]:\n        \"\"\"\n        Create `messages` by filtering out all keys except for role/content per `history` turn\n        \"\"\"\n        # Remove system messages if it is a demonstration\n        if is_demonstration:\n            history = [entry for entry in history if entry[\"role\"] != \"system\"]\n            return \"\\n\".join([entry[\"content\"] for entry in history])\n        # Return history components with just role, content fields\n        return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=15),\n        reraise=True,\n        stop=stop_after_attempt(3),\n        retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n    )\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Query the OpenAI API with the given `history` and return the response.\n        \"\"\"\n        try:\n            # Perform OpenAI API call\n            response = self.client.chat.completions.create(\n                messages=self.history_to_messages(history),\n                model=self.api_model,\n                temperature=self.args.temperature,\n                top_p=self.args.top_p,\n            )\n        except BadRequestError:\n            msg = f\"Context window ({self.model_metadata['max_context']} tokens) exceeded\"\n            raise CostLimitExceededError(msg)\n        # Calculate + update costs, return response\n        input_tokens = response.usage.prompt_tokens\n        output_tokens = response.usage.completion_tokens\n        self.update_stats(input_tokens, output_tokens)\n        return response.choices[0].message.content\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OpenAIModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>messages</code> by filtering out all keys except for role/content per <code>history</code> turn</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(\n    self,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `messages` by filtering out all keys except for role/content per `history` turn\n    \"\"\"\n    # Remove system messages if it is a demonstration\n    if is_demonstration:\n        history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        return \"\\n\".join([entry[\"content\"] for entry in history])\n    # Return history components with just role, content fields\n    return [{k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history]\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.OpenAIModel.query","title":"<code>query(history)</code>","text":"<p>Query the OpenAI API with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=15),\n    reraise=True,\n    stop=stop_after_attempt(3),\n    retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n)\ndef query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query the OpenAI API with the given `history` and return the response.\n    \"\"\"\n    try:\n        # Perform OpenAI API call\n        response = self.client.chat.completions.create(\n            messages=self.history_to_messages(history),\n            model=self.api_model,\n            temperature=self.args.temperature,\n            top_p=self.args.top_p,\n        )\n    except BadRequestError:\n        msg = f\"Context window ({self.model_metadata['max_context']} tokens) exceeded\"\n        raise CostLimitExceededError(msg)\n    # Calculate + update costs, return response\n    input_tokens = response.usage.prompt_tokens\n    output_tokens = response.usage.completion_tokens\n    self.update_stats(input_tokens, output_tokens)\n    return response.choices[0].message.content\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.ReplayModel","title":"<code>ReplayModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class ReplayModel(BaseModel):\n    MODELS = {\"replay\": {}}\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n\n        if self.args.replay_path is None or not os.path.exists(self.args.replay_path):\n            msg = \"--replay_path must point to a file that exists to run a replay policy\"\n            raise ValueError(msg)\n\n        self.replays = [list(json.loads(x).values())[0] for x in open(self.args.replay_path).readlines()]\n        self.replay_idx = 0\n        self.action_idx = 0\n\n    def _next_replay(self) -&gt; None:\n        \"\"\"Called after last action\"\"\"\n        self.replay_idx += 1\n        self.action_idx = 0\n\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Logic for tracking which replay action to pass to SWEEnv\n        \"\"\"\n        actions = self.replays[self.replay_idx]\n        try:\n            action = actions[self.action_idx]\n        except IndexError:\n            msg = (\n                \"This seems to be an incomplete trajectory. \"\n                \"We reached the end of it, but `submit` was not called. \"\n                \"Calling it now.\"\n            )\n            logger.warning(msg)\n            action = \"```\\nsubmit\\n```\"\n\n        self.action_idx += 1\n\n        # Assuming `submit` is always last action of replay trajectory\n        if action == \"submit\":\n            self._next_replay()\n\n        return action\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.ReplayModel.query","title":"<code>query(history)</code>","text":"<p>Logic for tracking which replay action to pass to SWEEnv</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Logic for tracking which replay action to pass to SWEEnv\n    \"\"\"\n    actions = self.replays[self.replay_idx]\n    try:\n        action = actions[self.action_idx]\n    except IndexError:\n        msg = (\n            \"This seems to be an incomplete trajectory. \"\n            \"We reached the end of it, but `submit` was not called. \"\n            \"Calling it now.\"\n        )\n        logger.warning(msg)\n        action = \"```\\nsubmit\\n```\"\n\n    self.action_idx += 1\n\n    # Assuming `submit` is always last action of replay trajectory\n    if action == \"submit\":\n        self._next_replay()\n\n    return action\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.TogetherModel","title":"<code>TogetherModel</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>sweagent/agent/models.py</code> <pre><code>class TogetherModel(BaseModel):\n    # Check https://docs.together.ai/docs/inference-models for model names, context\n    # Check https://www.together.ai/pricing for pricing\n    MODELS = {\n        \"meta-llama/Llama-2-13b-chat-hf\": {\n            \"max_context\": 4096,\n            \"cost_per_input_token\": 2.25e-07,\n            \"cost_per_output_token\": 2.25e-07,\n        },\n        \"meta-llama/Llama-2-70b-chat-hf\": {\n            \"max_context\": 4096,\n            \"cost_per_input_token\": 9e-07,\n            \"cost_per_output_token\": 9e-07,\n        },\n        \"mistralai/Mistral-7B-Instruct-v0.2\": {\n            \"max_context\": 32768,\n            \"cost_per_input_token\": 2e-07,\n            \"cost_per_output_token\": 2e-07,\n        },\n        \"togethercomputer/RedPajama-INCITE-7B-Chat\": {\n            \"max_context\": 2048,\n            \"cost_per_input_token\": 2e-07,\n            \"cost_per_output_token\": 2e-07,\n        },\n        \"mistralai/Mixtral-8x7B-Instruct-v0.1\": {\n            \"max_context\": 32768,\n            \"cost_per_input_token\": 6e-07,\n            \"cost_per_output_token\": 6e-07,\n        },\n    }\n\n    SHORTCUTS = {\n        \"llama13b\": \"meta-llama/Llama-2-13b-chat-hf\",\n        \"llama70b\": \"meta-llama/Llama-2-70b-chat-hf\",\n        \"mistral7b\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n        \"mixtral8x7b\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n        \"redpajama7b\": \"togethercomputer/RedPajama-INCITE-7B-Chat\",\n    }\n\n    def __init__(self, args: ModelArguments, commands: list[Command]):\n        super().__init__(args, commands)\n        assert together.version &gt;= \"1.1.0\", \"Please upgrade to Together SDK v1.1.0 or later.\"\n\n        # Set Together key\n        together.api_key = keys_config[\"TOGETHER_API_KEY\"]\n\n    def history_to_messages(self, history: list[dict[str, str]], is_demonstration: bool = False) -&gt; str:\n        \"\"\"\n        Create `prompt` by filtering out all keys except for role/content per `history` turn\n        \"\"\"\n        # Remove system messages if it is a demonstration\n        if is_demonstration:\n            history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        # Map history to TogetherAI format\n        mapping = {\"user\": \"human\", \"assistant\": \"bot\", \"system\": \"bot\"}\n        prompt = [f'&lt;{mapping[d[\"role\"]]}&gt;: {d[\"content\"]}' for d in history]\n        prompt = \"\\n\".join(prompt)\n        return f\"{prompt}\\n&lt;bot&gt;:\"\n\n    @retry(\n        wait=wait_random_exponential(min=1, max=15),\n        reraise=True,\n        stop=stop_after_attempt(3),\n        retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n    )\n    def query(self, history: list[dict[str, str]]) -&gt; str:\n        \"\"\"\n        Query the Together API with the given `history` and return the response.\n        \"\"\"\n        # Perform Together API call\n        prompt = self.history_to_messages(history)\n        # Anthropic's count_tokens is convenient because it caches and utilizes huggingface/tokenizers, so we will use.\n        max_tokens_to_sample = self.model_metadata[\"max_context\"] - Anthropic().count_tokens(prompt)\n        completion = together.Complete.create(\n            model=self.api_model,\n            prompt=prompt,\n            max_tokens=max_tokens_to_sample,\n            stop=[\"&lt;human&gt;\"],\n            temperature=self.args.temperature,\n            top_p=self.args.top_p,\n        )\n        # Calculate + update costs, return response\n        response = completion[\"choices\"][0][\"text\"].split(\"&lt;human&gt;\")[0]\n        input_tokens = completion[\"usage\"][\"prompt_tokens\"]\n        output_tokens = completion[\"usage\"][\"completion_tokens\"]\n        self.update_stats(input_tokens, output_tokens)\n        return response\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.TogetherModel.history_to_messages","title":"<code>history_to_messages(history, is_demonstration=False)</code>","text":"<p>Create <code>prompt</code> by filtering out all keys except for role/content per <code>history</code> turn</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def history_to_messages(self, history: list[dict[str, str]], is_demonstration: bool = False) -&gt; str:\n    \"\"\"\n    Create `prompt` by filtering out all keys except for role/content per `history` turn\n    \"\"\"\n    # Remove system messages if it is a demonstration\n    if is_demonstration:\n        history = [entry for entry in history if entry[\"role\"] != \"system\"]\n    # Map history to TogetherAI format\n    mapping = {\"user\": \"human\", \"assistant\": \"bot\", \"system\": \"bot\"}\n    prompt = [f'&lt;{mapping[d[\"role\"]]}&gt;: {d[\"content\"]}' for d in history]\n    prompt = \"\\n\".join(prompt)\n    return f\"{prompt}\\n&lt;bot&gt;:\"\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.TogetherModel.query","title":"<code>query(history)</code>","text":"<p>Query the Together API with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>@retry(\n    wait=wait_random_exponential(min=1, max=15),\n    reraise=True,\n    stop=stop_after_attempt(3),\n    retry=retry_if_not_exception_type((CostLimitExceededError, RuntimeError)),\n)\ndef query(self, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query the Together API with the given `history` and return the response.\n    \"\"\"\n    # Perform Together API call\n    prompt = self.history_to_messages(history)\n    # Anthropic's count_tokens is convenient because it caches and utilizes huggingface/tokenizers, so we will use.\n    max_tokens_to_sample = self.model_metadata[\"max_context\"] - Anthropic().count_tokens(prompt)\n    completion = together.Complete.create(\n        model=self.api_model,\n        prompt=prompt,\n        max_tokens=max_tokens_to_sample,\n        stop=[\"&lt;human&gt;\"],\n        temperature=self.args.temperature,\n        top_p=self.args.top_p,\n    )\n    # Calculate + update costs, return response\n    response = completion[\"choices\"][0][\"text\"].split(\"&lt;human&gt;\")[0]\n    input_tokens = completion[\"usage\"][\"prompt_tokens\"]\n    output_tokens = completion[\"usage\"][\"completion_tokens\"]\n    self.update_stats(input_tokens, output_tokens)\n    return response\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.anthropic_history_to_messages","title":"<code>anthropic_history_to_messages(model, history, is_demonstration=False)</code>","text":"<p>Create <code>prompt</code> by filtering out all keys except for role/content per <code>history</code> turn Reference: https://docs.anthropic.com/claude/reference/complete_post</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def anthropic_history_to_messages(\n    model: AnthropicModel | BedrockModel,\n    history: list[dict[str, str]],\n    is_demonstration: bool = False,\n) -&gt; str | list[dict[str, str]]:\n    \"\"\"\n    Create `prompt` by filtering out all keys except for role/content per `history` turn\n    Reference: https://docs.anthropic.com/claude/reference/complete_post\n    \"\"\"\n    # Preserve behavior for older models\n    if model.api_model in [\"claude-instant\", \"claude-2.0\"] or (\n        isinstance(model, BedrockModel) and model.api_model in [\"anthropic.claude-instant-v1\", \"anthropic.claude-v2\"]\n    ):\n        # Remove system messages if it is a demonstration\n        if is_demonstration:\n            history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        # Map history to Claude format\n        prompt = \"\\n\\n\"\n        for entry in history:\n            if entry[\"role\"] in {\"user\", \"system\"}:\n                prompt += f'{HUMAN_PROMPT} {entry[\"content\"]}\\n\\n'\n            elif entry[\"role\"] == \"assistant\":\n                prompt += f'{AI_PROMPT} {entry[\"content\"]}\\n\\n'\n        prompt += AI_PROMPT\n        return prompt\n\n    # Remove system messages if it is a demonstration\n    if is_demonstration:\n        history = [entry for entry in history if entry[\"role\"] != \"system\"]\n        return \"\\n\".join([entry[\"content\"] for entry in history])\n\n    # Return history components with just role, content fields (no system message)\n    messages = [\n        {k: v for k, v in entry.items() if k in [\"role\", \"content\"]} for entry in history if entry[\"role\"] != \"system\"\n    ]\n    compiled_messages = []  # Combine messages from the same role\n    last_role = None\n    for message in reversed(messages):\n        if last_role == message[\"role\"]:\n            compiled_messages[-1][\"content\"] = message[\"content\"] + \"\\n\" + compiled_messages[-1][\"content\"]\n        else:\n            compiled_messages.append(message)\n        last_role = message[\"role\"]\n    compiled_messages = list(reversed(compiled_messages))\n    # Replace any empty content values with a \"(No output)\"\n    for message in compiled_messages:\n        if message[\"content\"].strip() == \"\":\n            message[\"content\"] = \"(No output)\"\n    return compiled_messages\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.anthropic_query","title":"<code>anthropic_query(model, history)</code>","text":"<p>Query the Anthropic API with the given <code>history</code> and return the response.</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def anthropic_query(model: AnthropicModel | BedrockModel, history: list[dict[str, str]]) -&gt; str:\n    \"\"\"\n    Query the Anthropic API with the given `history` and return the response.\n    \"\"\"\n    # Preserve behavior for older models\n    if model.api_model in [\"claude-instant\", \"claude-2.0\", \"claude-2.1\"] or (\n        isinstance(model, BedrockModel) and model.api_model in [\"anthropic.claude-instant-v1\", \"anthropic.claude-v2\"]\n    ):\n        # Perform Anthropic API call\n        prompt = anthropic_history_to_messages(model, history)\n        if isinstance(model, BedrockModel):\n            # Use a dummy Anthropic client since count_tokens\n            # is not available in AnthropicBedrock\n            # https://github.com/anthropics/anthropic-sdk-python/issues/353\n            input_tokens = Anthropic().count_tokens(prompt)\n        else:\n            input_tokens = model.api.count_tokens(prompt)\n        completion = model.api.completions.create(\n            model=model.api_model,\n            prompt=prompt,\n            max_tokens_to_sample=model.model_metadata[\"max_context\"] - input_tokens\n            if isinstance(model, Anthropic)\n            else model.model_metadata[\"max_tokens_to_sample\"],\n            temperature=model.args.temperature,\n            top_p=model.args.top_p,\n        )\n        # Calculate + update costs, return response\n        response = completion.completion\n        if isinstance(model, BedrockModel):\n            output_tokens = Anthropic().count_tokens(response)\n        else:\n            output_tokens = model.api.count_tokens(response)\n        model.update_stats(input_tokens, output_tokens)\n        return response\n\n    # Get system message(s)\n    system_message = \"\\n\".join([entry[\"content\"] for entry in history if entry[\"role\"] == \"system\"])\n    messages = anthropic_history_to_messages(model, history)\n\n    # Perform Anthropic API call\n    response = model.api.messages.create(\n        messages=messages,\n        max_tokens=model.model_metadata[\"max_tokens\"],\n        model=model.api_model,\n        temperature=model.args.temperature,\n        top_p=model.args.top_p,\n        system=system_message,\n    )\n\n    # Calculate + update costs, return response\n    model.update_stats(response.usage.input_tokens, response.usage.output_tokens)\n    return \"\\n\".join([x.text for x in response.content])\n</code></pre>"},{"location":"reference/models/#sweagent.agent.models.get_model","title":"<code>get_model(args, commands=None)</code>","text":"<p>Returns correct model object given arguments and commands</p> Source code in <code>sweagent/agent/models.py</code> <pre><code>def get_model(args: ModelArguments, commands: list[Command] | None = None):\n    \"\"\"\n    Returns correct model object given arguments and commands\n    \"\"\"\n    if commands is None:\n        commands = []\n    if args.model_name == \"instant_empty_submit\":\n        return InstantEmptySubmitTestModel(args, commands)\n    if args.model_name == \"human\":\n        return HumanModel(args, commands)\n    if args.model_name == \"human_thought\":\n        return HumanThoughtModel(args, commands)\n    if args.model_name == \"replay\":\n        return ReplayModel(args, commands)\n    elif (\n        args.model_name.startswith(\"gpt\")\n        or args.model_name.startswith(\"ft:gpt\")\n        or args.model_name.startswith(\"azure:gpt\")\n    ):\n        return OpenAIModel(args, commands)\n    elif args.model_name.startswith(\"claude\"):\n        return AnthropicModel(args, commands)\n    elif args.model_name.startswith(\"bedrock\"):\n        return BedrockModel(args, commands)\n    elif args.model_name.startswith(\"ollama\"):\n        return OllamaModel(args, commands)\n    elif args.model_name in TogetherModel.SHORTCUTS:\n        return TogetherModel(args, commands)\n    elif args.model_name == \"instant_empty_submit\":\n        return InstantEmptySubmitTestModel(args, commands)\n    else:\n        msg = f\"Invalid model name: {args.model_name}\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>We currently provide two interfaces to SWE-agent:</p> <ul> <li> <p> Command line interface (CLI)</p> <p>The default way of running SWE-agent with maximum options.</p> <p> Get started</p> </li> <li> <p> Graphical user interface</p> <p>We provide a browser-based graphical user interface particularly optimized for developers wanting to use SWE-agent as a tool.</p> <p> Get started</p> </li> </ul>"},{"location":"usage/benchmarking/","title":"Benchmarking","text":"<p>There are two steps to the SWE-agent pipeline. First SWE-agent takes an input GitHub issue and returns a pull request that attempts to fix it. We call that step inference. The second step (currently, only available for issues in the SWE-bench benchmark) is to evaluate the pull request to verify that it has indeed fixed the issue.</p> <p>Architectures</p> <p>At this moment, there are known issues with a small number of repositories that don't install properly for <code>arm64</code> / <code>aarch64</code> architecture computers. We're working on a fix, but if you'd like to run and evaluate on the entirety of SWE-bench, the easiest way is by using an <code>x86</code> machine.</p>"},{"location":"usage/benchmarking/#inference","title":"\ud83d\udc69\u200d\ud83d\udcbb Inference","text":"<p>Run SWE-agent on SWE-bench Lite and generate patches.</p> <pre><code>python run.py --model_name gpt4 \\\n  --per_instance_cost_limit 2.00 \\\n  --config_file ./config/default.yaml\n</code></pre> <p>If you'd like to run on a single issue from SWE-bench, use the <code>--instance_filter</code> option as follows: <pre><code>python run.py --model_name gpt4 \\\n  --instance_filter marshmallow-code__marshmallow-1359\n</code></pre></p>"},{"location":"usage/benchmarking/#evaluation","title":"\ud83e\uddea Evaluation","text":"<p>The <code>evaluation/</code> folder provides SWE-agent compatible scripts for running SWE-bench style evaluation on model patch predictions. In addition, we also include additional scripts to quantify model performance on \"subtasks\" within the SWE-bench task, such as identifying the right file(s) to edit.</p>"},{"location":"usage/benchmarking/#quick-start","title":"\ud83d\udc07 Quick Start","text":"<p>You can run evaluations on SWE-bench by passing in the predictions generated by SWE-agent (usually named <code>all_preds.jsonl</code>). Simply run the following script:</p> <pre><code>./run_eval.sh &lt;path to predictions&gt;\n</code></pre> <p>The <code>&lt;predictions_path&gt;</code> arguments should look like</p> <pre><code>../trajectories/&lt;username&gt;/&lt;model&gt;-&lt;dataset&gt;-&lt;hyperparams&gt;/all_preds.jsonl\n</code></pre> <p>Depending on the number of task instances and how long setting up the execution environment takes, the evaluation could take a couple minutes or to 7 hours for the entirety of the SWE-bench test split.</p> <p>When evaluation finishes, you should see an output similar to the following: <pre><code>2024-03-31 16:47:00,263 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Installing with command: . /n/fs/p-swe-bench/testbed/ba397fe0d6/pvlib__pvlib-python/0.8/tmpom22t9na/miniconda3/bin/activate pvlib__pvlib-python__0.8 &amp;&amp; echo 'activate successful' &amp;&amp; pip install -e .[all]\n2024-03-31 16:47:10,602 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Installation successful\n2024-03-31 16:47:10,619 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Apply patch successful (test)\n2024-03-31 16:47:10,635 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Apply patch successful (pred)\n2024-03-31 16:47:13,453 - taskenv_context_manager - INFO - [pvlib__pvlib-python__0.8] [pvlib__pvlib-python-1395] Test script run successful\n==================================\nLog directory for evaluation run: /n/fs/p-swe-bench/results/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4\n== Evaluation Report ==\n{'# Not Generated': 1, '# Generated': 36, '# Applied': 34, '# Resolved': 5}\n- Wrote per-instance scorecards to /&lt;path to SWE-agent&gt;/trajectories/carlosejimenez/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4/scorecards.json\n- Wrote summary of run to /&lt;path to SWE-agent&gt;/trajectories/carlosejimenez/gpt-4-1106-preview__swe-bench-dev-40-seed24__default_sys-env_window100-detailed_cmd_format-full_history-1_demos__t-0.20__p-0.95__c-4.00__install-1__sweep-01-run-4/results.json\nReference Report:\n{'# Not Generated': 1, '# Generated': 36, '# Applied': 34, '# Resolved': 5}\n</code></pre></p>"},{"location":"usage/benchmarking/#swe-bench-evaluation","title":"\ud83e\ude91 SWE-bench Evaluation","text":"<p><code>evaluation.py</code>: This script contains the logic for SWE-bench evaluation adapted for the SWE-agent setting. Given a set of predictions (e.g. <code>trajectories/&lt;user&gt;/&lt;experiment&gt;/all_preds.jsonl</code>), we...</p> <ol> <li>Filter + analyze predictions.</li> <li>Run SWE-bench style execution based evaluation.</li> <li>Save outcomes to <code>results.json</code> and <code>scorecards.json</code> files with info about task-specific and overall performance.</li> </ol> <p>Examples</p> <p><code>run_eval.sh</code> (see above) is provided as an example of how to run <code>evaluation.py</code></p> <p>Arguments:</p> <ul> <li><code>--predictions_path (required)</code>: The path to the file containing predictions (.jsonl format). This file includes the predictions that need to be evaluated against the benchmark tasks.</li> <li><code>--log_dir (required)</code>: The directory path where log files related to the evaluation process will be stored. It's used for saving logs that are generated during the evaluation.</li> <li><code>--swe_bench_tasks (required)</code>: The path to the file containing the SWE-bench task instances. This file includes the details of the tasks against which the predictions will be evaluated.</li> <li><code>--testbed (required)</code>: The directory path for the testbed, which is likely used for setting up the environment or context for the evaluations.</li> <li><code>--skip_existing (optional)</code>: If specified, the script will skip over log files that already exist, preventing re-evaluation of those tasks.</li> <li><code>--timeout (optional)</code>: Specifies the timeout in seconds for the evaluation process (default is 900 seconds). This helps in controlling the duration of each evaluation task to avoid excessively long running times.</li> <li><code>--verbose (optional)</code>: Enables verbose mode, which will provide more detailed output during the script execution. This is useful for debugging or getting more insight into the process.</li> <li><code>--conda_link (optional)</code>: Allows specifying a URL to a Conda installation that should be used for the evaluation environment. This can be necessary if the evaluation requires a specific software environment.</li> <li><code>--log_suffix (optional)</code>: An additional parameter to specify a suffix for log files. This can be used for organizing logs more effectively, especially when running multiple evaluations in parallel or under different configurations.</li> </ul>"},{"location":"usage/benchmarking/#viewing-results","title":"\ud83d\udcc8 Viewing Results","text":"<p><code>aggregate_results.py</code>: This script aggregates and displays experiment results from the <code>trajectories/</code> folder.</p> <ul> <li>Experiments are grouped by <code>(Model, Dataset, Config File, Temp., Top P, Cost, Install)</code>.</li> <li>The following statistics for each experiment run are shown:<ul> <li><code>Not Generated</code>: # of task instances with no patch generated</li> <li><code>Generated</code>: # of task instances with patch</li> <li><code>Applied</code>: # of patches that applied successfully</li> <li><code>Resolved</code>: # of task instances resolved</li> <li><code>Costs [Success|Failed|Overall]</code>: Cost of [successful|failed|any] run</li> </ul> </li> <li>If there are multiple runs of an experiment (distinguished by <code>--suffix run&lt;i&gt;</code>), the above statistics are aggregate as totals or means.</li> </ul> <p>Usage:</p> <pre><code>python aggregate_results.py\n</code></pre> <p>Arguments:</p> <ul> <li><code>--folder (type: str, default: ../trajectories)</code>: Specifies the folder containing the experiment * results. This is where the script will look to gather data.</li> <li><code>--model (type: str, nargs: '+')</code>: Filters the results by model(s). Only results corresponding to the * specified model(s) will be included.</li> <li><code>--dataset (type: str, nargs: '+')</code>: Filters the results by dataset(s). Only results for the specified * dataset(s) will be analyzed.</li> <li><code>--setup (type: str, nargs: '+')</code>: Filters the results by setup(s). This allows focusing on specific * experiment configurations.</li> <li><code>--runs_min (type: int)</code>: The minimum number of runs an experiment should have to be included in the * analysis. Helps exclude experiments with insufficient data.</li> <li><code>--runs_max (type: int)</code>: The maximum number of runs to consider for each experiment. This can limit the data to the most relevant runs.</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/cl_tutorial/","title":"Command line usage tutorial","text":"<p>This tutorial walks you trough running SWE-agent from the command line. Beginners might also be interested in the our web-based GUI (see here). This tutorial focuses on using SWE-agent as a tool to solve individual issues. Benchmarking SWE-agent is covered separately.</p>"},{"location":"usage/cl_tutorial/#getting-started","title":"Getting started","text":"<p>For the CLI, use the <code>run.py</code> script. Let's start with an absolutely trivial example and solve an issue about a simple syntax error (<code>swe-agent/test-repo #1</code>)</p> <pre><code>python run.py \\\n  --model_name gpt4 \\\n  --data_path https://github.com/SWE-agent/test-repo/issues/1 \\\n  --config_file config/default_from_url.yaml \\\n  --per_instance_cost_limit 2.00\n</code></pre> <p>Here,</p> <ul> <li><code>--model_name</code> sets the language model that is used by SWE-agent (with <code>gpt4</code> being the default). More information on the available models in our FAQ</li> <li><code>--data_path</code> points to the source of the problem statement (for example, the GitHub issue that you want to solve). You can also point it to local files (see below)</li> <li><code>--config_file</code> includes settings such as the prompts. Changing the config file is the easiest way to get started with modifying SWE-agent (more advanced options are discussed here).</li> <li><code>--per_instance_cost_limit</code> limits the total inference cost to $2 (default is $3).</li> </ul> <p>All options</p> <p>Run <code>python run.py --help</code> to see all available options for <code>run.py</code>. This tutorial will only cover a subset of options.</p> <p>Running more than once</p> <ul> <li>The complete details of the run are saved as a \"trajectory\" file (more about them here). They can also be turned into new demonstrations.</li> <li>If you run the same command more than once, you will find that SWE-agent aborts with <code>Skipping existing trajectory</code>. You can either remove the trajectory from the warning message, or add the <code>--skip_existing=False</code> flag.</li> <li>If you solve multiple issues from the same repository/in the same environment, you can specify the   <code>--cache_task_images</code> flag. This will create a persistent docker image with the initialized environment   required for the problem.</li> </ul>"},{"location":"usage/cl_tutorial/#specifying-the-repository","title":"Specifying the repository","text":"<p>In the above example, the repository/codebase is inferred from the <code>--data_path</code>. This options is currently only available for GitHub issues. For all other use cases, you can specify <code>--repo_path</code>, which accepts either GitHub URLs or paths to local repositories.</p> <p>To try it out, let's clone the test repository from the previous section.</p> <pre><code>git clone git@github.com:SWE-agent/test-repo.git\n</code></pre> <p>and then run</p> <pre><code>python run.py \\\n  --data_path /path/to/test-repo/problem_statements/1.md \\\n  --repo_path /path/to/test-repo \\\n  --config_file config/default_from_url.yaml \\\n  --apply_patch_locally\n</code></pre> <p>where you replaced paths with the prefix <code>/path/to/.../</code> with the actual paths to the corresponding file/directory.</p> <p>We have also added a new flag, <code>--apply_patch_locally</code>, which will make SWE-agent apply the changes to the local repository (if it believes that it has successfully solved the issue).</p> <p>You can mix and match the different ways of specifying problem statements and repositories. For example, any of the following combination of options also works</p> <ul> <li>Local problem statement with GitHub repository (<code>--data_path /path/to/problem.md --repo_path https://github.com/...</code>): Let SWE-agent work on something that wasn't reported yet</li> <li>GitHub issue with local repository (<code>--data_path https://github.com/.../issues/.. --repo_path /path/to/... --apply_patch_locally</code>): Let SWE-agent solve a GitHub issue locally (for example to edit the solution afterwards)</li> <li>GitHub issue with different GitHub repository: Useful with the <code>--open_pr</code> flag (see below) when working from a fork.</li> </ul> <p>In addition, if <code>--repo_path</code> points to a GitHub repository, you can use <code>--base_commit</code> to specify</p> <ul> <li>A branch name (e.g., <code>dev</code>),</li> <li>A tag (e.g., <code>v1.0.0</code>),</li> <li>A commit hash (e.g., <code>a4464baca1f28d7733337df6e4daa6c1ed920336</code>).</li> </ul> <p>SWE-agent will then start from this commit when trying to solve the problem.</p> <p>Uncommitted changes</p> <p>When running with a local <code>--repo_path</code>, SWE-agent will use the last commit, i.e., all local, uncommitted changes will not be seen by SWE-agent.</p>"},{"location":"usage/cl_tutorial/#installing-dependencies-and-setting-up-the-environment","title":"Installing dependencies and setting up the environment","text":"<p>Now let's move on to a slightly more complicated issue (<code>swe-agent/test-repo #22</code>).</p> <p>What makes it more complicated? This time the problematic code is part of a library <code>testpkg</code>, so SWE-agent first has to install the package in order to reproduce the issue before searching for the problematic code.</p> <p>In most circumstances, GPT4 will attempt to install the package and requirements (usually with some form of <code>pip install .</code> or <code>pip install pkg</code>). However, this wastes valuable queries to the LM. In addition, you might need to run your software for a specific python version or have other specific environment settings. The <code>--environment_setup</code> flag is used to fix this problem.</p> <p>Let's try it:</p> <pre><code>python run.py \\\n  --data_path https://github.com/SWE-agent/test-repo/issues/22 \\\n  --config_file config/default_from_url.yaml \\\n  --environment_setup config/environment_setup/py310_default.yaml\n</code></pre> <p>This time, <code>pip install -e .</code> is called before SWE-agent gets to work, installing the package defined in the repository.</p> <p>Let's take a look at the config file</p> <pre><code>python: '3.10'\ninstall: 'pip install -e .'\n</code></pre> <p>Here, <code>install</code> is an arbitrary command that is run, while <code>python</code> will be the python version that is setup with conda.</p> <p>The config file also provides two more top level directives:</p> <ul> <li><code>packages</code>: Path to a <code>requirements.txt</code> or to a <code>env.yml</code> as readable by conda</li> <li><code>pip_packages</code>: A list of python packages that are installed with <code>pip install PACKAGE</code></li> </ul>"},{"location":"usage/cl_tutorial/#taking-actions","title":"Taking actions","text":"<ul> <li>As mentioned above, you can use <code>--apply_patch_locally</code> to have SWE-agent apply successful solution attempts to local files.</li> <li>Alternatively, when running on a GitHub issue, you can have the agent automatically open a PR if the issue has been solved by supplying the <code>--open_pr</code> flag.   Please use this feature responsibly (on your own repositories or after careful consideration).</li> </ul> <p>Alternatively, you can always retrieve the patch that was generated by SWE-agent. Watch out for the followoing message in the log:</p> <pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \ud83c\udf89 Submission successful \ud83c\udf89 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 SWE-agent has produced a patch that it believes will solve the issue you submitted! \u2502\n\u2502 Use the code snippet below to inspect or apply it!                                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>And follow the instructions below it:</p> <pre><code> # The patch has been saved to your local filesystem at:\n PATCH_FILE_PATH='/Users/.../patches/05917d.patch'\n # Inspect it:\n cat \"${PATCH_FILE_PATH}\"\n # Apply it to a local repository:\n cd &lt;your local repo root&gt;\n git apply \"${PATCH_FILE_PATH}\"\n</code></pre> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/inspector/","title":"Trajectory inspector","text":"<p>We provide a web interface for visualizing <code>.traj</code> files from the <code>trajectories</code> folder more easily.</p> <p>Set Up</p> <ul> <li>Change to the <code>inspector</code> directory</li> <li>Run <code>python server.py --directory insert_full_absolute_path_to_the_trajectories_folder_here/trajectories</code></li> <li>Open http://localhost:8000 in your browser to use the inspector.</li> </ul> <p>Additional flags</p> <ul> <li><code>--data_path</code>: Path to SWE-bench style dataset that trajectories were generated for (Optional)</li> <li><code>--directory</code>: Directory of trajectories to inspect (Defaults to <code>./trajectories</code> folder)</li> <li><code>--port</code>: Port to host web app (Defaults to <code>8000</code>).</li> </ul> <p>Example Usage</p> <p>From running the command:</p> <p><pre><code>python server.py --directory /Users/ofirp/swe-agent/trajectories\n</code></pre> The inspector will then be launched in the browser:</p> <p></p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/trajectories/","title":"Trajectories","text":"<p>The <code>trajectories/</code> folder is the default location that experiment results (invocations of <code>run.py</code>) will be written to.</p> <p>At a high level, the experiments folder is organized in the following manner: <pre><code>trajectories\n\u251c\u2500\u2500 &lt;user 1&gt; \ud83d\udc69\u200d\ud83d\udcbb\n\u2502 \u251c\u2500\u2500 &lt;experiment 1&gt; \ud83e\uddea\n\u2502 \u2502 \u251c\u2500\u2500 all_preds.jsonl\n\u2502 \u2502 \u251c\u2500\u2500 args.yaml\n\u2502 \u2502 \u251c\u2500\u2500 *.html (Webpage Files)\n\u2502 \u2502 \u2514\u2500\u2500 *.traj (Trajectories)\n\u2502 \u2514\u2500\u2500 &lt;experiment 2&gt; \ud83e\uddea\n\u2502   \u251c\u2500\u2500 all_preds.jsonl\n\u2502   \u251c\u2500\u2500 args.yaml\n\u2502   \u251c\u2500\u2500 *.html (Webpage Files)\n\u2502   \u2514\u2500\u2500 *.traj (Trajectories)\n\u251c\u2500\u2500 &lt;user 2&gt; \ud83d\udc68\u200d\ud83d\udcbb\n\u2502 \u251c\u2500\u2500 &lt;experiment 1&gt; \ud83e\uddea\n\u2502 \u2502 \u2514\u2500\u2500 ...\n\u2502 \u2514\u2500\u2500 &lt;experiment 2&gt; \ud83e\uddea\n\u2502   \u2514\u2500\u2500 ...\n...\n</code></pre> Where every experiment follows the pattern <code>trajectories/&lt;user name&gt;/&lt;experiment name&gt;</code>. The <code>&lt;user name&gt;</code> is automatically inferred from your system, and the <code>experiment name</code> is inferred from the arguments of the <code>run.py</code>.</p> <p>Viewing trajectories</p> <p>We provide a trajectory viewer for an easy viewing of trajectories.</p>"},{"location":"usage/trajectories/#how-an-experiment-folder-is-generated","title":"How an Experiment Folder is Generated","text":"<p>Each call to <code>run.py</code> produces a single <code>trajectories/&lt;user name&gt;/&lt;experiment name&gt;</code> folder containing the following assets:</p> <ul> <li><code>all_preds.jsonl</code>: A single file containing all of the predictions generated for the experiment (1 prediction per task instance), where each line is formatted as: <pre><code>{\n    \"instance_id\": \"&lt;Unique task instance ID&gt;\",\n    \"model_patch\": \"&lt;.patch file content string&gt;\",\n    \"model_name_or_path\": \"&lt;Model name here (Inferred from experiment configs)&gt;\",\n}\n</code></pre></li> <li><code>args.yaml</code>: A summary of the configurations for the experiment run.</li> <li><code>&lt;instance_id&gt;.traj</code>: A <code>.json</code> formatted file containing the (thought, action, observation) turns generated by SWE-agent towards solving <code>&lt;instance_id&gt;</code>.</li> <li><code>&lt;instance_id&gt;.html</code>: An <code>.html</code> single webpage render of the trajectory, which can be directly opened in the browser for easier viewing of the trajectory.</li> </ul> <p>Tip</p> <ul> <li>Evaluation is not completed by <code>run.py</code>, it is a separate step (see benchmarking)</li> <li><code>all_preds.jsonl</code> can be referenced directly into <code>evaluation/run_eval.sh</code> to run evaluation (see benchmarking)</li> <li>Trajectories can be turned into custom demonstrations for SWE-agent (more information).</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/usage_faq/","title":"Usage FAQ","text":""},{"location":"usage/usage_faq/#what-models-are-supported","title":"What models are supported?","text":"<p>Models are configured in <code>models.py</code> (we're working on giving a complete list of model settings).</p> <p>Here are some few examples:</p> <pre><code>gpt4\ngpt4o\ngpt4-turbo\nclaude-2\nclaude-opus\nclaude-sonnet\nclaude-haiku\n</code></pre>"},{"location":"usage/usage_faq/#ollama-support","title":"Ollama support","text":"<p>Models served with an ollama server can be used by specifying <code>--model</code> with <code>ollama:model_name</code> and <code>--host_url</code> to point to the url used to serve ollama (<code>http://localhost:11434</code> by default). See more details about using ollama here.</p> <pre><code>python run.py --model_name ollama:deepseek-coder:6.7b-instruct \\\n  --host_url http://localhost:11434 \\\n  --data_path https://github.com/pvlib/pvlib-python/issues/1603 \\\n  --config_file config/default_from_url.yaml\n</code></pre>"},{"location":"usage/usage_faq/#models-for-testing","title":"Models for testing","text":"<p>We also provide models for testing SWE-agent without spending any credits</p> <ul> <li><code>HumanModel</code> and <code>HumandThoughtModel</code> will prompt for input from the user that stands in for the output of the LM. This can be used to create new demonstrations.</li> <li><code>ReplayModel</code> takes a trajectory as input and \"replays it\"</li> <li><code>InstantEmptySubmitTestModel</code> will create an empty <code>reproduce.py</code> and then submit</li> </ul> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"},{"location":"usage/web_ui/","title":"Using the web interface","text":"<p>Our graphical web interface is optimized for using SWE-agent as a developer tool, fixing single GitHub issues or working in local repositories. However, it is still missing some of the options of the command line interface.</p>"},{"location":"usage/web_ui/#quickstart","title":"Quickstart","text":"<p>To start our web UI, simply run</p> <pre><code>./start_web_ui.sh\n</code></pre> <p>from the root of the repository.</p> <p>Opening the webpage</p> <p>If the user interface doesn't automatically open in your browser, please open it at <code>http://localhost:3000</code>. Running from GitHub codespaces? More tips here.</p> <p>Running from Docker</p> <p>If you run SWE-agent from the <code>docker-run</code> Docker container, please see here for how to start the web server.</p>"},{"location":"usage/web_ui/#manually-starting-frontend-and-backend","title":"Manually starting frontend and backend","text":"<p>The web UI consists of a frontend written in react (showing the pretty control elements) and a backend written with flask. The <code>./start_web_ui.sh</code> starts both of them in the background. However, this might not be best for development and debugging. This section explains how to start both parts separately.</p> <p>First, let's start the backend:</p> <pre><code>python sweagent/api/server.py\n</code></pre> <p>You should see output similar to the following:</p> <pre><code> * Serving Flask app 'server'\n * Debug mode: on\n2024-05-23 11:30:45,436 - werkzeug - INFO - WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n * Running on http://127.0.0.1:8000\n2024-05-23 11:30:45,437 - werkzeug - INFO - Press CTRL+C to quit\n2024-05-23 11:30:45,437 - werkzeug - INFO -  * Restarting with watchdog (fsevents)\n2024-05-23 11:30:46,484 - werkzeug - WARNING -  * Debugger is active!\n2024-05-23 11:30:46,492 - werkzeug - INFO -  * Debugger PIN: 123-594-933\n</code></pre> <p>Port availability</p> <p>If see an error about port 8000 not being available, please first close any application that occupies it. The frontend currently expects the <code>flask</code> server on port 8000, so choosing a different port won't work.</p> <p>Now, open a new terminal tab and navigate to the <code>frontend</code> directory:</p> <pre><code>cd sweagent/frontend\n</code></pre> <p>First, let's install the react dependencies:</p> <pre><code>npm install\n</code></pre> <p>And start the server:</p> <pre><code>npm start\n</code></pre> <p>This should also open the corresponding page in your browser. If not, check with the tips above. The default port that is being served is port 3000.</p> <p>Possible errors</p> <p>If you see errors</p> <pre><code>Proxy error: Could not proxy request /socket.io/?EIO=4&amp;transport=polling&amp;t=O-c5kv9 from localhost:3000 to http://localhost:8000.\nSee https://nodejs.org/api/errors.html#errors_common_system_errors for more information (ECONNREFUSED).\n</code></pre> <p>something went wrong with the backend part.</p> <ul> <li> <p> Something broken?  Report bug</p> </li> <li> <p> Something unclear?  Ask question</p> </li> </ul>"}]}